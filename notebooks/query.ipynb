{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.core.storage.index_store import SimpleIndexStore\n",
    "from llama_index.core.vector_stores.simple import SimpleVectorStore\n",
    "from llama_index.core.callbacks import CallbackManager, TokenCountingHandler, EventPayload\n",
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "from llama_index.core.response.notebook_utils import display_response\n",
    "from llama_index.core.response.pprint_utils import pprint_response, pprint_metadata, pprint_source_node\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.llms.openai.utils import ALL_AVAILABLE_MODELS, CHAT_MODELS\n",
    "from llama_index.core import Settings\n",
    "\n",
    "PERSIST_DB_DIR = \"../db/db_storage/\"\n",
    "ALL_AVAILABLE_MODELS[\"gpt-4o-mini\"]= 128000\n",
    "CHAT_MODELS[\"gpt-4o-mini\"] = 128000\n",
    "\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_index(\n",
    "        docstore = SimpleDocumentStore(), \n",
    "        index_store=SimpleIndexStore(), \n",
    "        vector_store = SimpleVectorStore(), \n",
    "        embedding_model = Settings.embed_model,\n",
    "        persist_db_dir = PERSIST_DB_DIR\n",
    "    ):\n",
    "\n",
    "    storage_context = StorageContext.from_defaults(\n",
    "        docstore=docstore.from_persist_dir(persist_db_dir),\n",
    "        vector_store=vector_store.from_persist_dir(persist_db_dir, namespace=\"default\"),\n",
    "        index_store=index_store.from_persist_dir(persist_db_dir),\n",
    "    )\n",
    "\n",
    "    vector_index = load_index_from_storage(storage_context, index_id=\"vector_index\")\n",
    "\n",
    "    return vector_index\n",
    "\n",
    "vector_index = load_index(persist_db_dir=PERSIST_DB_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = vector_index.as_query_engine(llm=OpenAI(model=\"gpt-4o-mini\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = query_engine.query(\"What are the highlights of the lastest llamaindex newsletter??\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** The latest LlamaIndex newsletter features exciting updates, in-depth guides, demos, educational tutorials, and webinars aimed at enhancing user experience and understanding of the platforms and tools."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**`Source Node 1/2`**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 151c9156-9ba0-47f2-bf33-f6df229b9747<br>**Similarity:** 0.7101944796118811<br>**Text:** Hello Llama Fansü¶ô\n",
       "Step into this week's edition of the LlamaIndex newsletter, where we bring you ...<br>**Metadata:** {'title': 'LlamaIndex Newsletter 2024-06-11', 'link': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-06-11', 'date': 'Jun 11, 2024', 'tags': '[]'}<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**`Source Node 2/2`**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** c848514e-c711-4bd5-9d62-9cc82d5169f9<br>**Similarity:** 0.6910661516602047<br>**Text:** Hello to All Llama Lovers!ü¶ô\n",
       "Welcome to this week‚Äôs issue of the LlamaIndex newsletter! This editi...<br>**Metadata:** {'title': 'LlamaIndex Newsletter 2024-06-25', 'link': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-06-25', 'date': 'Jun 25, 2024', 'tags': '[]'}<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_response(res,show_source=True,show_source_metadata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores import MetadataInfo, VectorStoreInfo\n",
    "import datetime\n",
    "\n",
    "vector_store_info = VectorStoreInfo(\n",
    "    content_info=\"Receipts\",\n",
    "    metadata_info=[\n",
    "        MetadataInfo(\n",
    "            name=\"date\",\n",
    "            description=\"The result for this date\",\n",
    "            type=f\"date in MMM dd, yyyy format. Today is {datetime.datetime.today()}\",\n",
    "        ),\n",
    "        # MetadataInfo(\n",
    "        #     name=\"year\",\n",
    "        #     description=\"The year blog post was created\",\n",
    "        #     type=\"integer\",\n",
    "        # ),\n",
    "        # MetadataInfo(\n",
    "        #     name=\"month\",\n",
    "        #     description=\"The month blog post was created\",\n",
    "        #     type=\"integer\",\n",
    "        # ),\n",
    "        # MetadataInfo(\n",
    "        #     name=\"day\",\n",
    "        #     description=\"The day blog post was created\",\n",
    "        #     type=\"integer\",\n",
    "        # )\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import VectorIndexAutoRetriever\n",
    "\n",
    "retriever = VectorIndexAutoRetriever(\n",
    "    vector_index,\n",
    "    vector_store_info=vector_store_info,\n",
    "    similarity_top_k=2,\n",
    "    empty_query_top_k=10,  # if only metadata filters are specified, this is the limit\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using query str: newsletter\n",
      "Using filters: [('date', '==', 'Jul 16, 2024')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nodes = retriever.retrieve(\n",
    "    \"The newsletter in 16 Jul 2024?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NodeWithScore(node=TextNode(id_='2fd697ef-aec5-42a6-b2fe-356f99b19203', embedding=None, metadata={'title': 'LlamaIndex Newsletter 2024-07-16', 'link': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-16', 'date': 'Jul 16, 2024', 'tags': '[]'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=['file_name'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='fef0c07c-6811-464d-8784-db468bef5097', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'title': 'LlamaIndex Newsletter 2024-07-16', 'link': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-16', 'date': 'Jul 16, 2024', 'tags': '[]'}, hash='20b71507f27148a1973ab69881603705bfade99570303d5e500237eef61912f5'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='ce3296ae-cb81-456c-8272-4eb1ad3f3783', node_type=<ObjectType.TEXT: '1'>, metadata={'Header_2': 'ü§©\\xa0The highlights:'}, hash='f75093d8a4531286f507cc040ff1f86232320011a68285eaef4903c93fd906ea')}, text='Hello, Llama Family! ü¶ô\\n\\nWelcome to this week‚Äôs edition of the LlamaIndex newsletter! We‚Äôre thrilled to share some exciting updates about our products, the implementation of GraphRAG, demos that have achieved over $1M in ARR, extensive guides, in-depth tutorials, and hackathons.\\n\\nBefore we get into the details of our newsletter, we‚Äôre thrilled to share the beta launch of LlamaCloud. This new data processing layer boosts RAG workflows with sophisticated parsing, indexing, and retrieval functions. Alongside this, we‚Äôre also introducing LlamaTrace in partnership with Arize AI, which provides unmatched tracing, observability, and evaluation capabilities for LLM application workflows.\\n\\nSignup here: cloud.llamaindex.ai', mimetype='text/plain', start_char_idx=0, end_char_idx=721, text_template='Metadata: {metadata_str}\\n----------\\n\\nContent: {content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.34561796437008424),\n",
       " NodeWithScore(node=TextNode(id_='a56e7bae-fbe9-4e71-adc6-bf305a4b3eff', embedding=None, metadata={'Header_2': 'üí°\\xa0Demos:', 'title': 'LlamaIndex Newsletter 2024-07-16', 'link': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-16', 'date': 'Jul 16, 2024', 'tags': '[]'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=['file_name'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='fef0c07c-6811-464d-8784-db468bef5097', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'title': 'LlamaIndex Newsletter 2024-07-16', 'link': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-16', 'date': 'Jul 16, 2024', 'tags': '[]'}, hash='20b71507f27148a1973ab69881603705bfade99570303d5e500237eef61912f5'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='9800d7e5-0427-4231-b2ee-15bfa7ee3138', node_type=<ObjectType.TEXT: '1'>, metadata={'Header_2': '‚ú® Feature Releases and Enhancements:', 'title': 'LlamaIndex Newsletter 2024-07-16', 'link': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-16', 'date': 'Jul 16, 2024', 'tags': '[]'}, hash='94f0ef1b47e769a318ecc5cf55859ac647b2a094d9067b75d879ba097eed7a09'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='f1f12d88-51c1-40b8-9641-931e8a1aa7e6', node_type=<ObjectType.TEXT: '1'>, metadata={'Header_2': 'üó∫Ô∏è Guides:'}, hash='092d94c25720dca7bbf4d30a8f6b929de8d966c8361850f7a32ceabcb0cb7d27')}, text='üí°\\xa0Demos:\\n- Lyzrai has achieved over $1M ARR using LlamaIndex! This full-stack autonomous AI agent framework enhances AI sales and marketing functions with LlamaIndex‚Äôs data connectors and RAG capabilities, boasting rapid revenue growth, high accuracy, and customer satisfaction.', mimetype='text/plain', start_char_idx=2900, end_char_idx=3178, text_template='Metadata: {metadata_str}\\n----------\\n\\nContent: {content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.3138996682733809)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama Index Blog Crawler and Query Engine\n",
    "\n",
    "This project provides a tool to crawl blog posts, initialize and load an index, and query the index using a command-line interface. It supports re-crawling the blog, re-indexing, and running evaluations with optional retry support.\n",
    "\n",
    "## Features\n",
    "\n",
    "- Crawl blog posts and store them in a local database.\n",
    "- Initialize and load an index from local storage.\n",
    "- Query the index using a command-line interface.\n",
    "- Optional evaluation mode with retry support for generating answers.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- Python 3.9\n",
    "- Required Python packages (install via `requirements.txt`)\n",
    "\n",
    "## Installation\n",
    "\n",
    "Install the required packages:\n",
    "```sh\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "## Usage\n",
    "\n",
    "### API-KEY\n",
    "This project use openAI, you must store OPENAI_API_KEY to .env file\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=\n",
    "```\n",
    "\n",
    "### Command-Line Arguments\n",
    "\n",
    "- `--re-crawl`: Re-crawl the blog before running queries.\n",
    "- `--eval`: Run evaluation on the test set.\n",
    "\n",
    "### Running the Script\n",
    "\n",
    "To run the script, use the following command:\n",
    "```sh\n",
    "python main.py [--re-crawl] [--eval]\n",
    "```\n",
    "\n",
    "### Example\n",
    "1. Interact with the RAG via Q&A\n",
    "```sh\n",
    "python main.py\n",
    "```\n",
    "\n",
    "1. Re-crawl the blog and re-index\n",
    "```sh\n",
    "python main.py --re-crawl\n",
    "```\n",
    "\n",
    "3. Run evaluation using RAGAS\n",
    "```sh\n",
    "python main.py --eval\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
