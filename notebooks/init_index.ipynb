{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thienpham/git/vietai-assignment-01/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import sys\n",
    "from llama_index.core import (\n",
    "    SummaryIndex,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext, \n",
    "    MockEmbedding\n",
    ")\n",
    "from llama_index.core.storage.index_store import SimpleIndexStore\n",
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.core.vector_stores.simple import SimpleVectorStore\n",
    "from llama_index.core.node_parser import SentenceSplitter, MarkdownNodeParser\n",
    "from llama_index.core.ingestion import IngestionPipeline, IngestionCache\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 159 documents.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 159/159 [00:00<00:00, 933.45it/s] \n",
      "Generating embeddings:   0%|          | 0/1643 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:   6%|▌         | 100/1643 [00:02<00:42, 36.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:  12%|█▏        | 200/1643 [00:04<00:33, 42.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:  18%|█▊        | 300/1643 [00:07<00:30, 43.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:  24%|██▍       | 400/1643 [00:09<00:27, 45.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:  30%|███       | 500/1643 [00:11<00:25, 44.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:  37%|███▋      | 600/1643 [00:13<00:22, 47.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:  43%|████▎     | 700/1643 [00:15<00:19, 47.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:  49%|████▊     | 800/1643 [00:17<00:17, 47.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:  55%|█████▍    | 900/1643 [00:19<00:15, 48.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:  61%|██████    | 1000/1643 [00:21<00:13, 47.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:  67%|██████▋   | 1100/1643 [00:24<00:11, 45.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:  73%|███████▎  | 1200/1643 [00:26<00:10, 44.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:  79%|███████▉  | 1300/1643 [00:28<00:07, 43.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:  85%|████████▌ | 1400/1643 [00:30<00:05, 45.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:  91%|█████████▏| 1500/1643 [00:33<00:03, 44.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:  97%|█████████▋| 1600/1643 [00:35<00:00, 45.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 1643/1643 [00:36<00:00, 44.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 1643 nodes.\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "# DB initial config\n",
    "PERSIST_DB_DIR = \"./db/db_storage/\"\n",
    "DATA_DIR = \"./data/\"\n",
    "FILENAME = \"llama_index_blog_posts.csv\"\n",
    "docstore = SimpleDocumentStore()\n",
    "index_store=SimpleIndexStore()\n",
    "vector_store = SimpleVectorStore()\n",
    "\n",
    "# init the pipeline with transformations\n",
    "ingestion_pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        # SentenceSplitter(chunk_size=1024, chunk_overlap=10),\n",
    "        MarkdownNodeParser(),\n",
    "        # TitleExtractor(),\n",
    "        # QuestionsAnsweredExtractor(questions=3),\n",
    "        OpenAIEmbedding(model=\"text-embedding-ada-002\"),\n",
    "        # MockEmbedding(embed_dim=1536),\n",
    "    ],\n",
    "    vector_store=vector_store,\n",
    "    docstore=docstore,\n",
    ")\n",
    "\n",
    "# local cache of the pipeline\n",
    "if not os.path.exists(PERSIST_DB_DIR):\n",
    "    ingestion_pipeline.persist(PERSIST_DB_DIR)\n",
    "else: \n",
    "    ingestion_pipeline.load(PERSIST_DB_DIR)\n",
    "\n",
    "# load the documents\n",
    "import csv_processor\n",
    "documents = csv_processor.csv_load(DATA_DIR + FILENAME)\n",
    "print(f\"Loaded {len(documents)} documents.\")\n",
    "\n",
    "# run the pipeline to get nodes\n",
    "nodes = ingestion_pipeline.run(\n",
    "    documents=documents,\n",
    "    in_place=True,\n",
    "    show_progress=True,\n",
    ")\n",
    "print(f\"Created {len(nodes)} nodes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init storage context\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    docstore=docstore,\n",
    "    vector_store=vector_store,\n",
    "    index_store=index_store,\n",
    ")\n",
    "\n",
    "# create (or load) docstore and add nodes\n",
    "storage_context.docstore.add_documents(nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build index + save index\n",
    "vector_index = VectorStoreIndex(nodes, storage_context=storage_context)\n",
    "vector_index.set_index_id(\"vector_index\")\n",
    "vector_index.storage_context.persist(persist_dir=PERSIST_DB_DIR)\n",
    "\n",
    "summary_index = SummaryIndex(nodes,storage_context=storage_context)\n",
    "summary_index.set_index_id(\"summarise_index\")\n",
    "summary_index.storage_context.persist(persist_dir=PERSIST_DB_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import (\n",
    "    load_index_from_storage,\n",
    "    load_indices_from_storage,\n",
    "    load_graph_from_storage,\n",
    ")\n",
    "\n",
    "storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DB_DIR)\n",
    "indices = load_indices_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector_index\n",
      "summarise_index\n"
     ]
    }
   ],
   "source": [
    "for index in indices:\n",
    "    print(index.index_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
