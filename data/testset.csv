,question,contexts,ground_truth,evolution_type,metadata,episode_done
0,What are the key features of the Llama-Agents Framework?,"['Hello, Llama enthusiasts! ü¶ô\n\nWelcome to this week‚Äôs edition of the LlamaIndex newsletter! In this issue, we‚Äôre excited to bring you exciting updates about llama-agents, live demos, extensive guides, and in-depth tutorials to enhance your understanding of our tools.\n\nBefore moving into our newsletter, we have an exciting update on our enterprise offerings. We are thrilled to announce the waitlist release of LlamaCloud, our fully-managed ingestion service. Sign up now if you‚Äôre eager to collaborate and build LLM applications with LlamaCloud.\n\n## ü§©\xa0The highlights:\n- Launched Llama-Agents Framework: Our new alpha-release, llama-agents, enables multi-agent AI systems for production with a distributed architecture, HTTP API communication, and agentic orchestration. It‚Äôs designed for easy deployment, scalability, and observability. Blogpost, Tweet.\n- create-llama Integrated with LlamaCloud: Streamline your LLM application data pipelines with create-llama, now integrated with LlamaCloud for faster setup and efficient system maintenance. Tweet.\n\n## ‚ú® Feature Releases and Enhancements:\n- We have launched llama-agents - new alpha-release framework that enables multi-agent AI systems to go into production. It features a distributed, service-oriented architecture, communication through standard HTTP APIs, agentic orchestration of flows, and is designed for easy deployment, scalability, and observability. Blogpost, Tweet.\n- create-llama is now integrated with LlamaCloud to streamline the setup and management of data pipelines for LLM applications, providing a fast and efficient way to deploy and maintain these systems. Tweet.\n- We have integrated with DSPy for Optimized RAG by utilizing DSPy‚Äôs optimization capabilities with LlamaIndex‚Äôs data tools to enhance your query pipelines, optimize prompts, or repurpose DSPy predictors. Cookbook, Tweet.\n\n## üí°\xa0Demos:\n- Automating Code Reviews, project by Composio with LlamaIndex automates code reviews using an AI agent in under 100 lines of code that monitors GitHub PRs, reviews them immediately upon creation, and posts feedback directly to your Slack channel. Codebase.\n\n## üó∫Ô∏è Guides:\n- Guide to Building an Agentic RAG Service with our comprehensive notebook that walks you through creating vector indexes, transforming them into query engines, turning each engine into a tool, providing these tools to agents, and launching the agents as services.\n- Guide to AI Agents with LlamaIndex: Andrei‚Äôs comprehensive workshop from Gen AI Philippines, showcasing LLM applications through LlamaIndex. This beginner-friendly session covers topics from RAG to multi-hop agents. Video, Notebook.\n\n## ‚úçÔ∏è Tutorials:\n- Kingzzm‚Äôs tutorial on crafting a custom hybrid retriever using LlamaIndex‚Äôs flexible abstractions. This tutorial teaches you how to integrate full text and dense search capabilities from Elastic, and how to write your own reciprocal rank fusion function for optimal retrieval strategy.\n- Jeff‚Äôs tutorial on which outlines the essential tools needed to construct a report generator using a ReAct agent. Learn how to integrate a RAG tool over guideline documents, a web search tool, and a report generation tool that converts markdown text into PDFs.\n- 1littlecoder‚Äôs tutorial on llama-agents provides a detailed introduction to transforming multi-agent systems into microservices for production, including setup examples and a walkthrough of the architecture involving the control plane, message queue, and agent services using LlamaIndex abstractions.\n- Mervin Praison‚Äôs tutorial on the llama-agents framework provides a concise guide to setting up agent services, from notebook synchronization to server-client interactions, complete with over 10 practical examples.\n\n']","The key features of the Llama-Agents Framework include enabling multi-agent AI systems for production with a distributed architecture, HTTP API communication, agentic orchestration, easy deployment, scalability, and observability.",simple,"[{'title': 'LlamaIndex Newsletter 2024-07-02', 'link': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-02', 'date': 'Jul 2, 2024', 'tags': '[]'}]",True
1,What features does NewsGPT offer for tailored news recommendations?,"['We‚Äôre excited to share with you the thought process and solution design of NewsGPT (Neotice) ‚Äî Clickbait Buster, a Streamlit LLM hackathon-winning app powered by LlamaIndex, Streamlit, and Qdrant. In this article, we‚Äôll define the problem we‚Äôre trying to solve and discuss how we approached it. Lastly, we offer a workaround to enable LlamaIndex streaming on Streamlit Chat Bot and the all the code can be found here. We hope you‚Äôll find our insights helpful and informative.\n\n# Introduction\n## Problem Statement\nIt‚Äôs evident that people‚Äôs habits of consuming information have changed over time. Previously, we read lengthy articles content and watched long videos, such as newspapers and YouTube videos. However, we currently prefer reading titles and consuming short-form content, such as TikTok and YouTube shorts. Although this shift has made it easier to get more information in less time, it has also led to clickbait headlines that often contain incorrect information.\nWhen we started developing NewsGPT, our primary focus was to solve the above-stated pain points and provide a solution that 1) provides accurate information and 2) saves time for users.\n## Neotice\n\nWe are excited to announce that the beta version of the Neotice app, which is the production version of NewsGPT, is now available for users to try out! We are grateful to the Streamlit Hackathon for showcasing our prototype and philosophy. With the help of this platform, we are confident that our app will revolutionize the way people consume news.\n## Why NewsGPT Stands Out\nNewsGPT has four main components: Reliable News Sources, Tailored News Recommendations, Efficient Information Retrieval, and Time Saver.\n‚úÖ Reliable News Sources:\n- We‚Äôve established a dynamic data pipeline designed to ingest daily news, ensuring our information is up-to-date and relevant.\n- Sophisticated Named-Entity Recognition, Text Embedding with OpenAI API, and asynchronous article embedding processes are incorporated. This data is systematically stored in the QdrantVector Database, promoting accuracy and efficiency.\n\n‚úÖ Tailored News Recommendations:\n- Our system does more than just present news; it learns from you. By analyzing your reading habits, we leverage article embeddings to curate a personal news feed tailored to your interests.\n- A versatile search bar is always at your disposal, letting users explore any news topics that capture their interest.\n\n‚úÖ Efficient Information Retrieval:\n- With just a single click on an article of interest, NewsGPT gets to work. It collates similar news from multiple sources (3‚Äì5) and activates a Streamlit chatbot.\n- Your engagement begins immediately: the first query is autonomously forwarded to our chatbot to fetch a concise news summary.\n- For ease of user experience, we display predefined prompts as clickable buttons. This means users can receive information without the need for manual input.\n- Curiosity welcomed: any questions users may have about the news article will be addressed as long as the answers are detailed within the source articles.\n\n‚úÖ Time-Saving Reminder & Category Distribution Chart:\n- To keep you informed, our sidebar displays the time saved using NewsGPT and visually represents news category distribution.\n\n\n# Delving Deep into Architecture\n\n## Data Pipeline\nWe start with a reliable and sustainable data pipeline to support the users to get fresh news with two powerful libraries, pygooglenews, and newspaper3k.\npip install pygooglenews --upgrade\npip install newspaper3k\nBy utilizing Spark batch processing, we efficiently process data with NER(Named-Entity Recognition) and create embeddings via OpenAI API (Ada model). After the preprocessing of the data, we collect the metadata, including keywords, NER results, summary, body, title, author, and so on, in the payload and push the payload with embedding to the Qdrant Vector Database.\nWe will skip the part on how to create embeddings with the OpenAI Ada model, as there are many existing tutorials available. To perform Named Entity Recognition, we utilize the dslim/bert-base-NER model from HuggingFace.\nfrom transformers import pipeline\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\ntokenizer = AutoTokenizer.from_pretrained(""dslim/bert-base-NER"")\nmodel = AutoModelForTokenClassification.from_pretrained(""dslim/bert-base-NER"")\nnlp = pipeline(""ner"", model=model, tokenizer=tokenizer, batch_size=batch_size)\nner_results = nlp(texts)\n## Personalization']",NewsGPT offers tailored news recommendations by analyzing users' reading habits and leveraging article embeddings to curate a personal news feed tailored to their interests. It also includes a versatile search bar that allows users to explore any news topics that capture their interest.,simple,"[{'title': 'NewsGPT(Neotice): Summarize news articles with LlamaIndex ‚Äî Hackathon winning app', 'link': 'https://www.llamaindex.ai/blog/newsgpt-neotice-summarize-news-articles-with-llamaindex-hackathon-winning-app-9d7c8bcf9f11', 'date': 'Oct 27, 2023', 'tags': '[]'}]",True
2,What does Mean Reciprocal Rank (MRR) evaluate in a retrieval system?,"['UPDATE: The pooling method for the Jina AI embeddings has been adjusted to use mean pooling, and the results have been updated accordingly. Notably, the JinaAI-v2-base-en with bge-reranker-largenow exhibits a Hit Rate of 0.938202 and an MRR (Mean Reciprocal Rank) of 0.868539 and withCohereRerank exhibits a Hit Rate of 0.932584, and an MRR of 0.873689.\nWhen building a Retrieval Augmented Generation (RAG) pipeline, one key component is the Retriever. We have a variety of embedding models to choose from, including OpenAI, CohereAI, and open-source sentence transformers. Additionally, there are several rerankers available from CohereAI and sentence transformers.\nBut with all these options, how do we determine the best mix for top-notch retrieval performance? How do we know which embedding model fits our data best? Or which reranker boosts our results the most?\nIn this blog post, we‚Äôll use the Retrieval Evaluation module from LlamaIndex to swiftly determine the best combination of embedding and reranker models. Let\'s dive in!\nLet‚Äôs first start with understanding the metrics available in Retrieval Evaluation\n# Understanding Metrics in Retrieval Evaluation:\nTo gauge the efficacy of our retrieval system, we primarily relied on two widely accepted metrics: Hit Rate and Mean Reciprocal Rank (MRR). Let‚Äôs delve into these metrics to understand their significance and how they operate.\nHit Rate:\nHit rate calculates the fraction of queries where the correct answer is found within the top-k retrieved documents. In simpler terms, it‚Äôs about how often our system gets it right within the top few guesses.\nMean Reciprocal Rank (MRR):\nFor each query, MRR evaluates the system‚Äôs accuracy by looking at the rank of the highest-placed relevant document. Specifically, it‚Äôs the average of the reciprocals of these ranks across all the queries. So, if the first relevant document is the top result, the reciprocal rank is 1; if it‚Äôs second, the reciprocal rank is 1/2, and so on.\nNow that we‚Äôve established the scope and familiarized ourselves with the metrics, it‚Äôs time to dive into the experiment. For a hands-on experience, you can also follow along using our Google Colab Notebook\n# Setting Up the Environment\n!pip install llama-index sentence-transformers cohere anthropic voyageai protobuf pypdf\n# Setting Up the Keys\nopenai_api_key = \'YOUR OPENAI API KEY\'\ncohere_api_key = \'YOUR COHEREAI API KEY\'\nanthropic_api_key = \'YOUR ANTHROPIC API KEY\'\nopenai.api_key = openai_api_key\n# Download the Data\nWe will use Llama2 paper for this experiment. Let‚Äôs download the paper.\n!wget --user-agent ""Mozilla"" ""https://arxiv.org/pdf/2307.09288.pdf"" -O ""llama2.pdf""\n# Load the Data\nLet‚Äôs load the data. We will use Pages from start to 36 for the experiment which excludes table of contents, references, and appendix.\nThis data was then parsed by converted to nodes, which represent chunks of data we‚Äôd like to retrieve. We did use chunk_size as 512.\ndocuments = SimpleDirectoryReader(input_files=[""llama2.pdf""]).load_data()\n\nnode_parser = SimpleNodeParser.from_defaults(chunk_size=512)\nnodes = node_parser.get_nodes_from_documents(documents)\n# Generating Question-Context Pairs:\nFor evaluation purposes, we created a dataset of question-context pairs. This dataset can be seen as a set of questions and their corresponding context from our data. To remove bias for the evaluation of embedding(OpenAI/ CohereAI) and Reranker (CohereAI), we use Anthropic LLM to generate Question-Context Pairs.\nLet‚Äôs initialize a prompt template to generate question-context pairs.\n# Prompt to generate questions\nqa_generate_prompt_tmpl = """"""\\\nContext information is below.\n\n---------------------\n{context_str}\n---------------------\n\nGiven the context information and not prior knowledge.\ngenerate only questions based on the below query.\n\nYou are a Professor. Your task is to setup \\\n{num_questions_per']",Mean Reciprocal Rank (MRR) evaluates the system‚Äôs accuracy by looking at the rank of the highest-placed relevant document. It is the average of the reciprocals of these ranks across all the queries.,simple,"[{'title': 'Boosting RAG: Picking the Best Embedding & Reranker models', 'link': 'https://www.llamaindex.ai/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83', 'date': 'Nov 3, 2023', 'tags': ""['Embedding', 'LLM', 'OpenAI', 'Search', 'Llamaindex']""}]",True
3,What role does Retrieval-Augmented Generation (RAG) play in enhancing the processing of large documents with LLMs?,"['Despite recent motivation to utilize NLP for wider range of real world\n  applications, most NLP papers, tasks and pipelines assume raw, clean texts.\n  However, many texts we encounter in the wild, including a vast majority of\n  legal documents (e.g., contracts and legal codes), are not so clean, with many\n  of them being visually structured documents (VSDs) such as PDFs. PDFs are\n  versatile, preserving the visual integrity of documents, but they often pose a\n  significant challenge when it comes to extracting and manipulating their\n  contents.\nIn this discussion, our focus will primarily be on text-only layered PDFs, a\n  category often regarded by many as a resolved issue.\n# Complexity of Parsing PDFs\n- Layout Complexity: PDFs can contain complex layouts, such\n    as multi-column text, tables, images, and intricate formatting. This layout\n    diversity complicates the extraction of structured data.\n- Font encoding issues: PDFs use a variety of font encoding\n    systems, and some of these systems do not map directly to Unicode. This can\n    make it difficult to extract the text accurately.\n- Non-linear text storage: PDFs do not store text in the\n    order it appears on the page. Instead, they store text in objects that can\n    be placed anywhere on the page. This means that the order of the text in the\n    underlying code may not match the order of the text as it appears visually.\n- Inconsistent use of spaces: In some PDFs, spaces are not\n    used consistently or are not used at all between words. This can make it\n    difficult to even identify word boundaries.\n\n# Do we need an efficient parser?\nIn the Age of LLMs, is an Efficient Parser Still Essential When LLMs Can\n    Process Entire PDFs?\nThis question gains relevance if the answer to this next question is ‚ÄúYes‚Äù.\nDo we need Retrieval-Augmented Generation (RAG)?\nWhile LLMs are powerful, they have certain limitations in terms of the amount\n  of text they can process at once and the scope of information they can\n  reference. Further recent research have suggested LLM performance is often\n  highest when relevant information occurs at the beginning or end of the input\n  context, and significantly degrades when models must access relevant\n  information in the middle of long contexts. Techniques like RAG help overcome\n  these limitations, enabling more effective and efficient processing of large\n  documents and broader information retrieval.\nStill Skeptical? Let‚Äôs ask an LLM for confirmation.\n\nNow that we‚Äôve established the importance of an efficient parser, it becomes\n  instrumental in constructing an effective Retrieval-Augmented Generation (RAG)\n  pipeline to address the limitations of an LLM. Let‚Äôs explore how we are\n  achieving this today. It‚Äôs crucial to remember that the quality of the context\n  fed to an LLM is the cornerstone of an effective RAG, as the saying goes,\n  ‚ÄòGarbage In ‚Äî Garbage Out.‚Äô\nIn the context of building LLM-related applications,\n  chunking is the process of breaking down large pieces of text\n  into smaller segments. It‚Äôs an essential technique that helps optimize the\n  relevance of the content we get back from a database once we use the LLM to\n  embed content. Some of the strategies involved are\n- Fixed-size chunking. This is the most common and\n    straightforward approach to chunking: we simply decide the number of tokens\n    in our chunk and, optionally, whether there should be any overlap between\n    them. Easy to implement & most commonly used, but never makes it to a\n    production setting because the output is satisfactory in a Proof of Concept\n    (POC) setup, but its accuracy degrades as we conduct further testing.\n- ‚ÄúContent-aware‚Äù chunking. Set of methods for taking\n    advantage of the nature of the content we‚Äôre chunking and applying more\n    sophisticated chunking to it. Challenging to implement due to the reasons\n    mentioned above, but if tackled correctly, it could be the most ideal\n    building block for a production-grade Information Retrieval (IR) engine.\n\n# Where‚Äôs This Article Headed, Anyway?\nCertainly, let‚Äôs put an end to the historical and background details, shall\n  we?\nIntroducing\n  LayoutPDFReader for ‚ÄúContext-aware‚Äù chunking.\n  LayoutPDFReader\n ']","Retrieval-Augmented Generation (RAG) plays a crucial role in enhancing the processing of large documents with LLMs by helping to overcome the limitations of LLMs, such as the amount of text they can process at once and the scope of information they can reference. RAG techniques enable more effective and efficient processing of large documents and broader information retrieval, particularly when relevant information is located at the beginning or end of the input context.",simple,"[{'title': 'Mastering PDFs: Extracting Sections, Headings, Paragraphs, and Tables with Cutting-Edge Parser', 'link': 'https://www.llamaindex.ai/blog/mastering-pdfs-extracting-sections-headings-paragraphs-and-tables-with-cutting-edge-parser-faea18870125', 'date': 'Oct 18, 2023', 'tags': ""['NLP', 'Large Language Models', 'Llamaindex', 'Pdf', 'Rag']""}]",True
4,What are the main steps performed by the finetuning script on the parsed dataset?,"['LaMa\nThe first step here is to open up the Jupyter notebook. The notebook is organized into a series of runnable scripts that each perform the steps needed to load data.\nOur code uses Modal for every step of the orchestration, and Modal is best used on top of the Python scripts themselves. That is why a lot of these cells don‚Äôt contain Python blocks of their own.\nFirst we use Modal to load in the b-mc2/sql-create-context dataset. This is a simple task that just loads in the dataset and formats it into a .jsonl file.\nmodal run src.load_data_sql --data-dir ""data_sql""\nAs we can see, under the hood the task is quite straightforward:\n# Modal stubs allow our function to run remotely\n@stub.function(\n    retries=Retries(\n        max_retries=3,\n        initial_delay=5.0,\n        backoff_coefficient=2.0,\n    ),\n    timeout=60 * 60 * 2,\n    network_file_systems={VOL_MOUNT_PATH.as_posix(): output_vol},\n    cloud=""gcp"",\n)\ndef load_data_sql(data_dir: str = ""data_sql""):\n    from datasets import load_dataset\n\n    dataset = load_dataset(""b-mc2/sql-create-context"")\n\n    dataset_splits = {""train"": dataset[""train""]}\n    out_path = get_data_path(data_dir)\n\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n\n    for key, ds in dataset_splits.items():\n        with open(out_path, ""w"") as f:\n            for item in ds:\n                newitem = {\n                    ""input"": item[""question""],\n                    ""context"": item[""context""],\n                    ""output"": item[""answer""],\n                }\n                f.write(json.dumps(newitem) + ""\\n"")\n## Step 2: Run Finetuning Script\nThe next step is to run our finetuning script on the parsed dataset.\nmodal run src.finetune_sql --data-dir ""data_sql"" --model-dir ""model_sql""\nThe finetuning script performs the following steps.\nSplits the dataset into training and validation splits\ntrain_val = data[""train""].train_test_split(test_size=val_set_size, shuffle=True, seed=42)\ntrain_data = train_val[""train""].shuffle().map(generate_and_tokenize_prompt)\nval_data = train_val[""test""].shuffle().map(generate_and_tokenize_prompt)\nFormats each split into tuples of (input prompt, label): The input query and context are formatted into the same input prompt. The input prompt is then tokenized, and the labels are set to the exact same as the input prompt ‚Äî this allows the model to train on next-token prediction.\ndef generate_and_tokenize_prompt(data_point):\n  full_prompt = generate_prompt_sql(\n      data_point[""input""],\n      data_point[""context""],\n      data_point[""output""],\n  )\n  tokenized_full_prompt = tokenize(full_prompt)\n  if not train_on_inputs:\n      raise NotImplementedError(""not implemented yet"")\n  return tokenized_full_prompt\nThe input prompt is the exact same as what was given at the top of this blog.\nWhen the finetuning script is run, the model is saved in the remote cloud directory specified by model_dir (which is set to a default value if not']","The main steps performed by the finetuning script on the parsed dataset include splitting the dataset into training and validation splits, shuffling the data, mapping the data to generate and tokenize prompts, and formatting each split into tuples of (input prompt, label) for training on next-token prediction.",simple,"[{'title': 'Easily Finetune Llama 2 for Your Text-to-SQL Applications', 'link': 'https://www.llamaindex.ai/blog/easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d', 'date': 'Aug 17, 2023', 'tags': ""['Llamaindex', 'Fine Tuning', 'Large Language Models', 'AI', 'NLP']""}]",True
5,What is the total count of text documents extracted from the HTML files?,"['```\\n""\n            elif element.name == ""img"":\n                img_src = element.get(""src"")\n                img_caption = element.find_next(""figcaption"")\n                caption_text = img_caption.get_text().strip() if img_caption else """"\n                images.append(ImageDocument(image_url=img_src))\n            elif element.name in [""p"", ""span"", ""a""]:\n                current_section[""content""] += element.get_text().strip() + ""\\n""\n\n    if current_section[""content""].strip():\n        sections.append(current_section)\n\n    return images, sections\nI won‚Äôt go into details for the parsing code, but we split the text based on headers h1‚Äìh4 and extract image links. Then, we simply run all the articles through this function to extract all relevant information.\nall_documents = []\nall_images = []\n\n# Directory to search in (current working directory)\ndirectory = os.getcwd()\n\n# Walking through the directory\nfor root, dirs, files in os.walk(directory):\n    for file in files:\n        if file.endswith("".html""):\n            # Update the file path to be relative to the current directory\n            images, documents = process_html_file(os.path.join(root, file))\n            all_documents.extend(documents)\n            all_images.extend(images)\n\ntext_docs = [Document(text=el.pop(""content""), metadata=el) for el in all_documents]\nprint(f""Text document count: {len(text_docs)}"") # Text document count: 252\nprint(f""Image document count: {len(all_images)}"") # Image document count: 328\nWe get a total of 252 text chunks and 328 images. It‚Äôs a bit surprising that I created so many photos, but I know that some are only images of table results. We could use a vision model to filter out irrelevant photos, but I skipped this step here.\n## Indexing data vectors\nAs mentioned, we have to instantiate two vector stores, one for images and the other for text. The CLIP embedding model has a dimension of 512, while the ada-002 has 1536 dimension.\ntext_store = Neo4jVectorStore(\n    url=NEO4J_URI,\n    username=NEO4J_USERNAME,\n    password=NEO4J_PASSWORD,\n    index_name=""text_collection"",\n    node_label=""Chunk"",\n    embedding_dimension=1536\n)\nimage_store = Neo4jVectorStore(\n    url=NEO4J_URI,\n    username=NEO4J_USERNAME,\n    password=NEO4J_PASSWORD,\n    index_name=""image_collection"",\n    node_label=""Image"",\n    embedding_dimension=512\n\n)\nstorage_context = StorageContext.from_defaults(vector_store=text_store)\nNow that the vector stores have been initiated, we use the MultiModalVectorStoreIndex to index both modalities of information we have.\n# Takes 10 min without GPU / 1 min with GPU on Google collab\nindex = MultiModalVectorStoreIndex.from_documents(\n    text_docs + all_images, storage_context=storage_context, image_vector_store=image_store\n)\nUnder the hood, MultiModalVectorStoreIndex uses text and image embedding models to calculate the embeddings and store and index the results in Neo4j. Only the URLs are stored for images, not actual base64 or other representations of images.\n## Multimodal RAG pipeline\nThis piece']",The total count of text documents extracted from the HTML files is 252.,simple,"[{'title': 'Multimodal RAG pipeline with LlamaIndex and Neo4j', 'link': 'https://www.llamaindex.ai/blog/multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206', 'date': 'Dec 18, 2023', 'tags': ""['Neo4j', 'LLM', 'Llamaindex', 'ChatGPT', 'Graph Database']""}]",True
6,What is the significance of knowledge exchange in the context of the llama-index-networks package?,"['The main premise behind RAG is the injection of context (or knowledge) to the LLM in order to yield more accurate responses from it. As such, a crucial component to a RAG system is the data source from which it gets its knowledge. It‚Äôs intuitive then to reason that the more knowledge that the RAG system can tap into, the better it would ultimately become (in terms of answering queries of potentially both depth and breadth). The spirit of this concept is not so different from that found in basically every other data-driven discipline ‚Äî access to more (good) data, that is subsequently and effectively used, usually leads to better outcomes.\nIt is with that backdrop, that we‚Äôre thrilled to announce the release of our newest latest llama-index library extension, called llama-index-networks. This library extension makes it possible to build a network of RAGs built on top of external data sources and supplied by external actors. This new network paradigm allows for a new way for data suppliers to provide their data to those who want it in order to build more knowledgeable systems!\nIn this blog post, we‚Äôll introduce the main classes of the new extension library and show you how in just a couple lines of code, you can make your QueryEngine ready to contribute as part of a network of RAGs. We‚Äôll also share ideas of what this can mean for how data suppliers actually supply their data to consumers within this new era of LLMs.\nA note on terminology: in this post, we use llama-index-networks to refer to the actual extension, whereas llama-index[networks] refers to an installation of llama-index that comes with the llama-index-networks extension.\n## The story of Alex, Beth, and Bob\nAn illustrative example of actors in a network and their problem statements.\nTo illustrate how the llama-index-networks package can be used, we consider three made-up characters, Alex, Bob, and Beth, and the following scenario:\n- Both Bob and Beth each have their own set of documents and both have already built quite the outstanding RAG systems over them (using llama-index of course!)\n- Alex has heard about these insightful documents that both Bob and Beth have and would like to be able to query the individual RAGs built over the both of them.\n- Bob and Beth, being as kind as they are (or, perhaps they were paid some undisclosed dollar amount), agree to give access to Alex to their RAG systems.\n\nTo facilitate this new fashion of knowledge exchange, they agree to setup a network of RAGs that Alex can query.\n## Bob and Beth build a web service over their RAGs\nContributorService is built around a QueryEngine.\nWith the llama-index-networks package, Bob and Beth can make their respective QueryEngine‚Äôs ready to participate in the network with only a few lines of code.\n""""""Beth\'s contributor service file.\n\nBeth builds her QueryEngine and exposes it behind the standard\nLlamaIndex Network Contributor Service. \n\nNOTE: Bob would probably make use of Docker and cloud \ncompute services to make this production grade.\n""""""\n\nfrom llama_index.networks.contributor import ContributorService\nimport uvicorn\n\nbeth_query_engine = ...\nbeth_contributor_service = ContributorService.from_config_file(\n    "".env.contributor"",  # settings/secrets for the service\n    beth_query_engine\n)\n\n\nif __name__ == ""__main__:\n    uvicorn.run(beth_contributor_service.app, port=8000)\nBob would use similar lines of code to make his QueryEngine ready to contribute to any LlamaIndex network. Note, that the dotenv file .env.contributor contains settings for the service as well as any necessary api keys (e.g., OPENAI_API_KEY or ANTHROPIC_API_KEY), which under the hood is implemented as REST service using FastAPI.\n## Alex builds a NetworkQueryEngine\nAlex builds a NetworkQueryEngine that connects to Beth and Bob‚Äôs individual ContributorService‚Äôs.\nFor Alex‚Äôs part, he uses the NetworkQueryEngine class of the llama-index-networks extension to be able to connect to both Beth and Bob‚Äôs ContributorService‚Äôs.\n""""""Alex\'s network query engine.\n\nAlex builds a NetworkQueryEngine to connect to a \nlist of ContributorService‚Äôs.\n""""""\n\nfrom llama_index.networks.contributor import ContributorClient\nfrom llama_index.networks.']","The significance of knowledge exchange in the context of the llama-index-networks package lies in its ability to facilitate a network of RAGs, allowing data suppliers like Bob and Beth to share their insightful documents and RAG systems with others, such as Alex. This exchange enhances the knowledge base available to users, leading to more knowledgeable systems and better outcomes in answering queries.",simple,"[{'title': 'Querying a network of knowledge with llama-index-networks', 'link': 'https://www.llamaindex.ai/blog/querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f', 'date': 'Feb 27, 2024', 'tags': ""['LLM', 'Retrieval Augmented', 'AI', 'Distributed', 'Machine Learning']""}]",True
7,What is the significance of multi-turn systems in coordinating multiple agents?,"[' pauses for more user input.\nAnd that\'s the full system! The sub-agents can be arbitrarily complicated, multi-turn systems in themselves, and the outer loop doesn\'t need to know how they work, just how they depend on each other.\n## What\'s next\nWe think there\'s some novel stuff in here: coordinating multiple agents ""speaking"" simultaneously, creating implicit ""chains"" of agents through natural language instructions, using a ""continuation"" agent to manage those chains, and using a global state this way. We\'re excited to see what you do with the patterns we\'ve laid out here. Don\'t forget to check out the open-source repo!\n\n']","The significance of multi-turn systems in coordinating multiple agents lies in their ability to manage complex interactions where agents can communicate and depend on each other through natural language instructions, creating implicit chains of agents. This coordination allows for more sophisticated and efficient problem-solving.",simple,"[{'title': 'Building a multi-agent concierge system', 'link': 'https://www.llamaindex.ai/blog/building-a-multi-agent-concierge-system', 'date': 'Jul 17, 2024', 'tags': ""['llama-agents', 'Agents']""}]",True
8,What environment variables need to be set for the production Slack bot to function properly?,"[' any region is fine\n- Branch: main\n- Root directory: (blank, meaning root)\n- Runtime: Python 3\n- Build command: poetry install\n- Start command: gunicorn app:flask_app (this will definitely need to be set)\n\nYou‚Äôll also need to scroll down and set some environment variables:\n- PYTHON_VERSION: 3.11.6 (or whatever version you‚Äôre using)\n- OPENAI_API_KEY: your OpenAI API key\n- SLACK_BOT_TOKEN: your Slack bot token\n- SLACK_SIGNING_SECRET: your Slack signing secret from before\n\nThen click deploy and away you go!\n\nYou now have a production Slack bot listening to messages, remembering, learning, and replying. Congratulations!\n# What next?\nThere‚Äôs a whole bunch of features you could add to this bot, roughly in increasing order of difficulty:\n- Join every channel instead of just one, clearly!\n- Add a way to tell the bot to forget things (delete nodes)\n- Give the bot the ability to use more than one index, such as an index of your documentation, or connected to your email, or your calendar\n- Give the bot ‚Äútags‚Äù so it can attach metadata to nodes and answer questions only with (or ignore) things that have been tagged a certain way\n- Add multi-modal abilities, so the bot can read images and even reply with generated images\n- And tons more!\n\nThis bot is a lot of fun to play with and was a lot of fun to build, I hope you enjoyed learning about Slackbots and LlamaIndex as much as I enjoyed writing this tutorial!\n']","The environment variables that need to be set for the production Slack bot to function properly are: PYTHON_VERSION, OPENAI_API_KEY, SLACK_BOT_TOKEN, and SLACK_SIGNING_SECRET.",simple,"[{'title': 'Building a Slack bot that learns with LlamaIndex, Qdrant and Render', 'link': 'https://www.llamaindex.ai/blog/building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840', 'date': 'Jan 25, 2024', 'tags': ""['LLM', 'Qdrant', 'Render', 'AI', 'Slack']""}]",True
9,What is the process of video creation using D-ID in the knowledge transfer system?,"[""# Introduction:\nIn the world of IT and Software Development, knowledge transfer (KT) stands out as a big challenge. Whether it‚Äôs new hires trying to understand their roles, folks on their notice periods aiming for a smooth handover, or the daily tasks of developers and product specialists adapting to ever-changing projects ‚Äî the KT process often leads to stress and worry.\nThis gets more complicated with information spread out everywhere, the mix of new and old tech, and the fast pace of IT and Software Development projects. In this situation, broken bits of knowledge become the norm, causing delays, misunderstandings, and making learning harder.\nBut amidst these challenges, might there be a beacon of optimism shining through?\nVibhav and I have developed a system that seamlessly organizes KT sessions. By leveraging personal images, we generate video explanations that are paired with individual code snippets, making the code far more comprehensible. Our innovative approach was recognized when we secured the First Prize at the Google Cloud, Searce, and LifeSight hackathon. With the combined strengths of LlamaIndex and D-ID, our aim is not just to consolidate information but also to simplify tasks and elevate the KT process. In doing so, we‚Äôre transforming a daunting industry challenge into a straightforward and manageable endeavor.\nWant to see how LlamaIndex plays a key role in this change?\nLet‚Äôs dive in together!\n# Solution:\nThe solution has four stages:\n## Code Parsing:\n- Break down the code base into individual code snippets or blocks.\n\n## Summary and Explanation Generation with LlamaIndex:\n- Produce a comprehensive summary of the entire code base.\n- Create detailed explanations for each individual code block using LlamaIndex.\n\n## Video Creation with D-ID:\n- Generate videos using text-to-speech capabilities provided by D-ID.\n\n## Video-Code Integration:\n- Seamlessly stitch together the individual code blocks with their corresponding generated videos.\n\nLet‚Äôs dive into each stage in detail.\n\n## 1. Code Parsing: Breaking Down the Code\nCode Parser\nUnderstanding a code base starts with a high-level summary, but the true depth\n  lies in individual snippets or blocks. However, using entire code bases for\n  explanations can overwhelm language models like LLMs, causing them to either\n  exceed token limits or miss key details.\nOur approach is simple yet efficient: break the code into digestible sections\n  like import statements, classes, initializer functions, and methods without\n  losing the code‚Äôs flow. This segmentation is done through a dependency graph\n  approach, utilizing Python‚Äôs\n  ast library. By analyzing the code's structure, we can extract\n  classes, their docstrings, initializers, and other methods. This method not\n  only captures the essence of each segment but is also flexible, allowing for\n  further rules to extract additional code components.\nThe code_parser class embodies this strategy. It navigates the\n  code, distinguishing module-level functions from class-nested ones, and\n  arranges them systematically. The result? A granular yet comprehensive view of\n  the code, paving the way for precise and context-rich explanations.\n## 2. Summary and Explanation Generation with LlamaIndex\nProducing a Comprehensive Summary:\nThe initial step in understanding a code base is to grasp its overall essence.\n  This is achieved by generating a concise summary that gives a bird‚Äôs-eye view\n  of the entire code. LlamaIndex‚Äôs SummaryIndex\n  has been tailored for this exact task. In SummaryIndex, each block of code is\n  treated as a node. By inputting the structured blocks obtained from our code\n  parsing phase into SummaryIndex, we can produce a comprehensive snapshot that\n  serves as a summary of the entire code base.\n\nDetailed Explanations for Individual Code Blocks:\nWith a general understanding established, the next step is to delve into the\n  finer details. Starting from import statements, progressing to functions, and\n  eventually diving into classes and initializer functions, every block gets its\n  due attention. Here, LlamaIndex‚Äôs\n  accumulateresponse mode is a valuable asset, providing in-depth\n  explanations for each block.\nHowever, a challenge arises. While\n  accumulate provides in-depth insights into each block, it can\n  occasionally miss the broader context offered by preceding blocks. To address\n  this limitation, we‚Äôve adopted a two-pronged approach. As depicted in the\n  subsequent architecture, we employ two SummaryIndices for this endeavor.\n- We utilize the first SummaryIndex to generate a concise summary for each\n    block, treating each block as a Node in SummaryIndex.\n- For the second SummaaryIndex in the stack""]",The process of video creation using D-ID in the knowledge transfer system involves generating videos using text-to-speech capabilities provided by D-ID. This step follows the generation of detailed explanations for individual code blocks and is part of the overall solution that integrates video with corresponding code snippets.,simple,"[{'title': 'LlamaIndex: Automatic Knowledge Transfer (KT) Generation for Code Bases', 'link': 'https://www.llamaindex.ai/blog/llamaindex-automatic-knowledge-transfer-kt-generation-for-code-bases-f3d91f21b7af', 'date': 'Aug 29, 2023', 'tags': ""['Large Language Models', 'Llamaindex', 'AI', 'NLP', 'Software Development']""}]",True
10,What components are involved in generating a RAG pipeline?,"[' storing and retrieving embeddings in the RAG pipeline. This function supports the selection from multiple vector store options including Simple (Llamaindex default), Pinecone and Qdrant.\n\ndef select_vector_store():\n    st.header(""Choose Vector Store"")\n    vector_stores = [""Simple"", ""Pinecone"", ""Qdrant""]\n    selected_store = st.selectbox(""Select Vector Store"", vector_stores, on_change=reset_pipeline_generated)\n\n    vector_store = None\n    if selected_store == ""Pinecone"":\n        pc = Pinecone(api_key=os.environ[\'PINECONE_API_KEY\'])\n        index = pc.Index(""test"")\n        vector_store = PineconeVectorStore(pinecone_index=index)\n    elif selected_store == ""Qdrant"":\n        client = qdrant_client.QdrantClient(location="":memory:"")\n        vector_store = QdrantVectorStore(client=client, collection_name=""sampledata"")\n    st.write(selected_store)\n    return vector_store, selected_store\n## generate_rag_pipeline Function\nThis core function ties together the selected components to generate a RAG pipeline. It initializes the pipeline with the chosen LLM, embedding model, node parser, response synthesis method, and vector store. It is triggered by pressing the ‚ÄòGenerate RAG Pipeline‚Äô button.\ndef generate_rag_pipeline(file, llm, embed_model, node_parser, response_mode, vector_store):\n    if vector_store is not None:\n        # Set storage context if vector_store is not None\n        storage_context = StorageContext.from_defaults(vector_store=vector_store)\n    else:\n        storage_context = None\n\n    # Create the service context\n    service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model, node_parser=node_parser)\n\n    # Create the vector index\n    vector_index = VectorStoreIndex.from_documents(documents=file, storage_context=storage_context, service_context=service_context, show_progress=True)\n    if storage_context:\n        vector_index.storage_context.persist(persist_dir=""persist_dir"")\n\n    # Create the query engine\n    query_engine = vector_index.as_query_engine(\n        response_mode=response_mode,\n        verbose=True,\n    )\n\n    return query_engine\n## generate_code_snippet Function\nThis function is the culmination of the user‚Äôs selections, generating the Python code necessary to implement the configured RAG pipeline. It dynamically constructs the code snippet based on the chosen LLM, embedding model, node parser, response synthesis method, and vector store, including the parameters set for the node parser.\ndef generate_code_snippet(llm_choice, embed_model_choice, node_parser_choice, response_mode, vector_store_choice):\n    node_parser_params = st.session_state.get(\'node_parser_params\', {})\n    print(node_parser_params)\n    code_snippet = ""from llama_index.llms import OpenAI, Gemini, Cohere\\n""\n    code_snippet += ""from llama_index.embeddings import HuggingFaceEmbedding\\n""\n    code_snippet += ""from llama_index import ServiceContext, VectorStoreIndex, StorageContext\\n""\n    code_snippet += ""from llama_index.node_parser import SentenceSplitter, CodeSplitter, SemanticSplitterNodeParser, TokenTextSplitter\\n""\n    code_snippet += ""from llama_index.node_parser.file import HTMLNodeParser, JSONNodeParser, MarkdownNodeParser\\n""\n    code_snippet += ""from llama_']","The components involved in generating a RAG pipeline include the chosen LLM (Language Model), embedding model, node parser, response synthesis method, and vector store.",simple,"[{'title': 'RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex', 'link': 'https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089', 'date': 'Feb 2, 2024', 'tags': ""['Rag', 'No Code', 'Llamaindex', 'OpenAI', 'Code Generation']""}]",True
11,What is the purpose of creating a Vector Store Index in the context of evaluating response time and accuracy?,"['(chunk_size, eval_questions)\n  print(f""Chunk size {chunk_size} - Average Response time: {avg_response_time:.2f}s, Average Faithfulness: {avg_faithfulness:.2f}, Average Relevancy: {avg_relevancy:.2f}"")\n# Bringing It All Together\nLet‚Äôs compile the processes:\nimport nest_asyncio\n\nnest_asyncio.apply()\n\nfrom llama_index import (\n    SimpleDirectoryReader,\n    VectorStoreIndex,\n    ServiceContext,\n)\nfrom llama_index.evaluation import (\n    DatasetGenerator,\n    FaithfulnessEvaluator,\n    RelevancyEvaluator\n)\nfrom llama_index.llms import OpenAI\n\nimport openai\nimport time\n\nopenai.api_key = \'OPENAI-API-KEY\'\n\n# Download Data\n!mkdir -p \'data/10k/\'\n!wget \'https://raw.githubusercontent.com/jerryjliu/llama_index/main/docs/examples/data/10k/uber_2021.pdf\' -O \'data/10k/uber_2021.pdf\'\n\n# Load Data\nreader = SimpleDirectoryReader(""./data/10k/"")\ndocuments = reader.load_data()\n\n# To evaluate for each chunk size, we will first generate a set of 40 questions from first 20 pages.\neval_documents = documents[:20]\ndata_generator = DatasetGenerator.from_documents()\neval_questions = data_generator.generate_questions_from_nodes(num = 20)\n\n# We will use GPT-4 for evaluating the responses\ngpt4 = OpenAI(temperature=0, model=""gpt-4"")\n\n# Define service context for GPT-4 for evaluation\nservice_context_gpt4 = ServiceContext.from_defaults(llm=gpt4)\n\n# Define Faithfulness and Relevancy Evaluators which are based on GPT-4\nfaithfulness_gpt4 = FaithfulnessEvaluator(service_context=service_context_gpt4)\nrelevancy_gpt4 = RelevancyEvaluator(service_context=service_context_gpt4)\n\n# Define function to calculate average response time, average faithfulness and average relevancy metrics for given chunk size\ndef evaluate_response_time_and_accuracy(chunk_size):\n    total_response_time = 0\n    total_faithfulness = 0\n    total_relevancy = 0\n\n    # create vector index\n    llm = OpenAI(model=""gpt-3.5-turbo"")\n    service_context = ServiceContext.from_defaults(llm=llm, chunk_size=chunk_size)\n    vector_index = VectorStoreIndex.from_documents(\n        eval_documents, service_context=service_context\n    )\n\n    query_engine = vector_index.as_query_engine()\n    num_questions = len(eval_questions)\n\n    for question in eval_questions:\n        start_time = time.time()\n        response_vector = query_engine.query(question)\n        elapsed_time = time.time() - start_time\n        \n        faithfulness_result = faithfulness_gpt4.evaluate_response(\n            response=response_vector\n        ).passing\n        \n        relevancy_result = relevancy_gpt4.evaluate_response(\n            query=question, response=response_vector\n        ).passing\n\n        total_response_time += elapsed_time\n        total_faithfulness += faithfulness_result\n        total_relevancy += relevancy_result']","The purpose of creating a Vector Store Index in the context of evaluating response time and accuracy is to enable efficient querying of the documents, allowing for the assessment of response times and the evaluation of the quality of responses based on faithfulness and relevancy metrics.",simple,"[{'title': 'Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex', 'link': 'https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5', 'date': 'Oct 5, 2023', 'tags': ""['Llamaindex', 'AI', 'LLM', 'OpenAI', 'Retrieval']""}]",True
12,What is the purpose of LlamaFS in organizing files?,"[""Greetings, LlamaIndex Family! ü¶ô\nWelcome to your latest weekly update from LlamaIndex! We're excited to present a variety of outstanding integration updates, detailed guides, demos, educational tutorials, and informative webinars this week.\n## ü§©\xa0The highlights:\n- Secure Code Execution with AzureCodeInterpreterTool: Securely run LLM-generated code with Azure Container Apps, integrated with LlamaIndex for safe code execution.\n- Build Automated Email Agents: Create email agents with MultiOn and LlamaIndex that autonomously read, index, and respond to emails.\n- LlamaFS for Organized Files: Alex Reibman's team developed LlamaFS to automatically structure messy file directories, enhanced by Llama 3 and Groq Inc.'s API.\n- RAGApp's No-Code Chatbots: Deploy RAG chatbots easily with RAGApp's no-code interface, fully open-source and cloud-compatible.\n\n## ‚ú® Feature Releases and Enhancements:\n- We have launched Azure Container Apps dynamic sessions to securely run LLM-generated code in a sandbox. Integrated into LlamaIndex, this feature ensures safe execution of complex code tasks by your agents. Set up a session pool on Azure, add the AzureCodeInterpreterTool to your agent, and you‚Äôre ready to go. Blogpost, Tweet.\n- We have integrated with the open source Nomic embed, now fully operable locally. This integration allows for completely local embeddings and introduces a dynamic inference mode that optimizes embedding latency. The system automatically selects between local and remote embeddings based on speed, ensuring optimal performance. Docs, Tweet.\n- We have integrated the Vespa vector store, supporting hybrid search with BM25. Docs, Tweet.\n- We have integrated with MyMagic AI to facilitate batch data processing for GenAI applications. This setup allows you to pre-process large datasets with an LLM, enabling advanced analysis and querying capabilities. Docs, Tweet.\n\n## üó∫Ô∏è Guides:\n- Guide to building an automated Email Agent with MultiOn and LlamaIndex that can autonomously read and index emails for easy retrieval and draft responses using advanced browsing capabilities.\n- Guide to building Full-Stack Job Search Assistant by Rishi Raj Jain using Gokoyeb, MongoDB, and LlamaIndex. This guide takes you through setting up MongoDB Atlas, crafting a Next.js application, developing UI components, and deploying your app on Koyeb, complete with real-time response streaming and continuous job updates.\n\n## üñ•Ô∏è\xa0Demos:\n- LlamaFS, a project developed by Alex Reibman and his team, automatically organizes messy file directories into neatly structured folders with interpretable names. Enhanced by Llama 3 and supported by Groq Inc.'s API, Ollama's fully local mode and LlamaIndex, this tool significantly improves file management efficiency. Code, Tweet.\n- RAGApp, a project developed by Marcus Schiesser, offers a no-code interface for configuring RAG chatbots as simply as GPTs by OpenAI. This fully open-source docker container can be deployed on any cloud platform, allowing users to set up the LLM, define system prompts, upload knowledge bases, and launch chatbots via UI or API. Code, Tweet.\n\n## ‚úçÔ∏è Tutorials:\n- Phil Chirchir‚Äôs tutorial on DSPy RAG with LlamaIndex. It demonstrates how to integrate DSPy bootstrapping models with a LlamaIndex RAG pipeline powered by LlamaParse.\n- Pavan Kumar‚Äôs tutorial on advanced image indexing for RAG demonstrates how to combine image embeddings with structured annotations using multimodal models. It details how to enhance image search with LlamaIndex and Qdrant Engine‚Äôs capabilities.\n- Jayita Bhattacharyya‚Äôs tutorial on Building a RAG Chatbot using Llamaindex, Groq with Llama3 & Chainlit.\n\n## üìπ\xa0Webinar:\n- Webinar with OpenDevin team to learn how to build an Open-Source Coding Assistant using OpenDevin.\n\n""]","LlamaFS is designed to automatically structure messy file directories into neatly organized folders with interpretable names, significantly improving file management efficiency.",simple,"[{'title': 'LlamaIndex Newsletter 2024-05-28', 'link': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-05-28', 'date': 'May 28, 2024', 'tags': '[]'}]",True
13,What improvements were made to the PDF parsing service released by LlamaIndex?,"['Greetings, LlamaIndex devotees! ü¶ô\nIt was another fun week to be at the center of the LLM universe, and we have tons to share!\nü§©\xa0The highlights:\n- We shared our thoughts on the future of long-context RAG. As LLMs with context windows over 1M tokens begin to appear, what changes about RAG, and how will LlamaIndex evolve? Tweet, Blog post\n- llama-index-networks lets you build a super-RAG application by combining answers from independent RAG apps over the network. Tweet, Blog post, repo\n- People loved our release of LlamaParse, a world-beating PDF parsing service, so we made it even better! Tweet, blog post\n\n‚ú® Feature Releases and Enhancements:\n- We released a new llama-index-networks feature that lets you combine multiple independent RAG applications over the network, allowing you to run a single query across all the applications and get a single, combined answer. Tweet, Blog post, repo\n- Inference engine Groq wowed us and the world with their incredibly fast query times and we were delighted to introduce first-class support for their LLM APIs. Tweet, notebook\n- Users love LlamaParse, the world-beating PDF parsing service we released last week. We pushed improved parsing and OCR support for 81+ languages! We also increased the usage cap from 1k to 10k pages per day. Tweet, blog post\n- We migrated our blog off of Medium, we hope you like the new look and the absence of nag screens!\n- RAPTOR is a new tree-structured technique for advanced RAG; we turned the paper into a LlamaPack, allowing you to use the new technique in one line of code. Tweet, package, notebook, original paper\n\nüé•\xa0Demos:\n- The Koda Retriever is a new retrieval concept: hybrid search where the alpha parameter controlling the importance of vector search vs. keyword search is tuned on a per-query basis by the LLM itself, based on a few-shot examples. Tweet, notebook, package, blog post\n- Mixedbread.ai released some state-of-the-art rerankers that perform better than anything seen before; we whipped up a quick cookbook to show you how to use them directly in LlamaIndex. Tweet, Notebook, blog post\n\nüó∫Ô∏è Guides:\n- Function-calling cookbook with open source models shows you how to use Fireworks AI‚Äôs OpenAI-compatible API to use all native LlamaIndex support for function calling. Notebook, Tweet.\n- We released a best practices cookbook showing how to use LlamaParse, our amazing PDF parser. Tweet, notebook\n- A comprehensive guide to semantic chunking for RAG by Florian June covers embedding-based chunking, BERT-based chunking techniques, and LLM-based chunking for everything you need to know about this highly effective technique to improve retrieval quality. Tweet, Blog post\n\n‚úçÔ∏è Tutorials:\n- Our own Andrei presented a notebook on building Basic RAG with LlamaIndex at Vector Institute‚Äôs RAG bootcamp. Tweet, Notebook\n- ClickHouse presented an in-depth tutorial using LlamaIndex to query both structured and unstructured data, and built a bot that queries Hacker News to find what people are saying about the most popular technologies. Tweet, blog post\n- POLM (Python, OpenAI, LlamaIndex, MongoDB) is a new reference architecture for building RAG applications and MongoDB has a beautiful, step-by-step tutorial for building it out. Tweet, blog post\n\nüé•\xa0Webinar:\n- Our CEO Jerry Liu will do a joint webinar with Adam Kamor of Tonic.ai about building fully-local RAG applications with Ollama and Tonic. People love local models! Tweet, Registration page\n- Jerry also did a webinar with Traceloop on leveling up your LLM application with observability. Tweet, YouTube\n- Our hackathon at the beginning of February was a huge success! Check out this webinar in which we invited the winners to come and talk about their projects. Tweet, YouTube.\n\n']","Improvements made to the PDF parsing service, LlamaParse, include enhanced parsing and OCR support for 81+ languages, as well as an increased usage cap from 1k to 10k pages per day.",simple,"[{'title': 'LlamaIndex Newsletter 2024-03-05', 'link': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-03-05', 'date': 'Mar 5, 2024', 'tags': ""['LLM', 'Newsletter', 'AI']""}]",True
14,What is the purpose of the CustomRetriever class in retrieval operations?,"['. Define CustomRetriever\nWe will define CustomRetriever class to perform retrieval operations with and without a reranker.\nclass CustomRetriever(BaseRetriever):\n    """"""Custom retriever that performs hybrid search with and without reranker""""""\n\n    def __init__(\n        self,\n        vector_retriever: VectorIndexRetriever,\n        reranker: CohereRerank\n    ) -&gt; None:\n        """"""Init params.""""""\n\n        self._vector_retriever = vector_retriever\n        self._reranker = reranker\n\n    def _retrieve(self, query_bundle: QueryBundle) -&gt; List[NodeWithScore]:\n        """"""Retrieve nodes given query.""""""\n\n        retrieved_nodes = self._vector_retriever.retrieve(query_bundle)\n\n        if self._reranker != None:\n            retrieved_nodes = self._reranker.postprocess_nodes(retrieved_nodes, query_bundle)\n        else:\n            retrieved_nodes = retrieved_nodes[:4]\n\n        return retrieved_nodes\n\n    async def _aretrieve(self, query_bundle: QueryBundle) -&gt; List[NodeWithScore]:\n        """"""Asynchronously retrieve nodes given query.\n\n        Implemented by the user.\n\n        """"""\n        return self._retrieve(query_bundle)\n\n    async def aretrieve(self, str_or_query_bundle: QueryType) -&gt; List[NodeWithScore]:\n        if isinstance(str_or_query_bundle, str):\n            str_or_query_bundle = QueryBundle(str_or_query_bundle)\n        return await self._aretrieve(str_or_query_bundle)\n9. Define functions for retriever evaluation and metrics computation\nWe will look into retriever performance for different alpha values with and without reranker.\n# Alpha values and datasets to test\nalpha_values = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\n\n# Function to evaluate retriever and return results\nasync def evaluate_retriever(alpha, dataset, reranker=None):\n    retriever = VectorIndexRetriever(index,\n                                     vector_store_query_mode=""hybrid"",\n                                     similarity_top_k=10,\n                                     alpha=alpha)\n    custom_retriever = CustomRetriever(retriever,\n                                       reranker)\n\n    retriever_evaluator = RetrieverEvaluator.from_metric_names([""mrr"", ""hit_rate""], retriever=custom_retriever)\n    eval_results = await retriever_evaluator.aevaluate_dataset(dataset)\n    return eval_results\n\n# Function to calculate and store metrics\ndef calculate_metrics(eval_results):\n    metric_dicts = []\n    for eval_result in eval_results:\n        metric_dict = eval_result.metric_vals_dict\n    ']","The purpose of the CustomRetriever class is to perform retrieval operations with and without a reranker, allowing for hybrid search capabilities.",simple,"[{'title': 'LlamaIndex: Enhancing Retrieval Performance with Alpha Tuning in Hybrid Search in RAG', 'link': 'https://www.llamaindex.ai/blog/llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00', 'date': 'Jan 31, 2024', 'tags': ""['Llamaindex', 'LLM', 'OpenAI', 'Retrieval Augmented', 'AI']""}]",True
15,What is the purpose of the Retrieval-Augmented Dual Instruction Tuning (RA-DIT) method in enhancing large language models?,"['## Introduction\nLarge Language Models (LLMs) improve performance by accessing external data for background knowledge tasks related. However, existing approaches require costly modifications during LM‚Äôs pre-training or integrating the data store after the model has been trained. On the downside, both strategies lead to suboptimal performance.\nTo address this problem an AI Research team at Meta has proposed a method called RA-DIT: RETRIEVAL-AUGMENTED DUAL INSTRUCTION TUNING that allows any LLM to be upgraded to include retrieval features.\nIn this blog post, we will explore RA-DIT capabilities to have better performance on Retrieval Augmentation Generation (RAG) through building the dataset and fine-tuning the models.\nThe RA-DIT approach involves two distinct fine-tuning steps:\n- Update a pre-trained LM to better use retrieved information.\n- Update the retriever to return more relevant results\n\n## How it works\nThe RA-DIT approach separately fine-tunes the LLM and the retriever. The LLM is updated to maximize the probability of the correct answer given the retrieval-augmented instructions, while the retriever is updated to minimize how much the document is semantically similar (relevant) to the query.\nBelow we are going through each step from generating the fine-tuning dataset, fine-tuning the language model for better predictions, and refining the retrieval search process.\n## Fine-tuning Dataset\nThe fine-tuning dataset is tailored to enhance the language model‚Äôs ability to leverage knowledge and boost its contextual awareness during prediction generation. Generating Q/A pairs, summarizing data, and incorporating chain-of-thought reasoning can lead to improved results when integrated with the models.\n\nFollowing our LamaIndex implementation, we retrieve the top_k nodes, generate Q/A pairs from the documents, and then augment the data. We use the Q/A pairs through the QueryResponseDataset module, which returns a (query, response) pair for the fine-tuning dataset. While the retrieval fine-tuning data set is created on Q/A pairs data.\n## Language Model Fine-tuning\nWith our fine-tuning dataset in hand, we can refine our LLM to achieve two main benefits: Adapt the LLM to better utilization of relevant background knowledge and train the LLM to produce accurate predictions even with incorrectly retrieved chunks, empowering the model to rely on its own knowledge.\n## Retriever Fine-tuning\n\nThe retriever is fine-tuned using the LM-Supervised Retrieval (LSR) method. In this approach, the LLM assesses the information fetched by the retriever. If the LLM finds the information misaligned with the given query, it sends feedback to the retriever. Using this feedback, the retriever refines its search process, ensuring it fetches data that the LLM can effectively use. This collaboration enhances the overall quality of the answers provided.\n## Evaluation\nTo assess the suggested method, the authors employed specific datasets and metrics. Let‚Äôs delve into each of these to grasp the experimental results better.\n## Metrics\nAn ‚Äúexact match‚Äù (EM) metric was used to measure how closely the model‚Äôs prediction matches the ground truth answer.\n## Dataset\nThe methodology was tested on two distinct tasks:\n- Knowledge-intensive tasks.\n- Commonsense reasoning.\n\nLet‚Äôs explore the datasets utilized for both of these tasks.\n## Knowledge-intensive dataset\nFor knowledge-intensive tasks the selected datasets predominantly focus on the model‚Äôs capacity to access, understand, and relay deep and specific knowledge. They encompass questions rooted in facts, general trivia, and complex domain-specific queries;\nThe datasets used are MMLU, Natural Questions (NQ), TriviaQA, and a subset of tasks from the KILT benchmark.\n## Commonsense reasoning dataset\nCommonsense reasoning datasets challenge the model‚Äôs ability to reason and infer based on general knowledge and everyday scenarios. They contain questions and scenarios that typically don‚Äôt rely on deep domain knowledge but rather on intuitive and general world understanding.\nThe datasets used are BoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC-E, ARC-C, OBQA.\nFor a better understanding of how these datasets were utilized you can check the paper for better understanding.\n## Results\nIn a comparative analysis of model performance on knowledge-intensive (Table 2 below) and commonsense reasoning tasks (Table 3 below), three models were considered:\n- LLAMA 65B\n- LLAMA 65B REPLUG (only retrieval augmentation)\n- RA-DIT 65B\n\n## Knowledge Intensive Tasks\nEvaluations are conducted in 0-shot, 5-shot, and 64']","The purpose of the Retrieval-Augmented Dual Instruction Tuning (RA-DIT) method is to upgrade any large language model (LLM) to include retrieval features, thereby improving its performance on Retrieval Augmentation Generation (RAG) tasks. This is achieved through two distinct fine-tuning steps: updating a pre-trained LLM to better use retrieved information and updating the retriever to return more relevant results.",simple,"[{'title': 'Improving RAG effectiveness with Retrieval-Augmented Dual Instruction Tuning (RA-DIT)', 'link': 'https://www.llamaindex.ai/blog/improving-rag-effectiveness-with-retrieval-augmented-dual-instruction-tuning-ra-dit-01e73116655d', 'date': 'Oct 18, 2023', 'tags': ""['AI', 'Large Language Models', 'Machine Learning', 'Fine Tuning', 'Llamaindex']""}]",True
16,What is the significance of the Gmail API's ability to update draft emails with optional inputs?,"[' to write tools that are more tolerant of their inputs, for example by making inputs optional when the value can be inferred from somewhere else. Take the example of drafting an email, but this time let‚Äôs consider a tool that updates a draft email:\ndef update_draft(\n        self,\n        draft_id: str,\n        to: Optional[List[str]] = None,\n        subject: Optional[str] = None,\n        message: Optional[str] = None,\n    ) -&gt; str:\n        """"""Update a draft email.\n           Print the returned draft\'s message and id.\n           This function is required to be passed a draft_id that is obtained when creating messages\n           Returns: Draft object, including draft id and message meta data.\n\n        Args:\n            draft_id (str): the id of the draft to be updated\n            to (Optional[str]): The email addresses to send the message to\n            subject (Optional[str]): The subject for the event\n            message (Optional[str]): The message for the event\n        """"""\nThe Gmail API requires all of the above values when updating a draft, however using just the draft_id we can fetch the current content of the draft and use the existing values as defaults if the Agent did not provide the values when updating the draft:\ndef update_draft(...):\n  ...\n  draft = self.get_draft(draft_id)\n  headers = draft[\'message\'][\'payload\'][\'headers\']\n  for header in headers:\n      if header[\'name\'] == \'To\' and not to:\n          to = header[\'value\']\n      elif header[\'name\'] == \'Subject\' and not subject:\n          subject = header[\'value\']\n    elif header[\'name\'] == \'Message\' and not message:\n      message = header[\'values\']\n  ...\nBy providing the above logic in the update_draft function, the Agent can invoke update_draft with only one of the fields (and the draft_id), and we can update the draft as the user expects. This means that in more circumstances the Agent can complete the task successfully, instead of returning an error or needing to ask for more information.\n## Validating input and Agent error handling\nDespite best efforts at prompting and tolerance, we can end up in circumstances where the Agent invokes a tool in a way that it can‚Äôt complete the task at hand. However, we can detect this and prompt the Agent to recover the error on its own.\nFor example, in the update_draft example above, what do we do if the agent calls the function without a draft_id? We could simply pass along the null value and return an error from the Gmail API library, but we could also detect that a null draft_id will invariably cause an error, and return a prompt for the agent instead:\ndef update_draft(...):\n  if draft_id == None:\n    return ""You did not provide a draft id when calling this function. If you previously created or retrieved the draft, the id is available in context""\nNow, if the Agent invokes update_draft without a draft_id , it is made aware of the exact mistake it made and given instructions on how it can correct the issue.\nIn my experience working with this tool, the Agent will often immediately call the update_draft function in the correct way when receiving this prompt, or if there is no draft_id available, it will inform the user of the issue and ask the user for a draft_id. Either scenario is much better than crashing or returning an opaque error from a library to the user.\n## Providing simple functions related to the tool\nAgents can struggle at what would otherwise be simple functions for a computer to calculate. For example, when building a tool for creating events in Google Calendar, a user may prompt the Agent with something like this:\nCreate an event on my Calendar to discuss the Tools PR with adam@example.com tomorrow at 4pm\nCan you see the problem? If we try asking ChatGPT what day it is:\nagent']","The significance of the Gmail API's ability to update draft emails with optional inputs lies in its tolerance for incomplete information. By allowing inputs like 'to', 'subject', and 'message' to be optional, the API can fetch existing values from the draft if they are not provided, enabling the agent to successfully complete the task without requiring all details upfront. This enhances user experience by reducing errors and the need for additional prompts.",simple,"[{'title': 'Building Better Tools for LLM Agents', 'link': 'https://www.llamaindex.ai/blog/building-better-tools-for-llm-agents-f8c5a6714f11', 'date': 'Jul 17, 2023', 'tags': ""['AI', 'Llamaindex', 'LLM', 'Agents', 'NLP']""}]",True
17,What benefits does automating administrative tasks provide for crisis counselors?,"[' to assess the stage of the conversation, given that there are recommended styles and questions for each stage (e.g. establishing rapport, risk assessment)\n- Closed-loop feedback cycle: Allow counselors to thumbs-up or thumbs-down selected responses, as a natural way to collect human feedback which can be used for further model or agent training\n\n## Conclusion: A Step Toward Efficient and Effective Crisis Care\nOur AI copilot for crisis counselors represents a significant step toward more efficient and effective crisis care. By automating administrative tasks, it frees up counselors to focus on their core mission of providing youth in crisis a safe place to talk. This not only enhances the quality of care provided but also addresses the pressing issue of counselor shortage by maximizing the impact of existing resources. As we continue to refine and expand this technology, we envision a future where crisis counseling is more accessible, responsive, and impactful for all those in need\u200b\u200b.\n## References\n- https://rag-a-thon.devpost.com/\n- https://devpost.com/software/counselor-copilot\n- https://www.mhanational.org/issues/state-mental-health-america\n- https://www.aamc.org/news/growing-psychiatrist-shortage-enormous-demand-mental-health-services\n- https://www.thetrevorproject.org/\n- https://github.com/zrizvi93/trevorhack\n\n']",Automating administrative tasks provides crisis counselors with the benefit of freeing up their time to focus on their core mission of providing youth in crisis a safe place to talk. This enhances the quality of care provided and addresses the pressing issue of counselor shortage by maximizing the impact of existing resources.,simple,"[{'title': 'Bridging the Gap in Crisis Counseling: Introducing Counselor Copilot', 'link': 'https://www.llamaindex.ai/blog/bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3', 'date': 'Feb 24, 2024', 'tags': ""['Llamaindex', 'Mental Health', 'Retrieval Augmented', 'Generative Ai Use Cases', 'LLM']""}]",True
18,What is the significance of AI accessibility in the context of the C3 Voice Assistant project?,"['SpeechRecognition || window.webkitSpeechRecognition;\n    if (SpeechRecognition &amp;&amp; !recognitionRef.current) {\n      recognitionRef.current = new SpeechRecognition();\n      recognitionRef.current.continuous = false;\n      recognitionRef.current.interimResults = false;\n\n      recognitionRef.current.onresult = (event) =&gt; {\n        const lastResultIndex = event.results.length - 1;\n        const transcriptResult = event.results[lastResultIndex][0].transcript;\n        setTranscript(transcriptResult);\n        setAppState(""playing"");\n        setShowSpeechBubble(true);\n        setTimeout(() =&gt; setShowSpeechBubble(false), speechBubbleTimeout);\n        fetchResponseFromLLM(transcriptResult);\n      };\n\n      recognitionRef.current.onend = () =&gt; {\n        setShowSpinner(true);\n      };\n    }\n  }, []);\n## 3. Handling User Speech and Response\ntoggleRecording controls the speech recognition process, while fetchResponseFromLLM sends the user\'s speech to the LLM backend and handles the response. This response is then spoken out via speech synthesis and also used to update the chat history displayed on the UI.\nconst toggleRecording = () =&gt; {\n    try {\n      if (appState === ""idle"") {\n        recognitionRef.current.start();\n        setAppState(""listening"");\n      } else if (appState === ""listening"") {\n        recognitionRef.current.stop();\n      }\n    } catch (error) {\n    }\n  };\nconst fetchResponseFromLLM = async (text) =&gt; {\n    try {\n      const response = await fetch(\n        `https://c3-python-nostream.onrender.com/api/chat`,\n        {\n          method: ""POST"",\n          headers: { ""Content-Type"": ""application/json"" },\n          body: JSON.stringify({\n            messages: [\n              {\n                role: ""user"",\n                content:\n                  ""You are an AI voice assistant called C3. You can provide any general information as well as answer basic questions about the Nvidia 10k report for year ended Jan 2023"" +\n                  text,\n              },\n            ],\n          }),\n        }\n      );\n      const data = await response.json();\n\n      setChatHistory((prevHistory) =&gt; [\n        ...prevHistory,\n        { query: text, response: data.result.content },\n      ]);\n      speak(data.result.content);\n    } catch (error) {\n      console.error(""Error communicating with LLM:"", error);\n    }\n  };\n## 4. Speech Synthesis\nThe speak function takes the text response from the LLM and uses the SpeechSynthesis API to read it aloud, providing an interactive experience for the user.\nconst speak = (text) =&gt; {\n    if (synthRef.current &amp;&amp; text) {', '\n      const utterance = new SpeechSynthesisUtterance(text);\n\n      const voices = window.speechSynthesis.getVoices();\n      if (voices.length &gt; 0) {\n        utterance.voice = voices[3]; // You can change this to select different voices\n      }\n\n      utterance.onstart = () =&gt; {\n        console.log(""TTS starts speaking"");\n        setShowSpinner(false);\n      };\n\n      utterance.onend = () =&gt; {\n        setAppState(""idle"");\n        if (wakeWordRecognitionRef.current) {\n          wakeWordRecognitionRef.current.start(); // Restart wake word listener after speaking\n        }\n      };\n      synthRef.current.speak(utterance);\n    }\n## 5. UI Rendering\nThe return statement of the App function contains the JSX code for rendering the app\'s UI. This includes buttons for starting/stopping the voice interaction, a display area for the transcript, and a chat sidebar showing the history of interactions.\nBy combining voice recognition, LLM integration, and speech synthesis, this frontend component provides a comprehensive and accessible interface for interacting with the C3 Voice Assistant.\n# Backend Server Setup\n- Initialize Create-Llama: Run npx create-llama@latest in your terminal.\n- Follow the prompts to set up a Python FastAPI backend, which we can be integrated with our frontend.\n- Use poetry install and poetry shell to prepare the environment.\n- Create a .env file with OPENAI_API_KEY=<openai_api_key>.\n- Generate Embeddings (optional): If a ./data directory exists, run python app/engine/generate.py.\n- Execute python main.pyto start the server.\n- Test the API: Use curl --location \'localhost:8000/api/chat\' --header \'Content-Type: application/json\' --data \'{ ""messages"": [{ ""role"": ""user"", ""content"": ""Hello"" }] }\' to test.\n- Modify API behavior in app/api/routers/chat.py. The server supports CORS for all origins, alterable with the ENVIRONMENT=prod setting.\n\n# Integration\nOnce the backend server is set up, integrating it with the frontend is straightforward. Simply update the fetchResponseFromLLM function in your frontend\'s App.js to call the backend server URL. This change ensures that when the frontend makes a request, it communicates with your newly configured backend, thus effectively integrating the two components.\n# Final Thoughts\nWrapping up, the C3 Voice Assistant isn‚Äôt just a tech showcase; it‚Äôs a stride towards democratizing AI. It‚Äôs about making powerful AI tools, like LLMs and RAG, accessible and user-friendly. This project is more than lines of code ‚Äî it‚Äôs a push to break down tech barriers and empower everyone.\nYour thoughts and feedback are invaluable ‚Äî let‚Äôs make AI more accessible together!\nLink to Github Repo: Frontend and Backend\nConnect with Me on LinkedIn\nLinkedin Post\n']","The significance of AI accessibility in the context of the C3 Voice Assistant project lies in its aim to democratize AI, making powerful tools like LLMs and RAG accessible and user-friendly. This project represents a push to break down tech barriers and empower everyone, emphasizing the importance of making AI tools available to a broader audience.",simple,"[{'title': 'AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5 (Deployed in Prod on Vercel and Render)', 'link': 'https://www.llamaindex.ai/blog/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a', 'date': 'Jan 14, 2024', 'tags': ""['Llamaindex', 'Voice Assistant', 'OpenAI', 'Accessibility', 'Inclusive Ai']""}, {'title': 'AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5 (Deployed in Prod on Vercel and Render)', 'link': 'https://www.llamaindex.ai/blog/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a', 'date': 'Jan 14, 2024', 'tags': ""['Llamaindex', 'Voice Assistant', 'OpenAI', 'Accessibility', 'Inclusive Ai']""}]",True
19,How do the performance metrics of Mistral compare to those of LLaMA-2 across different model sizes?,"[':\nThis image appears to be a set of four graphs, each comparing the performance of three different language models on various tasks: Mistral, LLaMA-2, and a hypothetical ‚ÄúSize 7B‚Äù model. The graphs are labeled with task-specific performance metrics: ‚ÄúMLM U‚Äù, ‚ÄúReasoning‚Äù, ‚ÄúKnowledge‚Äù, and ‚ÄúCommonsense‚Äù. The x-axes on the graphs represent model size in terms of the number of parameters, with three points that likely correspond to the sizes of the models being compared. The y-axes represent performance as a percentage, which could mean accuracy, precision, recall, or another relevant performance metric depending on the specific task. The graphs appear to show that Mistral performs better than LLaMA-2 and the Size 7B model across all metrics. This indicates that within the context of these measurements and tasks, Mistral is a stronger model. The exact nature of the tasks or what ‚ÄúMLM U‚Äù, ‚ÄúReasoning‚Äù, ‚ÄúKnowledge‚Äù, and ‚ÄúCommonsense‚Äù specifically refer to are not detailed in the image, but they likely correspond to standard NLP tasks designed to test understanding of language, ability to reason, knowledge recall, and commonsense reasoning, respectively. The performance improvements are depicted as increasingly significant with larger model sizes.\n## Observation:\nNow by giving the detail that mistral is available in 7B series, it is able to answer correctly.\n# Chain of thought prompting:\nQuestion: Based on the image provided. Follow the steps and answer the query ‚Äî Assuming mistral is available in 7B series. How well does mistral model compared to llama2 model?.\nExamine the Image: Look at the mentioned category in the query in the Image.\nIdentify Relevant Data: Note the respective percentages.\nEvaluate: Compare if there is any comparision required as per the query.\nDraw a Conclusion: Now draw the conclusion based on the whole data.\nAnswer:\nExamine the Image: The image contains four graphs, each graph compares the performance of three different models ‚Äî Llama 2, Mistral, and an unnamed third model ‚Äî across different metrics: mAP@L (mean Average Precision at L), Reason@L (presumably a reasoning score at L), Knowledge@L, and Comprehension@L. Each graph shows performance as a function of model size (in terms of billion parameters). Identify Relevant Data: We need to focus on the Mistral and Llama 2 models across all four graphs to extract the relevant data. For mAP@L: ‚Äî Llama 2 reaches above 65% when reaching 70 billion parameters. ‚Äî Mistral starts at around 5 billion parameters with about 48% and reaches almost 65% by the time it gets to 34 billion parameters. For Reason@L: ‚Äî Llama 2 reaches about 68% when reaching 70 billion parameters. ‚Äî Mistral starts at around 5 billion parameters with about 52% and reaches almost 68% by the time it gets to 34 billion parameters. For Knowledge@L: ‚Äî Llama 2 starts at about 45% with 1.3 billion parameters and reaches just slightly under 60% by 70 billion parameters. ‚Äî Mistral is not plotted in this graph, so no direct comparison can be made. For Comprehension@L: ‚Äî Llama 2 starts just above 40% with 1.3 billion parameters and reaches just under 60% by 70 billion parameters. ‚Äî Mistral starts around 13 billion parameters with about 52% and continues above 62% by 34 billion parameters. Evaluate: From the graphs, we can see that Mistral and Llama 2 are very comparable in performance in the metrics where both are shown. However, Mistral is not displayed on the Knowledge@L graph, so no comparison can be made for that particular metric. Draw a Conclusion: In the graphs where both models can be compared (mAP@L, Reason@L, and Comprehension@L), Mistral appears to perform comparably to Llama 2, reaching similar levels of performance at smaller parameter sizes relative to the plotted sizes for Llama 2. This suggests that Mistral could be more parameter-efficient. However, since there is no data for Mistral in the Knowledge@L graph, we cannot draw a complete comparison\n## Observation:\nThere is a hallucination with a number of model parameters and percentage points though the final conclusion is partially correct.\n# Image 3 ‚Äî Performances of different LLMs across various NLP tasks.\n\n# General Question:\nQuestion: Analyze the Image.\nAnswer:\nThe image appears to be a table containing numerical data, which seems to be a comparative analysis of various models across multiple parameters or tests. Each row represents a different model or configuration as indicated by names such as ‚ÄúMPT‚Äù and ‚ÄúFalcon‚Äù followed by a size specification like ÔøΩ']","The performance metrics of Mistral compared to LLaMA-2 show that Mistral performs comparably in the metrics where both models are plotted (mAP@L, Reason@L, and Comprehension@L). Mistral reaches similar performance levels at smaller parameter sizes relative to LLaMA-2, suggesting it could be more parameter-efficient. However, there is no data for Mistral in the Knowledge@L graph, so a complete comparison cannot be made for that metric.",simple,"[{'title': 'GPT4-V Experiments with General, Specific questions and Chain Of Thought prompting(COT) techniques.', 'link': 'https://www.llamaindex.ai/blog/gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9', 'date': 'Nov 17, 2023', 'tags': ""['Llamaindex', 'Gpt 4', 'LLM', 'Vision', 'OpenAI']""}]",True
20,What new capabilities do LLMs with Knowledge Graphs provide in the LlamaIndex Update?,"['Greetings, LlamaIndex community!\nWe‚Äôre excited to introduce our new blog series, the LlamaIndex Update. Recognizing the fast pace of our open-source project, this series will serve as your continual guide, tracking the latest advancements in features, webinars, hackathons, and community events.\nOur goal is simple: to keep you updated, engaged, and inspired. Whether you‚Äôre a long-time contributor or a new joiner, these updates will help you stay in sync with our progress.\nSo, let‚Äôs explore the recent happenings in our premier edition of the LlamaIndex Update.\n## Features And Integrations:\n- LLMs with Knowledge Graphs, supported by NebulaGraph. This new stack enables unique retrieval-augmented generation techniques. Our Knowledge Graph index introduces a GraphStore abstraction, complementing our existing data store types.\nDocs, Tweet\n- Better LLM app UX supports in-line citations of its sources, enhancing interpretability and traceability. Our new CitationQueryEngine enables these citations and ensures they correspond with retrieved documents. This feature marks a leap towards improving transparency in LlamaIndex applications.\nDocs, Tweet\n- LlamaIndex integrates with Microsoft Guidance to ensure structured outputs from LLMs. It allows direct prompting of JSON keys and facilitates the conversion of Pydantic objects into the Guidance format, enhancing structured interactions. It can be used independently or with the SubQuestionQueryEngine.\nDocs, Tweet\n- The GuidelineEvaluator module allows users to set text guidelines, thereby aiding in the evaluation of LLM-generated text responses. This paves the way toward automated error correction capabilities.\nNotebook, Tweet\n- We now include a simple OpenAIAgent, offering an agent interface capable of sequential tool use and async callbacks. This integration was made possible with the help of the OpenAI function API and the LangChain abstractions.\nTweet\n- OpenAIPydanticProgram in LlamaIndex enhances structured output extraction. This standalone module allows any LLM input to be converted into a Pydantic object, providing a streamlined approach to data structuring.\nDocs, Tweet\n- We now incorporate the FLARE technique for a knowledge-augmented long-form generation. FLARE uses iterative retrieval to construct extended content, deciding to perform retrieval with each sentence. Unlike conventional vector index methods, our FLARE implementation builds a template iteratively, filling gaps with retrieval for more pertinent responses. Please note, this is a beta feature and works best with GPT-4.\nDocs, Tweet\n- We now employ the Maximal Marginal Relevance (MMR) algorithm to enhance diversity and minimize redundancy in retrieved results. This technique measures the similarity between a candidate document and the query while minimizing similarity with previous documents, depending on a user-specified threshold. Please note that careful calibration is necessary to ensure that increased diversity doesn‚Äôt introduce irrelevant context. The threshold value is key to balancing diversity and relevance.\nDocs, Tweet\n- We now support recursive Pydantic objects for complex schema extraction. This enhancement, inspired by parsing directory trees, employs a mix of recursive (Node) and non-recursive (DirectoryTree) Pydantic models, facilitating more sophisticated agent-tool interactions.\nTweet\n- We have developed agents that can perform advanced query planning over data using the Function API and Pydantic. These agents input a full Pydantic graph in the function signature of a query plan tool, which is then executed. This system can work with any tool and has the potential to construct complex query plans. However, it has limitations like difficulty in producing deep nesting and the possibility of outputting invalid responses.\nDocs, Tweet\n- `OpenAIAgent` is capable of advanced data retrieval and analysis, such as auto-vector database retrieval and joint text-to-SQL and semantic search. We have also built a query plan tool interface that allows the agent to generate structured/nested query plans, which can then be executed against any set of tools, enabling advanced reasoning and analysis.\nDocs: OpenAI Agent + Query Engine, Retrieval Augmented OpenAI Agent, OpenAI Agent Query Planning.\nTweet\n- The new multi-router feature allows for QA over complex data collections, where answers may be spread across multiple sources. It uses a ‚ÄúMultiSelector‚Äù object to select relevant choices given a query. The router can pick up to a maximum number of choices. It can use either a raw LLM completion API or the OpenAI Function API. If the Function API is used, schema validity can be enforced. A simple usage example involves a RouterQueryEngine, where the PydanticMultiSelector selects the relevant vector and keyword index to synthesize an answer.\nDocs, Tweet\n- We have made a significant upgrade to our token tracking feature. Users can now easily track prompt']","LLMs with Knowledge Graphs in the LlamaIndex Update enable unique retrieval-augmented generation techniques. The Knowledge Graph index introduces a GraphStore abstraction, which complements existing data store types.",simple,"[{'title': 'LlamaIndex Update ‚Äî 06/26/2023', 'link': 'https://www.llamaindex.ai/blog/llamaindex-update-6-26-2023-ed30a9d45f84', 'date': 'Jun 26, 2023', 'tags': ""['Large Language Models', 'Llamaindex', 'NLP', 'AI', 'OpenAI']""}]",True
21,What are the observed performance differences between Gemini Pro and GPT-3.5 in the benchmark evaluations?,"[' original MT-Bench Human Judgement dataset. Specifically, in the original dataset, there are some replications with respect to the triple (query, model A, model B) examples since for some of these more than one human evaluation was provided. Since our prompt allows the LLM evaluator to deem a tie, and to our knowledge, this wasn‚Äôt made an option for the human evaluators, we have aggregated the results across the different human evaluations to get the proportion of times model A wins versus model B for each triple (query, model A, model B). We then say that human evaluators deem a tie if the proportion lies between 0.4 and 0.6. It should be emphasized here that the reference evaluations are provided by humans, and so the benchmark metrics that we produce and share here represent the LLM agreement with humans.\n(We skip showing the code snipped to produce the results here, because they‚Äôre essentially the same as the previously shared code snipper with the exception of requiring a PairwiseComparisonEvaluator instead of a CorrectnessEvaluator.)\n# Benchmark Results\nInvalid_predictions are as defined in the previous benchmark. Inconclusive‚Äôs represent the case when an LLM evaluator flips its winner after prompting it with the same evaluation task but instead flipping the order of presentation of the two answers (i.e. to mitigate against position bias). Two agreement rates, with the inclusion and exclusion of ties, are also produced ‚Äî note that these are both conditional in the event that the prediction (or evaluation) is valid.\nObservations\n- In terms of agreement rates, all three models seem quite close. Note again that these are conditional on the prediction/evaluation being valid. And so, one should ‚Äúdiscount‚Äù these with the invalid and inconclusive counts.\n- Gemini Pro and GPT-3.5 seem to be a bit more assertive than GPT-4 resulting in only 50‚Äì60 ties to GPT-4‚Äôs 100 ties.\n- Overall, it seems that Gemini Pro is up to snuff with GPT models, and would say that it outperforms GPT-3.5!\n\n# Go now and evaluate your evaluators (and eat your veggies)!\nIt is, for obvious reasons, important to evaluate your LLM evaluators, as these are now being relied upon to evaluate the performance of our LLM systems ‚Äî a broken compass is not really helpful! With these newly introduced llama-datasets, we hope that it is easy for you to compile your own benchmark datasets on your own data, and then even easier to produce your benchmark metrics. As mentioned before, the two llama-datasets discussed in this blog are available for download and use through LlamaHub. Be sure to visit and make use of the datasets there to build an exhaustive benchmark suite! (We welcome contributed llama-datasets as well!)\n# Related Links\n- MT-Bench Human Judgement Benchmarking Notebook\n- MT-Bench Single Grading Benchmarking Notebook\n\n']","In the benchmark evaluations, Gemini Pro seems to outperform GPT-3.5, as it is noted to be up to snuff with GPT models. Additionally, Gemini Pro and GPT-3.5 appear to be more assertive than GPT-4, resulting in fewer ties compared to GPT-4's 100 ties.",simple,"[{'title': 'Two new llama-datasets and a Gemini vs. GPT showdown', 'link': 'https://www.llamaindex.ai/blog/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5', 'date': 'Dec 20, 2023', 'tags': ""['Benchmarking', 'Gpt 4', 'Gemini', 'Llamaindex', 'Evaluation']""}]",True
22,What is the role of the reference evaluator GPT-4 in the benchmarking process of LLM evaluators?,"[' answer. That is, the LLM evaluator ranks the two answers to determine the superior one. With these predictions in hand, we can evaluate how good the evaluations are by comparing them to the reference feedbacks and scores. A llama-pack called EvaluatorBenchmarkerPack makes benchmarking a one-liner.\n# Benchmarking Gemini and GPT models as LLM evaluators: Gemini achieves GPT-3.5 performance!\nIn this section, we will put our new llama-dataset types to use in order to pit Gemini Pro against GPT models. For this, we‚Äôre going to use slightly adapted versions of the MT-Bench dataset. These adapted versions have been made available for download and use through LlamaHub along with today‚Äôs release!\n# Mini MT-Bench Single Grading Dataset\nThis llama-dataset is a LabelledEvaluatorDataset and is a miniature version of the MT-Bench single-grading dataset. In particular, we consider all of the 160 original questions (i.e., 80 x 2, since MT Bench is a two-turn question dataset), but only the responses produced by Llama2-70b. For the reference evaluations, we use GPT-4. As with the original LabelledRagDataset, we‚Äôve produced a new llama-pack EvaluatorBenchmarkerPack (of course, also made available in today‚Äôs release!) to make benchmarking an LLM evaluator on the new llama-datasets relatively effortless. The below snippet of code is how you can replicate the results of this benchmark\nfrom llama_index.llama_dataset import download_llama_dataset\nfrom llama_index.llama_pack import download_llama_pack\nfrom llama_index.evaluation import CorrectnessEvaluator\nfrom llama_index.llms import Gemini\nfrom llama_index import ServiceContext\n# download dataset\nevaluator_dataset, _ = download_llama_dataset(\n    ""MiniMtBenchSingleGradingDataset"", ""./mini_mt_bench_data""\n)# define evaluator\ngemini_pro_context = ServiceContext.from_defaults(\n    llm = Gemini(model=""models/gemini-pro"", temperature=0)\n)\nevaluator = CorrectnessEvaluator(service_context=gemini_pro_context)# download EvaluatorBenchmarkerPack and define the benchmarker\nEvaluatorBenchmarkerPack = download_llama_pack(""EvaluatorBenchmarkerPack"", ""./pack"")\nevaluator_benchmarker = EvaluatorBenchmarkerPack(\n    evaluator=evaluators[""gpt-3.5""],\n    eval_dataset=evaluator_dataset,\n    show_progress=True,\n)# produce the benchmark result\nbenchmark_df = await evaluator_benchmarker.arun(\n\t\tbatch_size=5,\n\t\tsleep_time_in_seconds=0.5\n)\n# Benchmark Results\nInvalid_predictions occurs whenever the LLM evaluator fails to produce the desired output structure and as well as other exceptions. Correlations represent the correlations with the scores produced by each of the evaluators with the reference scores produced by the reference evaluator GPT-4. Similarly, the remaining two metrics, MAE (i.e., mean absolute error, which is a sum of the absolute differences between each pair of evaluator and reference scores) and Hamming (i.e., which counts how many times evaluator and reference scores are equivalent), are computed with the scores produced by the evaluators and those from the reference evaluator.\nObservations\n- It seems that Gemini-Pro and GPT-3.5 are quite close in terms of their closeness to the reference evaluator GPT-4!\n- As for GPT-4 versus the reference GPT-4, this is mostly used for assessing self-consistency of the LLM, for which we see it does a fairly good job at that.\n\n# MT-Bench Human Judgement Dataset\nFor this benchmark, we‚Äôll evaluate the LLM evaluators on the task of ranking two LLM responses, to determine which of the two is the better one. And it is for this such task that LabelledPairwiseEvaluatorDataset exists. The llama-dataset that we‚Äôve curated here is a slightly adapted version of the']","The role of the reference evaluator GPT-4 in the benchmarking process of LLM evaluators is to provide reference scores against which the performance of other evaluators, such as Gemini and GPT-3.5, can be compared. It serves as a standard for assessing the accuracy and effectiveness of the evaluations produced by these models.",simple,"[{'title': 'Two new llama-datasets and a Gemini vs. GPT showdown', 'link': 'https://www.llamaindex.ai/blog/two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5', 'date': 'Dec 20, 2023', 'tags': ""['Benchmarking', 'Gpt 4', 'Gemini', 'Llamaindex', 'Evaluation']""}]",True
23,What is the purpose of the StorageContext in the RAG pipeline setup?,"['      code_snippet += ""vector_store = QdrantVectorStore(client=client, collection_name=\'sampledata\')\\n""\n    elif vector_store_choice == ""Simple"":\n        code_snippet += ""vector_store = None  # Simple in-memory vector store selected\\n""\n\n    code_snippet += ""\\n# Finalizing the RAG pipeline setup\\n""\n    code_snippet += ""if vector_store is not None:\\n""\n    code_snippet += ""    storage_context = StorageContext.from_defaults(vector_store=vector_store)\\n""\n    code_snippet += ""else:\\n""\n    code_snippet += ""    storage_context = None\\n\\n""\n\n    code_snippet += ""service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model, node_parser=node_parser)\\n\\n""\n\n    code_snippet += ""_file = \'path_to_your_file\'  # Replace with the path to your file\\n""\n    code_snippet += ""vector_index = VectorStoreIndex.from_documents(documents=_file, storage_context=storage_context, service_context=service_context, show_progress=True)\\n""\n    code_snippet += ""if storage_context:\\n""\n    code_snippet += ""    vector_index.storage_context.persist(persist_dir=\'persist_dir\')\\n\\n""\n\n    code_snippet += ""query_engine = vector_index.as_query_engine(response_mode=response_mode, verbose=True)\\n""\n\n    return code_snippet\n# Conclusion\nRAGArch stands at the intersection of innovation and practicality, offering a streamlined no-code approach to RAG pipeline development. It‚Äôs designed to demystify the complexities of AI configurations. With RAGArch, both seasoned developers and AI enthusiasts can craft custom pipelines with ease, accelerating the journey from idea to implementation.\nYour insights and contributions are invaluable as I continue to evolve this tool. Check out RAGArch on Github and let‚Äôs start a conversation on Linkedin. I‚Äôm always eager to collaborate and share knowledge with fellow tech adventurers.\nGitHub Repo\nConnect with Me on LinkedIn\nLive Demo\n']","The purpose of the StorageContext in the RAG pipeline setup is to manage the storage of the vector store, which can be either a QdrantVectorStore or a simple in-memory vector store. It is created from the vector_store if it is not None, allowing the pipeline to utilize the selected storage method for managing documents and queries.",simple,"[{'title': 'RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex', 'link': 'https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089', 'date': 'Feb 2, 2024', 'tags': ""['Rag', 'No Code', 'Llamaindex', 'OpenAI', 'Code Generation']""}]",True
24,What recent research has been published about the LK-99 material and its significance in the field of superconductors?,"[' conversation history and dumped into the LLM context window, then we may run into context window limitations.\nLlamaIndex offers tool abstractions to help deal with this. Our LoadAndSearchToolSpec wraps any given tool that may return a large amount of data, and it splits it into two tools: a load tool that will dynamically store the data in an index, and a search tool that allows for search over that index.\nOn the Metaphor side, this is also where we define a search_and_retrieve_documents endpoint that combines search and retrieve. This allows the agent to make a single query to retrieve a large number of documents, which when combined with the LoadAndSearchToolSpec will get directly stored within an index. If the agent were to call search and retrieve separately, then it would both take longer and consume more tokens to write the search results to conversation history, and then passing that into the prompt again to call retrieve over all document IDs.\nCreating the LoadAndSearchToolSpec:\nfrom llama_index.tools.tool_spec.load_and_search.base import LoadAndSearchToolSpec\n# The search_and_retrieve_documents tool is the third in the tool list, as seen above\nwrapped_retrieve = LoadAndSearchToolSpec.from_defaults(\n  metaphor_tool_list[2],\n)\nNow let‚Äôs walk through a full execution example:\n# Just pass the wrapped tools and the get_date utility\nagent = OpenAIAgent.from_tools(\n  [*wrapped_retrieve.to_tool_list(), metaphor_tool_list[4]],\n  verbose=True,\n)\nprint(agent.chat(\'Can you summarize everything published in the last month regarding news on superconductors\'))\nThe output here shows that the agent calls multiple tools in succession to get the right answer.\n=== Calling Function ===\nCalling function: current_date with args: {}\nGot output: 2023-08-20\n========================\n=== Calling Function ===\nCalling function: search_and_retrieve_documents with args: {\n  ""query"": ""superconductors"",\n  ""start_published_date"": ""2023-07-20"",\n  ""end_published_date"": ""2023-08-20""\n}\n[Metaphor Tool] Autoprompt: ""Here is an interesting article about superconductors:\nGot output: Content loaded! You can now search the information using read_search_and_retrieve_documents\n========================\n=== Calling Function ===\nCalling function: read_search_and_retrieve_documents with args: {\n  ""query"": ""superconductors""\n}\nGot output: \nSuperconductors are materials that can perfectly conduct electricity. They are used in a variety of applications, such as particle accelerators, nuclear fusion devices, MRI machines, and maglev trains. However, so far, no superconductor has been proven to work at ambient pressures and temperatures. On July 22, scientists in South Korea published research claiming to have solved this problem with a material called LK-99, which has an electrical resistivity that drops to near zero at 30 degrees Celsius (86 degrees Fahrenheit).\n========================\nIn the last month, there have been developments in the field of superconductors. Scientists in South Korea have published research on a material called LK-99, which has the ability to conduct electricity with near-zero resistance at a temperature of 30 degrees Celsius (86 degrees Fahrenheit). This breakthrough could potentially lead to the development of superconductors that work at ambient pressures and temperatures, opening up new possibilities for various applications such as particle accelerators, nuclear fusion devices, MRI machines, and maglev trains.\nThe agent used the get_date tool to determine the current month, and then applied the filters in Metaphor based on publication date when calling search. It then loaded the documents using retrieve_documents and read them using read_retrieve_documents.\n# Conclusion\nAs shown above, the integration between LlamaIndex data agents + Metaphor search has the potential to bypass existing limitations with LLMs and even RAG systems. We‚Äôre excited to continue exploring this further in future blog posts.\nWe encourage you to play around with the notebook ‚Äî make sure to check it out!\nResources:\n- Notebook: https://github.com/emptycrown/llama-hub/blob/main/llama_hub/tools/notebooks/metaphor.ipynb\n- LlamaHub: https://llamahub.ai/l/tools-metaphor\n- Metaphor: https://platform.metaphor.systems/\n- Metaphor API Docs: https://docs.metaphor']","Recent research published on July 22 by scientists in South Korea claims to have solved the problem of superconductors working at ambient pressures and temperatures with a material called LK-99. This material has an electrical resistivity that drops to near zero at 30 degrees Celsius (86 degrees Fahrenheit). This breakthrough could potentially lead to the development of superconductors that work under normal conditions, which would open up new possibilities for various applications such as particle accelerators, nuclear fusion devices, MRI machines, and maglev trains.",simple,"[{'title': 'LlamaIndex + Metaphor: Towards Automating Knowledge Work with LLMs', 'link': 'https://www.llamaindex.ai/blog/llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f', 'date': 'Aug 21, 2023', 'tags': ""['Search', 'LLM', 'Agents', 'NLP', 'Llamaindex']""}]",True
25,What are the benefits of using specific questioning techniques when analyzing images with GPT-4V?,"['# Introduction\nGPT-4V has amazed us with its ability to analyze images and even generate website code from visuals.\nThis blog post investigates GPT-4V‚Äôs proficiency in interpreting bar charts, scatter plots, and tables. We aim to assess whether specific questioning and chain of thought prompting can yield better responses compared to broader inquiries. Our demonstration seeks to determine if GPT-4V can exceed these known limitations with precise questioning and systematic reasoning techniques.\nWe observed in these experiments that asking specific questions, rather than general ones, yields better answers. Let‚Äôs delve into these experiments.\nYou can also follow along with this blog post in our Google Colab Notebook.\nNOTE: This blog post aims to inform the community about GPT-4V‚Äôs performance, though the results might not be universally applicable. We strongly advise conducting tests with similar questions on your own dataset before drawing conclusions.\n# Experiment:\nWhat we‚Äôll do is take an image of each of a bar chart, scatter plot, and table and analyze them by asking three types of questions.\n- General Question: Simply ask, ‚ÄúAnalyse the image.‚Äù\n- Specific Question: Specific Question the performance of a certain category by providing more details.\n- Chain of Thought Prompting: Use a step-by-step reasoning method to walk through the analysis.\n\nThese guidelines aim to test how different questioning techniques might improve the precision of the information we gather from the images.\n# Data for experiments:\nTo test out we selected the following images from Llama2 and MistralAI papers.\n- Violation percentage of safety with different LLMs across categories. (Llama2 paper)\n- Llama2 vs Mistral model performances across various NLP tasks.(Mistral paper)\n- Performances of different LLMs across various NLP tasks. (Llama2 paper)\n\nLet‚Äôs go through each of these data points one by one.\n# Image 1 ‚Äî Violation percentage of safety with different LLMs across categories.\n\n# General Question:\nQuestion: Analyse the Image.\nAnswer:\nThe image you‚Äôve provided is a bar chart displaying data grouped into three categories of online violations: hateful and harmful, illicit and criminal activity, and unqualified advice. Each of these categories has a number in parentheses, indicating the sample size of reported incidents for each type of violation (395, 728, and 311, respectively). The x-axis of the chart specifies various social media platforms or services, such as ‚ÄúVideo sharing‚Äù, ‚ÄúSocial networking‚Äù, ‚ÄúGaming‚Äù, ‚ÄúDating‚Äù, ‚ÄúForums & boards‚Äù, ‚ÄúCommercial Websites‚Äù, ‚ÄúMedia sharing‚Äù, ‚ÄúP2P/File sharing‚Äù, ‚ÄúWiki‚Äù, and ‚ÄúOther‚Äù. It appears to measure how often these categories of violations occur on each type of platform. The y-axis measures the percentage of violations reported, ranging from 0% to 60%. Each platform/service has three bars corresponding to the three violation categories, showing their respective percentages. The bars also have error bars, which typically represent the variability of the data, such as standard deviation, standard error, or confidence interval, indicating the precision of the estimates. The chart has a legend indicating the color corresponding to each of the three categories of violations. This visual representation helps to compare the prevalence of different types of violations across the different types of online services. Without specific numbers, it‚Äôs not possible to provide exact percentages, but we can observe trends, such as: ‚Äî ‚ÄúForums & boards‚Äù and ‚ÄúSocial networking‚Äù platforms have notably higher percentages across all three types of violations compared to other platforms. ‚Äî ‚ÄúCommercial Websites‚Äù seem to have lower percentages of reported hateful and harmful activities and unqualified advice but higher percentages of illicit and criminal activities when compared to other platforms like ‚ÄúWiki‚Äù or ‚ÄúP2P/File sharing‚Äù. ‚Äî ‚ÄúGaming‚Äù appears to have a moderate percentage of hateful and harmful violations, lower levels of illicit and criminal activity, and relatively higher levels of unqualified advice. Overall, this chart is a visual tool that likely aims to inform about the prevalence of certain types of online violations across different digital platforms.\n## Observation:\nAs you can see though the categories hateful and harmful, illicit and criminal activity, and unqualified advice but it hallucinated with x-axis values with ‚Äî ‚ÄúVideo sharing‚Äù, ‚ÄúSocial networking‚Äù, ‚ÄúGaming‚Äù, ‚ÄúDating‚Äù, ‚ÄúForums & boards‚Äù, ‚ÄúCommercial Websites‚Äù, ‚ÄúMedia sharing‚Äù, ‚ÄúP2P/File sharing‚Äù, ‚ÄúWiki‚Äù, and ‚ÄúOther‚Äù.\n# Specific Questions:']","The benefits of using specific questioning techniques when analyzing images with GPT-4V include yielding better answers compared to general questions, improving the precision of the information gathered, and allowing for a more detailed performance assessment of certain categories by providing more context.",simple,"[{'title': 'GPT4-V Experiments with General, Specific questions and Chain Of Thought prompting(COT) techniques.', 'link': 'https://www.llamaindex.ai/blog/gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9', 'date': 'Nov 17, 2023', 'tags': ""['Llamaindex', 'Gpt 4', 'LLM', 'Vision', 'OpenAI']""}]",True
26,What are the characteristics of graph databases and how do they differ from traditional databases?,"['BCypherReader` from the relevant file and follow the steps below.\nIf you wish to submit the loader on LlamaHub, the process is fairly straightforward. After adding an example to the readme which uses an always-on demo server with StackOverflow data, I was ready to create a pull request. After a short discussion the PR was quickly merged.\nThanks a lot Jerry for the smooth experience.\nNow let‚Äôs see how to use our two loaders.\n# Using the Graph Database Loader\nThe GraphDB Cypher loader, connects to graph databases, which are specialized databases that store data not in tables but in entities (Nodes) and their Relationships. Because they are schema free, you can store real-world knowledge without compromising on richness.\nImage for ‚ÄúNetwork Graph‚Äù generated by Midjourney by Author\nRelationships can also hold attributes, which can represent time, weights, costs or whatever defines the concrete relationship. Any node can have as many or as few attributes or relationships as needed.\nTo query a graph database you can use the Cypher query language, a pattern based language that expresses those relationships in visual ascii-art patterns. You encircle nodes in parentheses () and draw relationships as arrows --> with additional constraints put in square brackets. Otherwise Cypher provides many features known from SQL and also supports many graph operations as well as handling data structures like nested documents, of lists and dicts.\nLet‚Äôs use a movie graph database and ask the LLM a question about common action movie plots.\nSetting up the ServiceContext and the ChatGPTLLMPredictor is the same as before.\nThen we get the GraphDBCypherReader and connect it to our database (with an small example movie graph from TheMovieDB with permission).\nGraphDBCypherReader = download_loader(\'GraphDBCypherReader\')\n\nreader = GraphDBCypherReader(uri = ""neo4j+s://demo.neo4jlabs.com"", \\\n    username = ""recommendations"", password = ""recommendations"", database = ""recommendations"")\nThen we define our query to the graph database with a parameter of year that allows us to pick more recent movies. When loading the data, each row of results should turn into one Document where the text property of the document is the YAML representation of the row.\nquery = """"""\n    MATCH (m:Movie)-[rel:ACTED_IN|DIRECTED|IN_GENRE]-(other)\n    WHERE $year &lt; m.year and m.imdbRating &gt; $rating\n    WITH m, type(rel) as relation, collect(other.name) as names\n    RETURN m.title as title, m.year as year, m.plot as plot, relation, names\n    ORDER BY m.year ASC\n""""""\n\ndocuments = reader.load_data(query, parameters = {""year"":1990,""rating"":8})\nindex = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\n\nprint(len(documents))\nprint(documents[0:5])\nThe output will look similar to the following:\n829\n[Document(text=\'names:\\n- Saifei He\\n- Li Gong\\n- Jingwu Ma\\n- Cuifen Cao\\nplot: A young woman becomes the fourth wife of a wealthy lord, and must learn to live\\n  with the strict rules and tensions within the household.\\nrelation: ACTED_IN\\ntitle: Raise the Red Lantern (Da hong deng long gao gao gua)\\nyear: 1991\\n\', doc_id=\'782d9a63-251b-4bb8-aa3d-5d8f6d1fb5d2\', embedding=None, doc_hash=\'f9fd966bc5f2234e94d09efebd3be008db8c891f8666c1a364abf7812f5d7a1c\', extra_info=None), Document(text=\'names:\\n- Yimou Zhang\\nplot: A young woman becomes the fourth wife of a wealthy lord, and must learn to live\\n  with the strict rules and tensions within the household.\\nrelation: DIRECTED\\ntitle: Raise the Red Lantern (Da hong deng long gao gao gua)\\nyear: 1991\\n\', doc_id=\'2e13caf6-b9cf-4263-a264-7121bc77d1ee\', embedding=None, doc_hash=\'e1f']","Graph databases are specialized databases that store data in entities (Nodes) and their Relationships, rather than in tables. They are schema-free, allowing for the storage of real-world knowledge without compromising on richness. Relationships can hold attributes that define the relationship, and querying is done using the Cypher query language, which expresses relationships in visual patterns. This differs from traditional databases, which typically use structured tables and fixed schemas.",simple,"[{'title': 'Enriching LlamaIndex Models with GraphQL and Graph Databases', 'link': 'https://www.llamaindex.ai/blog/enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7', 'date': 'Jun 30, 2023', 'tags': ""['LLM', 'GraphQL', 'Graph Database', 'Llamaindex', 'Python']""}]",True
27,What are the two main stages of the retrieval-augmented generation (RAG) process?,"['The widespread consumption of videos on platforms like YouTube, Instagram, and others highlights the importance of efficiently processing and analyzing video content. This capability unlocks vast opportunities across various sectors, including media and entertainment, security, and education. However, the main challenge is effectively extracting meaningful information from videos, which are inherently complex and multimodal data streams.\nThis blog post introduces a solution that leverages the LlamaIndex Python API for using the advanced capabilities of OpenAI‚Äôs GPT4V, combined with the efficient data management by LanceDB across all data formats, to process videos.\n‚Ä¶But what does ‚ÄòRAG‚Äô even mean?\nRetrieval-augmented generation (RAG) is a technique that merges information retrieval with generative AI to produce systems capable of generating precise and contextually relevant responses by tapping into large data repositories.\n# Core Concept of RAG\nRAG operates in two stages:\n- Retrieval: Utilizes semantic search to find documents related to a query, leveraging the context and meaning beyond mere keywords.\n- Generation: Integrates retrieved information to produce coherent responses, allowing the AI to ‚Äúlearn‚Äù from a wide range of content dynamically.\n\n# RAG Architecture\nThe architecture typically involves a dense vector search engine for retrieval and a transformer model for generation. The process:\n- Performs a semantic search to fetch relevant documents.\n- Processes these documents with the query to create a comprehensive context.\n- The generative model then crafts a detailed response based on this enriched context.\n\n# Extending to Multimodality\nMultimodal RAG integrates various data types (text, images, audio, video) in both retrieval and generation phases, enabling richer information sourcing. For example, responding to queries about ‚Äúclimate change impacts on polar bears‚Äù might involve retrieving scientific texts, images, and videos to produce an enriched, multi-format response.\nLet‚Äôs return to our use case and dive into how it‚Äôs all done. Moving forward, you can access the full code on Google Colab.\nThe solution is divided into the following sections. Click on the topic to skip to a specific part:\n- Video Downloading\n- Video Processing\n- Building the Multi-Modal Index and Vector Store\n- Retrieving Relevant Images and Context\n- Reasoning and Response Generation\n\n# 1. Video Downloading\nTo begin, we need to locally download multimodal content from a publicly available source, I used pytube to download a YouTube video by 3Blue1Brown on the Gaussian function.\n# SET CONFIG\nvideo_url = ""https://www.youtube.com/watch?v=d_qvLDhkg00""\noutput_video_path = ""./video_data/""\noutput_folder = ""./mixed_data/""\noutput_audio_path = ""./mixed_data/output_audio.wav""\n\nfilepath = output_video_path + ""input_vid.mp4""\nPath(output_folder).mkdir(parents=True, exist_ok=True)\ndef download_video(url, output_path):\n    """"""\n    Download a video from a given url and save it to the output path.\n\n    Parameters:\n    url (str): The url of the video to download.\n    output_path (str): The path to save the video to.\n\n    Returns:\n    dict: A dictionary containing the metadata of the video.\n    """"""\n  from pytube import YouTube\n\n    yt = YouTube(url)\n    metadata = {""Author"": yt.author, ""Title"": yt.title, ""Views"": yt.views}\n    yt.streams.get_highest_resolution().download(\n        output_path=output_path, filename=""input_vid.mp4""\n    )\n    return metadata\nRun metadata_vid = download_video(video_url, output_video_path) to invoke the function and store the video locally.\n# 2. Video Processing\nWe need to now extract multimodal content ‚Äî Images, Text(via Audio). I extracted 1 frame every 5 seconds of the video (~160 frames) using moviepy .\ndef video_to_images(video_path, output_folder):\n    """"""\n    Convert a video to a sequence of images and save them to the output folder.\n\n    Parameters:\n    video_path (str): The path to the video file.\n    output_folder (str): The path to the folder to save the images to.\n\n    """"""\n    clip =']","The two main stages of the retrieval-augmented generation (RAG) process are Retrieval and Generation. Retrieval utilizes semantic search to find documents related to a query, while Generation integrates the retrieved information to produce coherent responses.",simple,"[{'title': 'MultiModal RAG for Advanced Video Processing with LlamaIndex & LanceDB', 'link': 'https://www.llamaindex.ai/blog/multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e', 'date': 'Feb 17, 2024', 'tags': ""['AI', 'Generative Ai Tools', 'Vector Database', 'Large Language Models', 'Open Source']""}]",True
28,What capabilities have been added to LlamaIndex for fine-tuning embedding models?,"['We‚Äôve added capabilities in LlamaIndex allowing you to fine-tune a linear adapter on top of embeddings produced from any model (sentence_transformers, OpenAI, and more).\nThis allows you to transform your embedding representations into a new latent space that‚Äôs optimized for retrieval over your specific data and queries. This can lead to small increases in retrieval performance that in turn translate to better performing RAG systems.\nA nice bonus: you do not need to re-embed your documents by using this adapter! Simply transform the query instead.\nWe have a full end-to-end guide showing how you can generate a synthetic dataset, fine-tune the linear adapter, and evaluate its performance.\n# Context\nThe concept of fine-tuning your embedding model is powerful. In fact, we were inspired to both add a full example repository / blog post as well as native abstractions in LlamaIndex showing how you can fine-tune a sentence_transformers model over any unstructured text corpus (with our SentenceTransformersFinetuneEngine).\nHowever, this approach has some limitations:\n- The SentenceTransformersFinetuneEngine is limited to fine-tuning sentence_transformers models.\n- After finetuning the embedding model, you will need to re-embed your document corpus.\n\nDuring our Finetuning + RAG webinar last Friday, Jo (Vespa) mentioned the exact same problem: fine-tuning the embeddings model requires you to reindex your documents. However, his work with Vespa explored the concept of ‚Äúfreezing‚Äù document embeddings using a foundation model, and instead training a transformation on the query embedding.\nThis inspired us to explore a similar embedding fine-tuning approach that was simultaneously more general but also allowed us to freeze existing document embeddings.\n# Approach\nOur brand-new EmbeddingAdapterFinetuneEngine fine-tunes a linear adapter on top of query embeddings produced by any model. The linear adapter is simply a linear transformation that specifically transforms the query embedding while keeping document embeddings fixed.\nThe linear adapter can be used on top of any existing embeddings model: SBERT embeddings, OpenAI embeddings, Cohere embeddings, and more. As a result you can just plug this in on top of any embedding model that you‚Äôre already using!\n\nSince document embeddings are unchanged, this means that you can always fine-tune this linear adapter after you‚Äôve generated embeddings for your documents. You can choose to arbitrarily re-train this adapter on top of changing data distributions, without needing to re-embed all your documents.\n## Technical Details\nAs mentioned above, the linear adapter simply performs a linear transformation on top of the query embedding while keeping the Document embeddings fixed (with a weight matrix W + bias term b):\n\nAnd that‚Äôs it! If document embeddings can be represented as a (n x d) matrix D, where n is number of documents and d is the embedding dimension, then embedding similarity is just measured by\n\nThe linear adapter is trained using a similar loss term as the MultipleNegativesRankingLoss function in sentence_transformers ‚Äî given a batch of positive (question, context) examples, the function uses cross-entropy loss under the hood to penalize the ground-truth (question, context) pairs for being far apart and swapped pairs for being too close.\nAdditional Notes: We ended up writing the bulk of this fine-tuning logic in plain PyTorch, but taking heavy inspiration from the sentence_transformers source code. We couldn‚Äôt use sentence_transformers directly since we take in embeddings as inputs rather than raw text. You can take a look at some of our training code here.\n# Notebook Walkthrough\nIn this notebook walkthrough, we follow a similar set of steps as our previous blog post on embedding fine-tuning:\n- Generate a synthetic question-context dataset for both training and evaluation.\n- Fine-tuning our linear adapter on top of an existing model (e.g. SBERT)\n- Getting the embedding model, and evaluating it.\n\nAs with the previous post, we use the UBER and LYFT 10K as example data. We use Lyft to generate our training dataset and Uber to generate our evaluation dataset.\nThe full guide is here: https://gpt-index.readthedocs.io/en/latest/examples/finetuning/embeddings/finetune_embedding_adapter.html\n## Generate a Synthetic Dataset for Trraining and Evaluation\nWe use our helper abstractions, generate_qa_embedding_']","LlamaIndex has added capabilities that allow you to fine-tune a linear adapter on top of embeddings produced from any model, such as sentence_transformers and OpenAI. This enables the transformation of embedding representations into a new latent space optimized for retrieval over specific data and queries, leading to improved retrieval performance without the need to re-embed documents.",simple,"[{'title': 'Fine-Tuning a Linear Adapter for Any Embedding Model', 'link': 'https://www.llamaindex.ai/blog/fine-tuning-a-linear-adapter-for-any-embedding-model-8dd0a142d383', 'date': 'Sep 6, 2023', 'tags': ""['Fine Tuning', 'Embedding', 'Llamaindex', 'NLP', 'AI']""}]",True
29,What is the significance of context-augmented generation in the use of Large Language Models?,"['Co-Authors:\n- Akash Sharma, founder and CEO, Vellum\n- Jerry Liu, co-founder and CEO, LlamaIndex\n\n# About Us\nThe central mission of LlamaIndex is to provide an interface between Large Language Models (LLM‚Äôs), and your private, external data. Over the past few months, it has become one of the most popular open-source frameworks for LLM data augmentation (context-augmented generation), for a variety of use cases: question-answering, summarization, structured queries, and more.\nVellum is a developer platform to build high quality LLM applications. The platform provides best-in-class tooling for prompt engineering, unit testing, regression testing, monitoring & versioning of in-production traffic and model fine tuning. Vellum‚Äôs platform helps companies save countless engineering hours to build internal tooling and instead use that time to build end user facing applications.\n# Why we partnered on this integration\nUntil recently, LlamaIndex users did not have a way to do prompt engineering and unit testing pre-production and versioning/monitoring the prompts post production. Prompt engineering and unit testing is key to ensure that your LLM feature is producing reliable results in production. Here‚Äôs an example of simple prompt that produces vastly different results between GPT-3, GPT-3.5 and GPT-4:\n\n## Unit testing your prompts\nCreating a unit test bank is a proactive approach to ensure prompt reliability ‚Äî it‚Äôs best practice to run 50‚Äì100 test cases before putting prompts in production. The test bank should comprise scenarios & edge cases anticipated in production, think of this as QAing your feature before it goes to production. The prompts should ‚Äúpass‚Äù these test cases based on your evaluation criteria. Use Vellum Test Suites to upload test cases in bulk via CSV upload.\n## Regression testing in production\nDespite how well you test before sending a prompt in production, edge cases can appear when in production. This is expected, so no stress! Through the Vellum integration, LlamaIndex users can change prompts and get prompt versioning without making any code changes. While doing that, however, it‚Äôs best practice to run historical inputs that were sent to the prompt in production to the new prompt and confirm it doesn‚Äôt break any existing behavior. LLMs are sometimes unpredictable, even changing the word ‚Äúgood‚Äù to ‚Äúgreat‚Äù in a prompt can result in differing outputs!\n\n# Best practices to leverage the integration\n## How to access the integration\nThis demo notebook goes into detail on how you can use Vellum to manage prompts within LlamaIndex.\nPrerequisites\n- Sign up for a free Vellum account at app.vellum.ai/signup\n- Go to app.vellum.ai/api-keys and generate a Vellum API key. Note it down somewhere.\n\n\nAuto-Register Prompts & Make Predictions Through Vellum\nIf you import a prompt in LlamaIndex, the VellumPredictor class will used to auto-register a prompt with Vellum to make predictions.\nBy registering a prompt with Vellum, Vellum will create:\n- A ‚ÄúSandbox‚Äù ‚Äî an environment where you can iterate on the prompt, it‚Äôs model, provider, params, etc.; and\n- A ‚ÄúDeployment‚Äù ‚Äî a thin API proxy between you and LLM providers and offering prompt versioning, request monitoring, and more\n\nYou can use VellumPromptRegistry to retrieve information about the registered prompt and get links to open its corresponding Sandbox and Deployment in Vellum‚Äôs UI. More details about Vellum‚Äôs Sandbox and Deployment features can be found here\n\n## Prompt engineering tips in context augmented use cases\nThink of the Large Language Model as a smart college graduate that needs instructions if the task at hand is not clear. If you‚Äôre not getting good results with the default prompt templates, follow these instructions:\n- Add use case specific details to the prompt to guide what the model focuses on.\n- Create 5‚Äì10 input scenarios to test performance\n- Iterate a few times: (i) Tweak the prompt by adding more specific instructions or examples for the scenarios with bad results, (ii) Evaluate against the target response for each scenario\n- In parallel, test out different foundation models and model providers using Vellum‚Äôs Sandbox. Maybe Claude or PaLM does better than GPT-4 for your use case.\n- If you would like additional reasoning or explanation, use a more prescriptive approach:\n\n- Add detailed step by step instructions to the end of the prompt and ask the LLM to walk though those steps when creating itÔøΩ']","The significance of context-augmented generation in the use of Large Language Models (LLMs) lies in its ability to enhance the performance of LLMs by providing an interface between LLMs and private, external data. This approach allows for improved question-answering, summarization, and structured queries, making it a pivotal aspect of LlamaIndex's mission to augment LLM capabilities.",simple,"[{'title': 'Vellum <> LlamaIndex Integration', 'link': 'https://www.llamaindex.ai/blog/vellum-llamaindex-integration-58b476a1e33f', 'date': 'Jun 5, 2023', 'tags': ""['NLP', 'AI', 'Mlops', 'Observability', 'Prompt Engineering']""}]",True
30,What is the significance of Faithfulness and Relevancy metrics in determining the optimal chunk size for a RAG system?,"['# Introduction\nRetrieval-augmented generation (RAG) has introduced an innovative approach that fuses the extensive retrieval capabilities of search systems with the LLM. When implementing a RAG system, one critical parameter that governs the system‚Äôs efficiency and performance is the chunk_size. How does one discern the optimal chunk size for seamless retrieval? This is where LlamaIndex Response Evaluation comes in handy. In this blog post, we\'ll guide you through the steps to determine the best chunk size using LlamaIndex‚Äôs Response Evaluation module. If you\'re unfamiliar with the Response Evaluation module, we recommend reviewing its documentation before proceeding.\n# Why Chunk Size Matters\nChoosing the right chunk_size is a critical decision that can influence the efficiency and accuracy of a RAG system in several ways:\n- Relevance and Granularity: A small chunk_size, like 128, yields more granular chunks. This granularity, however, presents a risk: vital information might not be among the top retrieved chunks, especially if the similarity_top_k setting is as restrictive as 2. Conversely, a chunk size of 512 is likely to encompass all necessary information within the top chunks, ensuring that answers to queries are readily available. To navigate this, we employ the Faithfulness and Relevancy metrics. These measure the absence of ‚Äòhallucinations‚Äô and the ‚Äòrelevancy‚Äô of responses based on the query and the retrieved contexts respectively.\n- Response Generation Time: As the chunk_size increases, so does the volume of information directed into the LLM to generate an answer. While this can ensure a more comprehensive context, it might also slow down the system. Ensuring that the added depth doesn\'t compromise the system\'s responsiveness is crucial.\n\nIn essence, determining the optimal chunk_size is about striking a balance: capturing all essential information without sacrificing speed. It\'s vital to undergo thorough testing with various sizes to find a configuration that suits the specific use case and dataset.\nFor a practical evaluation in choosing the right chunk_size, you can access and run the following setup on this Google Colab Notebook.\n# Setup\nBefore embarking on the experiment, we need to ensure all requisite modules are imported:\nimport nest_asyncio\n\nnest_asyncio.apply()\n\nfrom llama_index import (\n    SimpleDirectoryReader,\n    VectorStoreIndex,\n    ServiceContext,\n)\nfrom llama_index.evaluation import (\n    DatasetGenerator,\n    FaithfulnessEvaluator,\n    RelevancyEvaluator\n)\nfrom llama_index.llms import OpenAI\n\nimport openai\nimport time\nopenai.api_key = \'OPENAI-API-KEY\'\n# Download Data\nWe‚Äôll be using the Uber 10K SEC Filings for 2021 for this experiment.\n!mkdir -p \'data/10k/\'\n!wget \'https://raw.githubusercontent.com/jerryjliu/llama_index/main/docs/examples/data/10k/uber_2021.pdf\' -O \'data/10k/uber_2021.pdf\'\n# Load Data\nLet‚Äôs load our document.\ndocuments = SimpleDirectoryReader(""./data/10k/"").load_data()\n# Question Generation\nTo select the right chunk_size, we\'ll compute metrics like Average Response time, Faithfulness, and Relevancy for various chunk_sizes. The DatasetGenerator will help us generate questions from the documents.\ndata_generator = DatasetGenerator.from_documents(documents)\neval_questions = data_generator.generate_questions_from_nodes()\n# Setting Up Evaluators\nWe are setting up the GPT-4 model to serve as the backbone for evaluating the responses generated during the experiment. Two evaluators, FaithfulnessEvaluator and RelevancyEvaluator, are initialised with the service_context .\n- Faithfulness Evaluator ‚Äî It is useful for measuring if the response was hallucinated and measures if the response from a query engine matches any source nodes.\n- Relevancy Evaluator ‚Äî It is useful for measuring if the query was actually answered by the response and measures if the response + source nodes match the query.\n\n# We will use GPT-4 for evaluating the responses\ngpt4 = OpenAI(temperature=0, model=""gpt-4"")\n\n# Define service context for GPT-4 for evaluation\nservice_context_gpt4 = ServiceContext.from_defaults(llm=gpt4)\n\n# Define', '\n\n    average_response_time = total_response_time / num_questions\n    average_faithfulness = total_faithfulness / num_questions\n    average_relevancy = total_relevancy / num_questions\n\n    return average_response_time, average_faithfulness, average_relevancy\n\n# Iterate over different chunk sizes to evaluate the metrics to help fix the chunk size.\nfor chunk_size in [128, 256, 512, 1024, 2048]\n  avg_time, avg_faithfulness, avg_relevancy = evaluate_response_time_and_accuracy(chunk_size)\n  print(f""Chunk size {chunk_size} - Average Response time: {avg_time:.2f}s, Average Faithfulness: {avg_faithfulness:.2f}, Average Relevancy: {avg_relevancy:.2f}"")\n# Result\n\nThe above table illustrates that as the chunk size increases, there is a minor uptick in the Average Response Time. Interestingly, the Average Faithfulness seems to reach its zenith at chunk_sizeof 1024, whereas Average Relevancy shows a consistent improvement with larger chunk sizes, also peaking at 1024. This suggests that a chunk size of 1024 might strike an optimal balance between response time and the quality of the responses, measured in terms of faithfulness and relevancy.\n# Conclusion\nIdentifying the best chunk size for a RAG system is as much about intuition as it is empirical evidence. With LlamaIndex‚Äôs Response Evaluation module, you can experiment with various sizes and base your decisions on concrete data. When building a RAG system, always remember that chunk_size is a pivotal parameter. Invest the time to meticulously evaluate and adjust your chunk size for unmatched results.\n']","The Faithfulness and Relevancy metrics are significant in determining the optimal chunk size for a RAG system as they measure the absence of 'hallucinations' and the relevancy of responses based on the query and the retrieved contexts, respectively. These metrics help ensure that the responses generated are accurate and relevant to the queries, which is crucial for the efficiency and performance of the system.",simple,"[{'title': 'Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex', 'link': 'https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5', 'date': 'Oct 5, 2023', 'tags': ""['Llamaindex', 'AI', 'LLM', 'OpenAI', 'Retrieval']""}, {'title': 'Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex', 'link': 'https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5', 'date': 'Oct 5, 2023', 'tags': ""['Llamaindex', 'AI', 'LLM', 'OpenAI', 'Retrieval']""}]",True
31,What are the benefits of training ChatGPT on specific business data?,"['ChatGPT, developed by OpenAI, has changed the way we interact online. Being a general purpose chatbot, ChatGPT is limited to answering generic queries. But it becomes even more useful if you can get it to answer your questions specific to your business. To do that, you need to train ChatGPT on your data.\nEmbedAI is a no-code platform for creating AI chatbots trained on your business data. This includes data sourced from web pages, PDFs, Notion documents, or YouTube videos, allowing EmbedAI to adapt to a wide range of information sources.\nIn this blog post, we‚Äôll show you how we used LlamaIndex with EmbedAI to enable us to train ChatGPT on your own data, helping you create a customized and effective AI chatbot tailored for your business needs.\n# Chat with your data use-cases\nThere‚Äôs a variety of ways that a chatbot trained on your data could be helpful, including:\n- Customer Support Bot: Manages frequently asked questions about a product, addressing customer support inquiries efficiently.\n- Company Search Engine: Finds internal company documents and information fast, boosting workplace efficiency.\n- Personalized Learning Assistant: Offers tailored educational support and study guidance based on specific course content.\n- Technical Support assistant: Provides in-depth help for complex software issues, from troubleshooting to usage tips.\n- Healthcare Assistant: Gives general health advice and information, based on medical literature and FAQs.\n- Finance Chatbot: Assists with financial queries, offering advice on products, market trends, and investment strategies by training on financial data\n\nLet‚Äôs delve into creating our own chat apps that integrate with various data sources like PDFs, Notion documents, videos, webpages, and more.\n# Case 1: Custom ChatGPT for your site\nTo train ChatGPT on your website content, we need to scrape the content from all the relevant webpages. The steps to do this are:\n- Extract all the URLs from your website, such as from your sitemap\n- Include only relevant URLs which you need to train on\n- Use SimpleWebPageReader from LlamaIndex to download the content from these URLs\n\nHere‚Äôs some sample code to do that:\n\nOnce the data is ready, an AI chatbot can be trained on these documents by using LlamaIndex‚Äôs VectorStoreIndex class.\nTo create a ChatGPT chatbot on your website without coding you can use EmbedAI as outlined below which uses LlamaIndex internally:\n\n# Case 2: Custom ChatGPT for your PDF documents\nIf your business specific data is stored in PDF documents and you wish to create a chatbot that can surface the information in them we can do that with LlamaIndex using the PDFMiner library. This time the steps are:\n- Upload your PDFs and store them in the cloud\n- Install the PDFMiner library\n- Fetch the uploaded PDFs and extract the document text using LlamaIndex loader\n\nHere‚Äôs the code for creating an AI chatbot trained on PDF documents with LlamaIndex\n\nIf you want to create a ChatGPT chatbot on your PDF content without coding you can use EmbedAI as in the demo below which uses LlamaIndex internally\n\n# Case 3: Custom ChatGPT for your videos\nOften, valuable information is embedded in videos, which isn‚Äôt as accessible for users searching for information. However, by training an AI chatbot with this content, it can become an incredibly rich resource for your users, significantly enhancing their experience.\nLet‚Äôs see how we can fetch the information from our youtube videos to train an AI chatbot using LlamaIndex. The steps are:\n- Find your Channel ID\n- Install scrapetube and pass it your channel ID to get your list of videos\n- Install the Youtube transcript api and pass the video URLs from above to LlamaIndex loader to get a list of documents\n\nThe code looks like this:\n\nNow you can train an AI chatbot on these documents by using SimpleVectorIndex from LlamaIndex to create a ChatGPT bot trained on your youtube videos, and as before, you can use EmbedAI to create a chatbot with no code.\n# Case 4: Custom ChatGPT for Notion\nIn many modern companies, a significant portion of their content is stored in Notion. As this content grows, quickly locating specific information becomes increasingly challenging. To address this, we can develop a chatbot for Notion to streamline the process of finding the necessary information.\nSteps to prepare the data:\n- Fetch an access token from Notion following their instructions\n- Using the Notion API, parse data from Notion and generate LlamaIndex documents\n- Train a chatbot on these using VectorStoreIndex\n\n\nIf you']","Training ChatGPT on specific business data offers several benefits, including the ability to create a customized and effective AI chatbot tailored for business needs. This can enhance customer support by managing frequently asked questions, improve workplace efficiency through a company search engine, provide personalized educational support, assist with technical issues, offer healthcare advice, and help with financial queries.",simple,"[{'title': 'How to train a custom GPT on your data with EmbedAI + LlamaIndex', 'link': 'https://www.llamaindex.ai/blog/how-to-train-a-custom-gpt-on-your-data-with-embedai-llamaindex-8a701d141070', 'date': 'Dec 14, 2023', 'tags': ""['Machine Learning', 'Artificial Intelligence', 'Llamaindex', 'Customgpt', 'Python']""}]",True
32,What is the purpose of the open source project Llamafile from Mozilla?,"['This is a guest post from our friends at Mozilla about Llamafile\n\nllamafile, an open source project from Mozilla, is one of the simplest ways to run a large language model (LLM) on your laptop. All you have to do is download a llamafile from HuggingFace then run the file. That\'s it. On most computers, you won\'t need to install anything.\n\nThere are a few reasons why you might want to run an LLM on your laptop, including:\n\n1. Privacy: Running locally means you won\'t have to share your data with third parties.\n2. High availability: Run your LLM-based app without an internet connection.\n3. Bring your own model: You can easily test many different open-source LLMs (anything available on HuggingFace) and see which one works best for your task.\n4. Free debugging/testing: Local LLMs allow you to test many parts of an LLM-based system without paying for API calls.\n\nIn this blog post, we\'ll show how to set up a llamafile and use it to run a local LLM on your computer. Then, we\'ll show how to use LlamaIndex with your llamafile as the LLM & embedding backend for a local RAG-based research assistant. You won\'t have to sign up for any cloud service or send your data to any third party--everything will just run on your laptop.\nNote: You can also get all of the example code below as a Jupyter notebook from our GitHub repo.\n## Download and run a llamafile\nFirst, what is a llamafile? A llamafile is an executable LLM that you can run on your own computer. It contains the weights for a given open source LLM, as well as everything needed to actually run that model on your computer. There\'s nothing to install or configure (with a few caveats, discussed here).\n\nEach llamafile bundles 1) model weights & metadata in gguf format + 2) a copy of `llama.cpp` specially compiled using [Cosmopolitan Libc](https://github.com/jart/cosmopolitan). This allows the models to run on most computers without additional installation. llamafiles also come with a ChatGPT-like browser interface, a CLI, and an OpenAI-compatible REST API for chat models.\n\nThere are only 2 steps to setting up a llamafile:\n\n1. Download a llamafile\n2. Make the llamafile executable\n\nWe\'ll go through each step in detail below.\n### Step 1: Download a llamafile\nThere are many llamafiles available on the HuggingFace model hub (just search for \'llamafile\') but for the purpose of this walkthrough, we\'ll use TinyLlama-1.1B (0.67 GB, model info). To download the model, you can either click this download link: TinyLlama-1.1B or open a terminal and use something like `wget`. The download should take 5-10 minutes depending on the quality of your internet connection.\nwget https://huggingface.co/Mozilla/TinyLlama-1.1B-Chat-v1.0-llamafile/resolve/main/TinyLlama-1.1B-Chat-v1.0.F16.llamafile\nThis model is small and won\'t be very good at actually answering questions but, since it\'s a relatively quick download and its inference speed will allow you to index your vector store in just a few minutes, it\'s good enough for the examples below. For a higher-quality LLM, you may want to use a larger model like Mistral-7B-Instruct (5.15 GB, model info).\n### Step 2: Make the llamafile executable\nIf you didn\'t download the llamafile from the command line, figure out where your browser stored your downloaded llamafile.\n\nNow, open your computer\'s terminal and, if necessary, go to the directory where your llamafile is stored: `cd path/to/downloaded/llamafile`\n\nIf you\'re using macOS, Linux, or BSD, you\'ll need to grant permission for your computer to execute this new file. (You only need to do this once.):\n\nIf you\'re on Windows, instead just rename the file by adding "".exe"" on the end e.g. rename `TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile` to `TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile.exe`\nchmod +x TinyL']","The purpose of the open source project Llamafile from Mozilla is to provide a simple way to run a large language model (LLM) on your laptop without needing to install anything. It allows users to run LLM-based applications locally, ensuring privacy, high availability, and the ability to test different open-source LLMs easily.",simple,"[{'title': 'Using LlamaIndex and llamafile to build a local, private research assistant', 'link': 'https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant', 'date': 'May 14, 2024', 'tags': ""['llamafile']""}]",True
33,What role does the Microsoft text-to-speech synthesizer play in the video creation process?,"["", we feed the summarized context\n    from one node into the next. This ensures every node benefits from the\n    context of its predecessor. We then harness the\n    accumulate mode to provide detailed explanations, making\n    certain that every segment of the code is explained comprehensively,\n    preserving the broader perspective. The outcome? A deep, contextually rich\n    interpretation of each code section.\n\nNote: We utilized Google‚Äôs PaLM API in conjunction with LlamaIndex to generate\n  summaries and explanations. Alternatively, models like GPT-3.5, GPT-4, or\n  other LLM‚Äôs can be employed for this purpose.\n\n## 3. Video Creation with D-ID:\nAfter carefully crafting summaries and detailed explanations for each code\n  block, it‚Äôs essential to convey this information in a captivating and\n  accessible manner. Videos, given their dynamic appeal, have the power to make\n  intricate code explanations clearer and more engaging. This is where D-ID\n  comes into play.\nWith the prowess of D-ID‚Äôs cutting-edge technology, we‚Äôre able to create\n  realistic videos where avatars ‚Äî whether they‚Äôre of us or another chosen\n  figure ‚Äî articulate each code block. Now, what brings these avatars to life?\n  The answer lies in Microsoft‚Äôs text-to-speech synthesizer. This tool takes our\n  detailed textual explanations and transforms them into natural, fluent speech.\n  Thus, with D-ID, we‚Äôre not just generating video but also integrating audio,\n  culminating in a comprehensive and fluid video explanation.\nTo see this in action, let‚Äôs take a look at a sample output.\n\n## 4. Video-Code Integration:\nAfter generating insightful videos with avatars elucidating the code and\n  having our individual code snippets ready, the next crucial step is to marry\n  these two elements. This fusion ensures that viewers receive an immersive\n  visual experience, where they can simultaneously watch the explanation and\n  observe the related code.\nTo achieve this, we employed the\n  carbon library, which transforms our code snippets into visually\n  appealing images. These images, when presented side-by-side with our\n  explanatory videos, offer a clearer understanding of the code in focus. The\n  final touch is added with the moviepy library, which seamlessly\n  stitches the video and code images together, ensuring a smooth and integrated\n  visual flow. Below, you'll find a sample illustrating this compelling\n  combination.\n\n# Final Automatic Knowledge Transfer (KT) Generated Video\nFollowing our detailed process, we‚Äôve crafted a KT video where Jerry explains\n  the ChatEngine code base of LlamaIndex. Watch the video below to see it all\n  come together!\n\nCode Repository:\n  https://github.com/ravi03071991/KT_Generator\n# Conclusion\nThrough this post, we‚Äôve showcased the transformative potential of LlamaIndex\n  in creating Knowledge Transfer (KT) Videos for code bases. It‚Äôs genuinely\n  remarkable to envision the advancements we‚Äôre making in this space. The\n  methodology we‚Äôve adopted is language-neutral, allowing flexibility in\n  adapting to various code bases. With some tweaks to the code parsing phase, we\n  believe it‚Äôs feasible to scale this to cover expansive code repositories\n  within organizations. Imagine a platform akin to YouTube, perhaps\n  KodeTube(KT), where an organization‚Äôs entire codebase is\n  cataloged through explanatory videos. The horizon is bright with the\n  opportunities LlamaIndex brings, and we‚Äôre thrilled about the journey ahead.\n""]","The Microsoft text-to-speech synthesizer transforms detailed textual explanations into natural, fluent speech, which is then integrated into the videos created with D-ID, enhancing the overall video explanation.",simple,"[{'title': 'LlamaIndex: Automatic Knowledge Transfer (KT) Generation for Code Bases', 'link': 'https://www.llamaindex.ai/blog/llamaindex-automatic-knowledge-transfer-kt-generation-for-code-bases-f3d91f21b7af', 'date': 'Aug 29, 2023', 'tags': ""['Large Language Models', 'Llamaindex', 'AI', 'NLP', 'Software Development']""}]",True
34,What is Ray Serve and how does it facilitate the deployment of applications?,"[' is quite powerful; it can perform semantic search over one document, or combine results across multiple documents.\nFor instance, given the following question ‚ÄúWhat is Ray?‚Äù, we can break this into sub-questions ‚ÄúWhat is Ray according to the documentation‚Äù, and ‚ÄúWhat is Ray according to the blog posts‚Äù over the document query engine and blog query engine respectively.\n# Define a sub-question query engine, that can use the individual query engines as tools.\n        query_engine_tools = [\n            QueryEngineTool(\n                query_engine=self.ray_docs_engine,\n                metadata=ToolMetadata(name=""ray_docs_engine"", description=""Provides information about the Ray documentation"")\n            ),\n            QueryEngineTool(\n                query_engine=self.ray_blogs_engine, \n                metadata=ToolMetadata(name=""ray_blogs_engine"", description=""Provides information about Ray blog posts"")\n            ),\n        ]\n\nsub_query_engine = SubQuestionQueryEngine.from_defaults(query_engine_tools=query_engine_tools, service_context=service_context, use_async=False)\nHave a look at deploy_app.py to review the full implementation.\n# Deploying with Ray Serve\n\nWe‚Äôve now created an incredibly powerful query module over your data. As a next step, what if we could seamlessly deploy this function to production and serve users? Ray Serve makes this incredibly easy to do. Ray Serve is a scalable compute layer for serving ML models and LLMs that enables serving individual models or creating composite model pipelines where you can independently deploy, update, and scale individual components.\nTo do this, you just need to do the following steps:\n- Define an outer class that can ‚Äúwrap‚Äù a query engine, and expose a ‚Äúquery‚Äù endpoint\n- Add a `@ray.serve.deployment` decorator on this class\n- Deploy the Ray Serve application\n\nIt will look something like the following:\nfrom ray import serve\n\n@serve.deployment\nclass QADeployment:\n    def __init__(self):\n self.query_engine = ...\n\n    def query(self, query: str):\n            response =  self.query_engine.query(query)\n            source_nodes = response.source_nodes\n            source_str = """"\n            for i in range(len(source_nodes)):\n                node = source_nodes[i]\n                source_str += f""Sub-question {i+1}:\\n""\n                source_str += node.node.text\n                source_str += ""\\n\\n""\n            return f""Response: {str(response)} \\n\\n\\n {source_str}\\n""\n\n    async def __call__(self, request: Request):\n        query = request.query_params[""query""]\n        return str(self.query(query))\n\n# Deploy the Ray Serve application.\ndeployment = QADeployment.bind()\nHave a look at the deploy_app.py for full implementation.\n# Example Queries\nOnce we‚Äôve deployed the application, we can query it with questions about Ray.\nWe can query just one of the data sources:\nQ: ""What is Ray Serve?""\n\nRay Serve is a system for deploying and managing applications on a Ray\ncluster. It provides APIs for deploying applications, managing replicas, and\nmaking requests to applications. It also provides a command line interface\n(CLI) for managing applications and a dashboard']","Ray Serve is a system for deploying and managing applications on a Ray cluster. It facilitates the deployment of applications by providing APIs for deploying applications, managing replicas, and making requests to applications. Additionally, it offers a command line interface (CLI) for managing applications and a dashboard.",simple,"[{'title': 'Build and Scale a Powerful Query Engine with LlamaIndex and Ray', 'link': 'https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4', 'date': 'Jun 27, 2023', 'tags': ""['NLP', 'Distributed Systems', 'AI', 'Large Language Models', 'Parallel Computing']""}]",True
35,What role do LlamaIndex connectors play in enabling natural language access and retrieval of data?,"['The topic of Agentic RAG explores how agents can be incorporated into existing\n  RAG pipelines for enhanced, conversational search and retrieval.\n# Introduction\nConsidering the architecture below, it is evident how Agentic RAG creates\n      an implementation which easily scales. New documents can be added with\n      each new set being managed by a sub-agent.\n\nThe basic structure of LlamaIndex‚Äôs approach called Agentic RAG is shown in\n  the diagram below where a large set of documents are ingested, in this case it\n  was limited to 100.\nThe large corpus of data is broken up into smaller documents. An agent is\n  created for each document, and each of the numerous document agents have the\n  power of search via embeddings and to summarise the response.\nA top-level agent is created over the set of document agents. The meta-agent /\n  top-level agent performs tool retrieval and then uses Chain-of-Thought to\n  answer the user‚Äôs question.\nThe Rerank endpoint computes a relevance score for the query and each\n  document, and returns a sorted list from the most to the least relevant\n  document.\n\n# Notebook Example\nHere you will find a Colab\n  notebook\n  with a fully working and executed example of this\n  implementation.\nTo run the notebook, you will need an OpenAI and Cohere API key‚Ä¶\nimport osimport openaiimport cohereos.environ[""OPENAI_API_KEY""] = ""Your API Key goes here""os.environ[""COHERE_API_KEY""] = ""Your API Key goes here""\nFor each document agent, the system prompt is:\nYou are a specialized agent designed to answer queries about the `{file_base}.html` part of the LlamaIndex docs.You must ALWAYS use at least one of the tools provided when answering a question; do NOT rely on prior knowledge.\nThe system prompt for the meta-agent /top-agent:\nYou are an agent designed to answer queries about the documentation. Please always use the tools provided to answer a question. Do not rely on prior knowledge\n## Working Example\nI asked the agent the following question:\nTell me about LlamaIndex connectors\nBelow is the detailed answer generated by the agent:\nThought: I need to use a tool to help me answer the question.\nAction: tool_latest_search Action Input: {‚Äòinput‚Äô: ‚ÄòLlamaIndex\n      connectors‚Äô}\nAdded user message to memory: LlamaIndex connectors\n=== Calling Function === Calling function: vector_tool_latest_search\n  with args: { ‚Äúinput‚Äù: ‚ÄúLlamaIndex connectors‚Äù } Got output:\n  The LlamaIndex connectors can be found on the\n  /content/docs.llamaindex.ai/en/latest/search.html page.\n  ========================\nObservation: The LlamaIndex connectors are not specifically mentioned in\n      the `latest_search.html` part of the LlamaIndex docs. You may need to\n      check other sections of the documentation for detailed information about\n      LlamaIndex connectors.\nThought: I need to search for information about LlamaIndex connectors in\n      a different tool.\nAction: tool_latest_index Action Input: {‚Äòinput‚Äô: ‚ÄòLlamaIndex\n      connectors‚Äô}\nAdded user message to memory: LlamaIndex connectors\n=== Calling Function === Calling function: vector_tool_latest_index with\n  args: { ‚Äúinput‚Äù: ‚ÄúLlamaIndex connectors‚Äù }\nGot output: LlamaIndex provides data connectors that allow you to ingest your\n  existing data from various sources and formats. These connectors can work with\n  APIs, PDFs, SQL, and more. By using LlamaIndex connectors, you can easily\n  bring your data into the LlamaIndex ecosystem and make it accessible for\n  natural language access and retrieval. ========================\nObservation: LlamaIndex offers data connectors that enable you to import\n      your existing data from a variety of sources and formats. These connectors\n      are compatible with APIs, PDFs, SQL, and more. Utilizing LlamaIndex\n      connectors allows you to seamlessly integrate your data into the\n      LlamaIndex ecosystem, making it readily available for natural language\n      access and retrieval.\nThought: I can answer without using any more tools.\nAnswer: LlamaIndex connectors are used to']","LlamaIndex connectors play a crucial role in enabling natural language access and retrieval of data by allowing you to ingest existing data from various sources and formats, such as APIs, PDFs, SQL, and more. This integration makes the data accessible within the LlamaIndex ecosystem for natural language processing.",simple,"[{'title': 'Agentic RAG With LlamaIndex', 'link': 'https://www.llamaindex.ai/blog/agentic-rag-with-llamaindex-2721b8a49ff6', 'date': 'Jan 30, 2024', 'tags': ""['AI', 'Artificial Intelligence', 'Large Language Models', 'Conversational AI', 'Prompt Engineering']""}]",True
36,What is the purpose of the query engine in the context of deploying applications with Ray Serve?,"[' is quite powerful; it can perform semantic search over one document, or combine results across multiple documents.\nFor instance, given the following question ‚ÄúWhat is Ray?‚Äù, we can break this into sub-questions ‚ÄúWhat is Ray according to the documentation‚Äù, and ‚ÄúWhat is Ray according to the blog posts‚Äù over the document query engine and blog query engine respectively.\n# Define a sub-question query engine, that can use the individual query engines as tools.\n        query_engine_tools = [\n            QueryEngineTool(\n                query_engine=self.ray_docs_engine,\n                metadata=ToolMetadata(name=""ray_docs_engine"", description=""Provides information about the Ray documentation"")\n            ),\n            QueryEngineTool(\n                query_engine=self.ray_blogs_engine, \n                metadata=ToolMetadata(name=""ray_blogs_engine"", description=""Provides information about Ray blog posts"")\n            ),\n        ]\n\nsub_query_engine = SubQuestionQueryEngine.from_defaults(query_engine_tools=query_engine_tools, service_context=service_context, use_async=False)\nHave a look at deploy_app.py to review the full implementation.\n# Deploying with Ray Serve\n\nWe‚Äôve now created an incredibly powerful query module over your data. As a next step, what if we could seamlessly deploy this function to production and serve users? Ray Serve makes this incredibly easy to do. Ray Serve is a scalable compute layer for serving ML models and LLMs that enables serving individual models or creating composite model pipelines where you can independently deploy, update, and scale individual components.\nTo do this, you just need to do the following steps:\n- Define an outer class that can ‚Äúwrap‚Äù a query engine, and expose a ‚Äúquery‚Äù endpoint\n- Add a `@ray.serve.deployment` decorator on this class\n- Deploy the Ray Serve application\n\nIt will look something like the following:\nfrom ray import serve\n\n@serve.deployment\nclass QADeployment:\n    def __init__(self):\n self.query_engine = ...\n\n    def query(self, query: str):\n            response =  self.query_engine.query(query)\n            source_nodes = response.source_nodes\n            source_str = """"\n            for i in range(len(source_nodes)):\n                node = source_nodes[i]\n                source_str += f""Sub-question {i+1}:\\n""\n                source_str += node.node.text\n                source_str += ""\\n\\n""\n            return f""Response: {str(response)} \\n\\n\\n {source_str}\\n""\n\n    async def __call__(self, request: Request):\n        query = request.query_params[""query""]\n        return str(self.query(query))\n\n# Deploy the Ray Serve application.\ndeployment = QADeployment.bind()\nHave a look at the deploy_app.py for full implementation.\n# Example Queries\nOnce we‚Äôve deployed the application, we can query it with questions about Ray.\nWe can query just one of the data sources:\nQ: ""What is Ray Serve?""\n\nRay Serve is a system for deploying and managing applications on a Ray\ncluster. It provides APIs for deploying applications, managing replicas, and\nmaking requests to applications. It also provides a command line interface\n(CLI) for managing applications and a dashboard']","The purpose of the query engine in the context of deploying applications with Ray Serve is to create a powerful query module over the data, allowing for semantic search and the ability to combine results from multiple document sources. It enables the deployment of this functionality to production, serving users effectively.",simple,"[{'title': 'Build and Scale a Powerful Query Engine with LlamaIndex and Ray', 'link': 'https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4', 'date': 'Jun 27, 2023', 'tags': ""['NLP', 'Distributed Systems', 'AI', 'Large Language Models', 'Parallel Computing']""}]",True
37,What is the significance of Em City in the TV show 'Oz'?,"['ize the document!"",\n    ""output"": ""The document describes the TV show \'Oz\', set in the Oswald Maximum Security State Penitentiary. Known for its brutality, violence, and lack of privacy, it features an experimental section of the prison called Em City, where all the cells have glass fronts and face inwards. The show goes where others wouldn\'t dare, featuring graphic violence, injustice, and the harsh realities of prison life. The viewer may become comfortable with uncomfortable viewing if they can embrace their darker side.""\n  },\n  ""token_count"": 3391\n}\nNow let‚Äôs embed and store this document and ask questions using LlamaIndex‚Äôs query engine. Bring in our dependencies:\nimport os\n\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\nfrom llama_index.core.indices.vector_store import VectorStoreIndex\nfrom llama_index.core.settings import Settings\nfrom llama_index.core.readers import SimpleDirectoryReader\nfrom llama_index.llms.ollama import Ollama\nConfigure the embedding model and Llama3 model\nembed_model = HuggingFaceEmbedding(model_name=""BAAI/bge-base-en-v1.5"")\nllm = Ollama(model=""llama3"", request_timeout=300.0)\nUpdate settings for the indexing pipeline:\nSettings.llm = llm\nSettings.embed_model = embed_model\nSettings.chunk_size = 512 # This parameter defines the size of text chunks for embedding\n\ndocuments = SimpleDirectoryReader(""reviews_1_5.json"").load_data() #Modify path for your case\nNow create our index, our query engine and run a query:\nindex = VectorStoreIndex.from_documents(documents, show_progress=True)\n\nquery_engine = index.as_query_engine(similarity_top_k=3)\n\nresponse = query_engine.query(""What is the least favourite movie?"")\nprint(response)\nOutput:\nBased on query results, the least favourite movie is: review 1 with a rating of 3 out of 10.\nNow we know that the review 1 is the least favorite movie among these reviews.\n## Next Steps\nThis shows how batch inference combined with real-time inference can be a powerful tool for analyzing, storing and retrieving information from massive amounts of data. Get started with MyMagic AI‚Äôs API today!\n\n']","Em City is an experimental section of the Oswald Maximum Security State Penitentiary in the TV show 'Oz', characterized by its glass-fronted cells that face inwards. It represents a unique and brutal aspect of the prison's environment, showcasing the harsh realities of prison life and the show's willingness to explore graphic violence and injustice.",simple,"[{'title': 'Batch inference with MyMagic AI and LlamaIndex', 'link': 'https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex', 'date': 'May 22, 2024', 'tags': ""['MyMagic AI', 'Batch inference']""}]",True
38,What role does the Streamlit interface play in the Na2SQL application?,"['In the dynamic world of AI and data analytics, the ability to bridge the gap between complex data queries and non-technical users is a game-changer. My latest project, Na2SQL, showcases this exciting advancement. Leveraging the power of LlamaIndex and OpenAI‚Äôs GPT-3.5, this app allows users, regardless of their SQL knowledge, to derive valuable insights from a database using simple natural language.\n# Features\n- Intuitive Natural Language Queries: The core of this application is its ability to understand and process natural language queries. Users can ask questions in plain English and receive SQL queries and insights in return, all without any prior SQL experience.\n- Advanced Data Processing: The app doesn‚Äôt just stop at generating SQL queries; it executes these queries and analyzes the results to provide meaningful insights, making it a powerful tool for data analysis.\n- User-Friendly Interface with Streamlit: I chose Streamlit for its simplicity and effectiveness in creating interactive web applications. The app‚Äôs interface is straightforward, ensuring a smooth user experience.\n- Database Viewer: An interactive database viewer in the sidebar on the left allows users to explore the database structure, enhancing their understanding and interaction with the data.\n\n\n# The Tech Stack\nThis project harmoniously integrates several advanced technologies:\n- OpenAI‚Äôs GPT-3.5: At the heart of the application is GPT-3.5, enabling the app to understand natural natural language queries and transform them into valid SQL queries. Furthermore, it also generates the final analysis considering both the user‚Äôs query and the SQL output, thereby providing a comprehensive and relevant response.\n- LlamaIndex: A pivotal component of the app is LlamaIndex‚Äôs SQLTableQueryEngine. This powerful tool translates natural language queries into SQL, handles the execution of these queries, and plays a significant role in the subsequent analysis using GPT 3.5. Its integration ensures a smooth transition from user inputs to database insights, culminating in a meaningful final analysis that encapsulates the entire natural language-to-SQL-to-execution process.\n- LlamaIndex‚Äôs Streamlit LlamaPack: Using LlamaIndex‚Äôs Streamlit LlamaPack, we quickly assemble and highly functional Streamlit UI. This framework significantly simplifies the UI development process, allowing for rapid deployment and an enhanced user experience.\n- SQLite Database: The app interacts with an dummy SQLite ecommerce database, showcasing its ability to work with real-world data.\n\n# Deep Dive into the Code\nIn the heart of the application lies app.py, a script that brings to life the seamless interaction between natural language processing and SQL query generation.\nThis code is an evolution of the Streamlit chatbot LlamaPack available on Llama Hub, further tailored to meet the specific needs of ecommerce data analytics. Let\'s dive into some key portions of the app.py script:\n## 1. Initial Imports and Setup\nThe script begins by importing necessary modules such as Streamlit, SQLAlchemy for database interaction, LlamaIndex for language model services, and other essential libraries.\nimport streamlit as st\nfrom sqlalchemy import create_engine, inspect\nfrom typing import Dict, Any\n\nfrom llama_index import (\n    VectorStoreIndex,\n    ServiceContext,\n    download_loader,\n)\nfrom llama_index.llama_pack.base import BaseLlamaPack\nfrom llama_index.llms import OpenAI\nimport openai\nimport os\nimport pandas as pd\n## 2. StreamlitChatPack Class\nThe StreamlitChatPack class extends the base LlamaPack, setting up the page and modules necessary for the app\'s functionality.\nclass StreamlitChatPack(BaseLlamaPack):\n\n    def __init__(\n        self,\n        page: str = ""Natural Language to SQL Query"",\n        run_from_main: bool = False,\n        **kwargs: Any,\n    ) -&gt; None:\n        """"""Init params.""""""\n        self.page = page\n\n    # ... other methods ...\n## 3. The run Method\nThis method is where the magic happens. It sets up the Streamlit page configuration and initializes the chat functionality.\ndef run(self, *args: Any, **kwargs: Any) -&gt; Any:\n    """"""Run the pipeline.""""""\n    import streamlit as st\n\n    st.set_page_config(\n        page_title=f""{self']","The Streamlit interface in the Na2SQL application plays a crucial role by providing a user-friendly and straightforward interface that ensures a smooth user experience. It allows for the creation of interactive web applications, making it easier for users to interact with the application and derive insights from the database.",simple,"[{'title': 'Transforming Natural Language to SQL and Insights for E-commerce with LlamaIndex, GPT3.5, and Streamlit', 'link': 'https://www.llamaindex.ai/blog/transforming-natural-language-to-sql-and-insights-for-e-commerce-with-llamaindex-gpt3-5-e08edefa21f9', 'date': 'Dec 17, 2023', 'tags': ""['Natural Language To Sql', 'Llamaindex', 'LLM', 'Gpt35', 'OpenAI']""}]",True
39,What options are available for users to select a Large Language Model?,"['   except Exception as e:\n        st.error(f""Error saving file: {e}"")\n        return None\n## select_llm\nAllows users to select a Large Language Model and initializes it for use. You can choose from Google‚Äôs Gemini Pro, Cohere, OpenAI‚Äôs GPT 3.5 and GPT 4.\n\ndef select_llm():\n    st.header(""Choose LLM"")\n    llm_choice = st.selectbox(""Select LLM"", [""Gemini"", ""Cohere"", ""GPT-3.5"", ""GPT-4""], on_change=reset_pipeline_generated)\n    \n    if llm_choice == ""GPT-3.5"":\n        llm = OpenAI(temperature=0.1, model=""gpt-3.5-turbo-1106"")\n        st.write(f""{llm_choice} selected"")\n    elif llm_choice == ""GPT-4"":\n        llm = OpenAI(temperature=0.1, model=""gpt-4-1106-preview"")\n        st.write(f""{llm_choice} selected"")\n    elif llm_choice == ""Gemini"":\n        llm = Gemini(model=""models/gemini-pro"")\n        st.write(f""{llm_choice} selected"")\n    elif llm_choice == ""Cohere"":\n        llm = Cohere(model=""command"", api_key=os.environ[\'COHERE_API_TOKEN\'])\n        st.write(f""{llm_choice} selected"")\n    return llm, llm_choice\n## select_embedding_model\nOffers a dropdown for users to select the embedding model of their choice from a predefined list. I have included some of the top embedding models from Hugging Face‚Äôs MTEB leaderboard. Near the dropdown I have also included a handy link to the leaderboard where users can get more information about the embedding models.\n\ndef select_embedding_model():\n    st.header(""Choose Embedding Model"")\n    col1, col2 = st.columns([2,1])\n    with col2:\n        st.markdown(""""""\n                    [Embedding Models Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)\n                    """""")\n    model_names = [\n        ""BAAI/bge-small-en-v1.5"",\n        ""WhereIsAI/UAE-Large-V1"",\n        ""BAAI/bge-large-en-v1.5"",\n        ""khoa-klaytn/bge-small-en-v1.5-angle"",\n        ""BAAI/bge-base-en-v1.5"",\n        ""llmrails/ember-v1"",\n        ""jamesgpt1/sf_model_e5"",\n        ""thenlper/gte-large"",\n        ""infgrad/stella-base-en-v2"",\n        ""thenlper/gte-base""\n    ]\n    selected_model = st.selectbox(""Select Embedding Model"", model_names,  on_change=reset_pipeline_generated)\n    with st.spinner(""Please wait"") as status:\n        embed_model = HuggingFaceEmbedding(model_name=selected_model)\n        st.session_state[\'embed_model\'] = embed_model\n    ']","Users can select from the following Large Language Models: Gemini, Cohere, GPT-3.5, and GPT-4.",simple,"[{'title': 'RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex', 'link': 'https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089', 'date': 'Feb 2, 2024', 'tags': ""['Rag', 'No Code', 'Llamaindex', 'OpenAI', 'Code Generation']""}]",True
40,What are the benefits of using the PostgresML Managed Index in LlamaIndex for document storage and retrieval?,"['paul_graham/paul_graham_essay.txt\nFinally create the PostgresML Managed Index:\nfrom llama_index.core.readers import SimpleDirectoryReader\nfrom llama_index.indices.managed.postgresml import PostgresMLIndex\n\n\ndocuments = SimpleDirectoryReader(""data"").load_data()\nindex = PostgresMLIndex.from_documents(\n    documents, collection_name=""llama-index-example""\n)\nNote the collection_name is used to uniquely identify the index you are working with.\n\nHere we are using the SimpleDirectoryReader to load in the documents and then we construct the PostgresMLIndex from those documents.\n\nThis workflow does not require document preprocessing. Instead, the documents are sent directly to PostgresML where they are stored, split, and embedded per the pipeline specification. For more information on pipelines see: https://postgresml.org/docs/api/client-sdk/pipelines Custom Pipelines can be passed into the PostgresML index at creation, but by default documents are split using the recursive_character splitter and embedded with intfloat/e5-small-v2 .\n\nStep 3: Querying\nNow that we have created our index we can use it for retrieval and querying:\nretriever = index.as_retriever()\ndocs = retriever.retrieve(""Was the author puzzled by the IBM 1401?"")\nfor doc in docs:\n    print(doc)\nPostgreML does embedding and retrieval in a single network call. Compare this query against other common LlamaIndex embedding and vector storage configurations and you will notice a significant speed up.\n\nUsing the PostgresML Index as a query_engine is just as easy:\nresponse = index.as_query_engine().query(""Was the author puzzled by the IBM 1401?"")\nprint(response)\nOnce again, notice how fast the response was! The PostgresML Managed Index is doing embedding, retrieval, and augmented generation in one network call. The speed up becomes even more apparent when streaming:\nquery_engine = index.as_query_engine(streaming=True)\nresults = query_engine.query(""Was the author puzzled by the IBM 1401?"")\nfor text in results.response_gen:\n    print(text, end="""", flush=True)\nNote that by default the query_engine uses meta-llama/Meta-Llama-3-8B-Instruct but this is completely configurable.\n\nKey takeaways\nThe PostgresML Managed Index uniquely unifies embedding, vector search, and text generation into a single network call. LlamaIndex users can expect faster, more reliable, and easier-to-manage RAG workflows by using PostgresML as the backend.\n\nTo get started with PostgresML and LlamaIndex, you can follow the PostgresML intro guide to setup your account, and the examples above with your own data.\n', 'We‚Äôre happy to announce the recent integration of LlamaIndex with PostgresML ‚Äî a comprehensive machine learning platform built on PostgreSQL. The PostgresML Managed Index allows LlamaIndex users to seamlessly manage document storage, splitting, embedding, and retrieval. By using PostgresML as the backend, users benefit from a streamlined and optimized process for Retrieval-Augmented Generation (RAG). This integration unifies embedding, vector search, and text generation into a single network call, resulting in faster, more reliable, and easier-to-manage RAG workflows.\n\nThe problem with typical RAG workflows\nTypical Retrieval-Augmented Generation (RAG) workflows come with significant drawbacks, particularly for users.\n\nPoor performance is a major issue, as these workflows involve multiple network calls to different services for embedding, vector storage, and text generation, leading to increased latency. Additionally, there are privacy concerns when sensitive data is sent to various LLM providers. These user-centric issues are compounded by other challenges:\n\n- Increased dev time to master new technologies\n- Complicated maintenance and scalability issues due to multiple points of failure\n- Costly vendors required for multiple services\n\n\nThe diagram above illustrates the complexity, showing how each component interacts across different services ‚Äî exacerbating these problems.\n\nSolution\nThe PostgresML Managed Index offers a comprehensive solution to the challenges of typical RAG workflows.\n\nBy managing document storage, splitting, embedding generation, and retrieval all within a single system, PostgresML significantly reduces dev time, scaling costs, and overall spend when you eliminate the need for multiple point solutions. Most importantly, it enhances the user experience by consolidating embedding, vector search, and text generation into a single network call ‚Äî resulting in improved performance and reduced latency. Additionally, the use of open-source models ensures transparency and flexibility, while operating within the database addresses privacy concerns and provides users with a secure and efficient RAG workflow.\n\nAbout PostgresML\nPostgresML [github || website || docs] allows users to take advantage of the fundamental relationship between data and models, by moving the models to your database rather than constantly moving data to the models. This in-database approach to AI architecture results in more scalable, reliable and efficient applications. On the PostgresML cloud, you can perform vector operations, create embeddings, and generate real-time outputs in one process, directly where your data resides.\n\nKey highlights:\n- Model Serving - GPU accelerated inference engine for interactive applications, with no additional networking latency or reliability costs\n- Model Store - Access to open-source models including state of the art LLMs from Hugging Face, and track changes in performance between versions\n- Model Training - Train models with your application data using more than 50 algorithms for regression, classification or clustering tasks; fine tune pre-trained models like Llama and BERT to improve performance\n- Feature Store - Scalable access to model inputs, including vector, text, categorical, and numeric data: vector database, text search, knowledge graph and application data all in one low-latency system\n- Python and JavaScript SDKs - SDK clients can perform advanced ML/AI tasks in a single SQL request without having to transfer additional data, models, hardware or dependencies to your application\n- Serverless deployments - Enjoy instant autoscaling, so your applications can handle peak loads without overprovisioning\n\n\nPostgresML has a range of capabilities. In the following sections, we‚Äôll guide you through just one use case ‚Äì RAG ‚Äì and how to use the PostgresML Managed Index on LlamaIndex to build a better RAG app.\n\nHow it works in LlamaIndex\nLet‚Äôs look at a simple question-answering example using the PostgresML Managed Index. For this example, we will be using Paul Graham‚Äôs essays.\n\nStep 1: Get Your Database Connection String\nIf you haven‚Äôt already, create your PostgresML account. You‚Äôll get $100 in free credits when you complete your profile.\n\nSet the PGML_DATABASE_URL environment variable:\nexport PGML_DATABASE_URL=""{YOUR_CONNCECTION_STRING}""\nAlternatively, you can pass the pgml_database_url argument when creating the index.\n\nStep 2: Create the PostgresML Managed Index\nFirst install Llama_index and the PostgresML Managed Index component:\npip install llama_index llama-index-indices-managed-postgresml\nThen load in the data:\nmkdir data\ncurl -o data/paul_graham_essay.txt https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/']","The benefits of using the PostgresML Managed Index in LlamaIndex for document storage and retrieval include faster, more reliable, and easier-to-manage Retrieval-Augmented Generation (RAG) workflows. It unifies embedding, vector search, and text generation into a single network call, significantly reducing development time, scaling costs, and overall spend by eliminating the need for multiple point solutions. Additionally, it enhances user experience by consolidating processes and addresses privacy concerns by operating within the database.",simple,"[{'title': 'Simplify your RAG application architecture with LlamaIndex + PostgresML', 'link': 'https://www.llamaindex.ai/blog/simplify-your-rag-application-architecture-with-llamaindex-postgresml', 'date': 'May 28, 2024', 'tags': ""['Managed Indexes']""}, {'title': 'Simplify your RAG application architecture with LlamaIndex + PostgresML', 'link': 'https://www.llamaindex.ai/blog/simplify-your-rag-application-architecture-with-llamaindex-postgresml', 'date': 'May 28, 2024', 'tags': ""['Managed Indexes']""}]",True
41,What guidance does MistralAI's 8x22b Model Cookbook provide for using RAG and query routing?,"['Hello LlamaIndex Community! ü¶ô\nWelcome to another thrilling weekly update from LlamaWorld! We\'re excited to bring you a variety of outstanding updates, including Cookbooks, demos, guides, and tutorials.\n\n## ü§©\xa0The highlights:\n- MistralAI\'s 8x22b Model Cookbook: Released cookbook for MistralAI\'s 8x22b model with detailed guidance on RAG, query routing, and tool applications. Docs, Tweet.\n- Llama 3 Model Cookbook: A comprehensive cookbook for Meta\'s Llama 3 model from simple prompt runs to complex RAG pipeline, agents and tools, accessible directly from Hugging Face. Docs, Tweet.\n- create-llama Llama 3 template: create-llama template for Meta\'s Llama 3 to quickly start building full-stack LLM applications using the nextjs-llama3 template with a single CLI command. Tweet.\n\n\n## ‚ú® Feature Releases and Enhancements:\n- We have released a cookbook for the latest MistralAI model, the powerful 8x22b, which sets a new standard for open models. The cookbook covers RAG, query routing, and tool use cases. Docs, Tweet.\n- We have released a cookbook for latest Meta\'s new Llama 3 model, available directly from Hugging Face. This guide covers everything from running basic prompts to setting up a full RAG pipeline, agents and tools. Docs, Tweet.\n- We have introduced a template for integrating Meta\'s Llama 3 in create-llama. Simply run npx create-llama and select the nextjs-llama3 template to build full-stack LLM application with Llama 3 in one CLI command. Tweet.\n\n\n## üé•\xa0Demos:\n- Open Source AI Diagram Generator by Rohan using LlamaIndex\'s Pydantic program with partial JSON parsing and Vercel AI SDK to generate and stream diagrams dynamically for an enhanced user experience.\n- DREAM: A Distributed RAG Experimentation Framework by Aishwarya Prabhat, featuring a full-stack blueprint for optimizing RAG setups in a distributed environment. This setup includes Ray for computing, LlamaIndex for advanced techniques, Ragas for synthetic data, MinIO, MLflow, Project Jupyter, and ArgoCD.\n- Firecrawl from Mendable is an API service that crawls a given URL and converts its content, including all accessible subpages, into clean markdown format. It utilizes LlamaParse from LlamaIndex for PDF parsing.\n\n\n## üó∫Ô∏è Guides:\n- Guide to integrating Qdrant Hybrid Cloud with LlamaIndex, featuring JinaAI embeddings, MistralAI\'s Mixtral 8x7b, and our LlamaParse document parser.\n- Guide to building RAG using completely open and free components from Elastic, featuring Ollama and MistralAI, demonstrates how to assemble a RAG application with LlamaIndex using entirely free software.\n- Guide to Building a Code-Writing Agent: TechWithTimm demonstrated how to create an agent that writes code by reading your documentation. Learn how to set up local LLMs with Ollama, parse documentation using LlamaParse, build an agent, and teach it to write code.\n- Guide to Fine-tuning Embedding Models for RAG with LoRA by Mariboo demonstrates how to enhance Hugging Face models using LlamaIndex\'s finetuning techniques, including steps from quantization to fine-tuning with QLoRA.\n\n\n## ‚úçÔ∏è Tutorials:\n- Khye Wei\'s tutorial from Microsoft demonstrates how to use LlamaIndex with Azure\'s AI Search to create powerful RAG applications, including Hybrid Search, Query Rewriting, and SubQuestionQuery Engine.\n- Hanane Dupouy\'s tutorial on Building a Finance Agent with LlamaIndex to query public companies with tools for looking up stock prices, summarizing financial news, and plotting stock data, all streamlined through LlamaIndex\'s ReAct agent and API abstractions.\n- Andy Singal\'s tutorial on Building a ColBERT-powered Retrieval Agent with Memory demonstrates how to enhance a RAG pipeline with ""state"" storage for a more personalized, conversational assistant using LlamaIndex\'s custom agent and query pipeline abstractions.\n- Mariboo‚Äôs tutorial on Fine-tuning Embedding Models for RAG with LoRA using LlamaIndex\'s finetuning abstractions.\n\n']","MistralAI's 8x22b Model Cookbook provides detailed guidance on RAG (Retrieval-Augmented Generation), query routing, and tool applications.",simple,"[{'title': 'LlamaIndex Newsletter 2024-04-23', 'link': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-23', 'date': 'Apr 23, 2024', 'tags': '[]'}]",True
42,What is the purpose of LongLLMLingua in the context of prompt compression for retrieval-augmented generation (RAG) scenarios?,"[' easily lose key information because it cannot perceive the question information.\nIn recent submissions to ICLR‚Äô24, there have been some similar practices. For example, Recomp[4] reduces the use of tokens in RAG scenarios by jointly training compressors of two different granularities. RAG in Long Context[5] decomposes the long context into a series of chunks and uses retrieval methods for compression, which is actually the retrieval-based method implemented in the LongLLMLingua paper. In addition, Walking Down the Memory Maze[6] also designed a hierarchical summarization tree to enhance the LLM‚Äôs perception of key information.\n\n# Question-aware Fine-grained Prompt Compression\nIn order to make token-level prompt compression also perceive the information of the question, we propose a contrastive perplexity, which compares the difference between the perplexity distribution corresponding to the document and the perplexity distribution corresponding to the document with the question.\n\nAn intuitive feeling is that when the question serves as context, the perplexity corresponding to the relevant tokens in the document will decrease. This decrease in magnitude represents the importance of the tokens in the document relative to the question.\nFigure 3. Comparison between perplexities and contrastive perplexities of tokens in the prompt from Multi-documemnt QA dataset. The document with the ground truth is located on the left side of the dashed line.\nFigure 3 shows the distribution difference in extracting key tokens between perplexity and contrastive perplexity.\n# How to reduce the loss in the middle\nSince Coarse-grained Prompt compression far exceeds other retrieval methods in terms of accuracy, it is a very natural idea to use this ranking information to rearrange the documents that are more related to the question to the beginning and end of the prompt. However, through our testing, we found that rearranging to the beginning of the prompt is more effective than evenly distributing at both ends. So, we choose to reorder the most related document to the beginning of the prompt.\n\n# How to achieve adaptive granular control during compression?\nIn order to better use the information from the two grained compressions, in the fine-grained prompt compression, we dynamically allocate different compression ratios to different documents based on the rank information obtained from the coarse-grained compression, thereby preserving more important information from important documents.\n\n# How to improve the integrity of key information?\nSince LongLLMLingua is a token-level prompt compression, it will inevitably delete some tokens of the word, which may result in some retrieval-related tasks not getting complete results. But this can actually be recovered through a simple subsequence matching method. Specifically, there is a subsequence relationship between the original prompt, compressed prompt and response. By establishing the mapping relationship between the response subsequence that appears in the compressed prompt and the subsequence of the original prompt, the original prompt content can be effectively recovered.\n\n# Experiments\nTo evaluate the effectiveness of LongLLMLingua, we conducted detailed tests in Multi-document QA (RAG) and two long Context benchmarks. Particularly, the dataset chosen for Multi-document QA is very close to the actual RAG scenario (e.g. Bing Chat), where Contriever (one of the state-of-the-art retrieval systems) is used to recall 20 relevant documents including one ground-truth. The original documents have a high semantic relevance with the question.\n\nAs can be seen, compared to Retrieval-based methods and compression-based methods, LongLLMLingua improves performance more in the RAG scenario, and can increase up to 21.4 points at a 4x compression rate, avoiding the original ‚Äúlost in the middle‚Äù situation.\n\nThe results of the two benchmarks, LongBench and ZeroScrolls, also reached similar conclusions. LongLLMLingua is better at retaining key information related to the question in long context scenarios.\nBesides, LongLLMLingua is very efficient and can speed up the end-to-end inference process.\n\n# Used in LlamaIndex\nThank Jerry Liu for your help with the LongLLMLingua project. Now you can use LongLLMLingua as a NodePostprocessor in this widely used RAG framework. For specific usage, you can refer to the example 1, example 2 and the following code.\nfrom llama_index.query_engine import RetrieverQueryEngine\nfrom llama_index.response_synthesizers import CompactAndRefine\nfrom llama_index.indices.postprocessor import LongLLMLinguaPostprocessor\n\nnode_postprocessor = LongLLMLinguaPostprocessor(\n    instruction_str=""Given the context, please answer the final question"",\n    target_token=300,\n    rank_method=""longllmlingua"",\n    additional_compress']","The purpose of LongLLMLingua in the context of prompt compression for retrieval-augmented generation (RAG) scenarios is to improve the retention of key information related to the question in long context scenarios, enhance the effectiveness of token-level prompt compression, and avoid the loss of important information during the compression process. It achieves this by dynamically allocating different compression ratios to documents based on their relevance to the question and by establishing a mapping relationship to recover original prompt content.",simple,"[{'title': 'LongLLMLingua: Bye-bye to Middle Loss and Save on Your RAG Costs via Prompt Compression', 'link': 'https://www.llamaindex.ai/blog/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7', 'date': 'Nov 6, 2023', 'tags': ""['Large Language Models', 'LLM', 'Retrieval', 'NLP', 'ChatGPT']""}]",True
43,What is the purpose of the VectorStoreIndex in the context of image-based document extraction?,"['. we would then follow the left branch again to reach the leaf node with the value ""x"".i hope this helps clarify how tree index works!\nStep 5: Lets read the receipts\nfrom llama_index.readers.file.base import DEFAULT_FILE_READER_CLSfrom llama_index.readers.file.image_reader import ImageReaderimage_parser =ImageReader(    keep_image=True,    parse_text=True    )file_extractor = DEFAULT_FILE_READER_CLSfile_extractor.update({    "".jpg"": image_parser,    "".png"": image_parser,    "".jpeg"": image_parser,    })receipt_reader = SimpleDirectoryReader(    input_dir=""/content/data"",    file_metadata=filename_fn,    file_extractor=file_extractor,)receipt_documents = receipt_reader.load_data()print(len(receipt_documents))#Output3\nreceipts_index = VectorStoreIndex.from_documents(receipt_documents)from llama_index.query_engine import TransformQueryEnginequery_engine = receipts_index.as_query_engine()receipts_response = query_engine.query(    ""When was the last time I went to RESTAURANT and how much did I spend? this data is in your latest vector index."",)display_response(receipts_response)# Output Final Response: Based on the given context information, the last time the querying individual went to RESTAURANT was on July 5, 2019, and they spent $164.00.\n## Conclusion\nIn summary, the fusion of Zephyr 7b LLM and LlamaIndex initiates a new chapter\n  in image-based document extraction. Beyond addressing OCR‚Äôs inherent\n  challenges, it enhances the precision and efficiency of data extraction from\n  images, fostering improved productivity and decision-making in\n  document-focused workflows.\n‚ÄúStay connected and support my work through various platforms:\n- GitHub: For\n    all my open-source projects and Notebooks, you can visit my GitHub profile\n    at\n    https://github.com/andysingal. If you find my content valuable, don‚Äôt hesitate to leave a star.\n- Patreon: If you‚Äôd like to provide additional support, you can consider\n    becoming a patron on my Patreon page at\n    https://www.patreon.com/AndyShanu.\n- Medium: You\n    can read my latest articles and insights on Medium at\n    https://medium.com/@andysingal.\n- The Kaggle:\n    Check out my Kaggle profile for data science and machine learning projects\n    at\n    https://www.kaggle.com/alphasingal.\n- Hugging Face:\n    For natural language processing and AI-related projects, you can explore my\n    Huggingface profile at\n    https://huggingface.co/Andyrasika.\n- YouTube: To watch my video content, visit my YouTube channel at\n    https://www.youtube.com/@andy111007.\n- LinkedIn: To stay updated on my latest projects and posts, you can follow me\n    on LinkedIn. Here is the link to my profile:\n    https://www.linkedin.com/in/ankushsingal/.""\n\nRequests and questions: If you have a project in mind that you‚Äôd like me to\n  work on or if you have any questions about the concepts I‚Äôve explained, don‚Äôt\n  hesitate to let me know. I‚Äôm always looking for new ideas for future Notebooks\n  and I love helping to resolve any doubts you might have.\nRemember, each ‚ÄúLike‚Äù, ‚ÄúShare‚Äù, and ‚ÄúStar‚Äù greatly contributes to my work and\n  motivates me to continue producing more quality content. Thank you for your\n  support!\nIf you enjoyed this story, feel free\n  to subscribe\n  to Medium, and you will get notifications when my new articles will be\n  published, as well as full access to thousands of stories from other authors.\nResource:\n- Data used for above code\n- llama-index\n\n']","The purpose of the VectorStoreIndex in the context of image-based document extraction is to facilitate the querying of extracted data from images, enhancing the precision and efficiency of data extraction, which fosters improved productivity and decision-making in document-focused workflows.",simple,"[{'title': 'Becoming Proficient in Document Extraction', 'link': 'https://www.llamaindex.ai/blog/becoming-proficient-in-document-extraction-32aa13046ed5', 'date': 'Nov 20, 2023', 'tags': ""['Technology', 'Software Development', 'Software Engineering', 'Tech', 'Artificial Intelligence']""}]",True
44,What is the role of evolved RAG architectures in the context of long-context LLMs?,"['Google recently released Gemini 1.5 Pro with a 1M context window, available to a limited set of developers and enterprise customers. Its performance has caught the imagination of AI Twitter. It achieves 99.7% recall in the ‚ÄúNeedle in a Haystack‚Äù experiment popularized by Greg Kamradt. Early users have shared results feeding dozens of research papers, financial reports at once and report impressive results in terms of its ability to synthesize across vast troves of information.\nNaturally, this begs the question - is RAG dead? Some folks think so, while others disagree. Those in the first camp make valid points. Most small data use cases can fit within a 1-10M context window. Tokens will get cheaper and faster to process over time. Having an LLM natively interleave retrieval/generation via attention layers leads to a higher response quality compared to the one-shot retrieval present in naive RAG.\nWe were fortunate to have a preview of Gemini 1.5 Pro‚Äôs capabilities, and through playing around with it developed a thesis for how context-augmented LLM applications will evolve. This blog post clarifies our mission as a data framework along with our view of what long-context LLM architectures will look like. Our view is that while long-context LLMs will simplify certain parts of the RAG pipeline (e.g. chunking), there will need to be evolved RAG architectures to handle the new use cases that long-context LLMs bring along. No matter what new paradigms emerge, our mission at LlamaIndex is to build tooling towards that future.\n## Our Mission Goes Beyond RAG\nThe goal of LlamaIndex is very simple: enable developers to build LLM applications over their data. This mission goes beyond just RAG. To date we have invested a considerable amount of effort in advancing RAG techniques for existing LLMs, and we‚Äôve done so because it‚Äôs enabled developers to unlock dozens of new use cases such as QA over semi-structured data, over complex documents, and agentic reasoning in a multi-doc setting.\nBut we‚Äôre also excited about Gemini Pro, and we will continue to advance LlamaIndex as a production data framework in a long-context LLM future.\nAn LLM framework is intrinsically valuable. As an open-source data framework, LlamaIndex paves the cowpaths towards building any LLM use case from prototype to production. A framework makes it easier to build these use cases versus building from scratch. We enable all developers to build for these use cases, whether it‚Äôs setting up the proper architecture using our core abstractions or leveraging the hundreds of integrations in our ecosystem. No matter what the underlying LLM advancements are and whether RAG continues to exist in its current form, we continue to make the framework production-ready, including watertight abstractions, first-class documentation, and consistency.\nWe also launched LlamaCloud last week. Our mission for LlamaCloud remains building the data infra enabling any enterprise to make their vast unstructured, semi-structured, and structured data sources production-ready for use with LLMs.\n## Initial Gemini 1.5 Pro Observations\nDuring our initial testing we played around with some PDFs: SEC 10K Filings, ArXiv papers, this monster Schematic Design Binder, and more. We will do a lot more deeper analyses once the APIs are available, but in the meantime we share observations below.\nGemini results are impressive and consistent with what we‚Äôve seen in the technical report and on socials:\n- Gemini has impressive recall of specific details: We threw in 100k-1M tokens of context, and asked questions over very specific details in these documents (unstructured text and tabular data), and in all cases Gemini was able to recall the details. See above for Gemini comparing table results in the 2019 Uber 10K Filing.\n- Gemini has impressive summarization capabilities. The model can analyze large swaths of information across multiple documents and synthesize answers.\n\nThis figure shows a question-response pair from Gemini over the 2019 Uber 10K filing. The question and answer is shown at the top and the source table is shown at the bottom. Gemini is able to return the correct answer.\n\nThere are some parts where we noticed Gemini struggles a bit.\n- Gemini doesn‚Äôt read all tables and charts correctly. Gemini Pro still has a hard time being able to read figures and complex tables.\n- Gemini can take a long time. Returning an answer over the Uber 10K Filing (~160k) takes ~20 seconds. Returning an answer over the LHS Schematic Design Binder (~890k) takes ~60+ seconds.\n- Gemini can hallucinate page numbers. When asked to give a summary but also with page number citations, Gemini hallucinated the sources.\n\nAn example where Gemini']","Evolved RAG architectures are necessary to handle the new use cases that long-context LLMs bring along, even though long-context LLMs will simplify certain parts of the RAG pipeline, such as chunking.",simple,"[{'title': 'Towards Long Context RAG', 'link': 'https://www.llamaindex.ai/blog/towards-long-context-rag', 'date': 'Mar 1, 2024', 'tags': ""['LLM']""}]",True
45,What is the primary focus of the QuALITY dataset in question answering?,"['A: Aimed at question-answering over entire books and movie scripts, NarrativeQA provides question-answer pairs created from summaries of these extensive sources.\n- QuALITY: A multiple-choice question answering set based on stories and articles, QuALITY emphasizes thorough reading, with half the questions designed to be challenging and require careful consideration.\n- MuSiQue: Designed for multi-hop reasoning in question answering, MuSiQue creates multi-hop questions from single-hop ones, emphasizing connected reasoning and minimizing shortcuts.\n- HotpotQA: Based on Wikipedia, HotpotQA requires reading multiple supporting documents for reasoning. It features diverse questions and provides sentence-level support for answers.\n- MultiFieldQA-en: Curated to test long-context understanding across fields, MFQA uses sources like legal documents and academic papers, with annotations done by Ph.D. students.\n\n## Evaluation Metrics:\nThe research team used a wide range of metrics suited to each dataset. The geometric mean of ROUGE scores for QM, the exact matching (EM) score for QLTY, and F1 scores for others were the primary metrics.\n# Results\n- Baseline models without retrieval, having a 4K sequence length, performed poorly since valuable texts get truncated.\n- With retrieval, performance for 4K models like LLaMA2‚Äì70B-4K and GPT-43B-4K significantly improved.\n- HotpotQA, a multi-hop dataset, particularly benefits from longer sequence models.\n- Models with longer contexts (16K, 32K) outperform their 4K counterparts even when fed the same evidence chunks.\n- There exists a unique ‚ÄúU-shaped‚Äù performance curve for LLMs due to the lost in the middle phenomenon, making them better at utilizing information at the beginning or end of the input.\n- The study presents a contrasting perspective to LongBench‚Äôs findings, emphasizing that retrieval is beneficial for models regardless of their context window size.\n\n## Comparing to OpenAI Models:\n- The LLaMA2‚Äì70B-32k model with retrieval surpasses the performance of GPT-3.5-turbo variants and is competitive with Davinci-003, underscoring its robustness in handling long context tasks.\n\n## Comparison of Different Retrievers:\n- Retrieval consistently enhances the performance across different retrievers.\n- Public retrievers outperformed proprietary ones like OpenAI embeddings.\n\n## Comparing with the number of retrieved chunks:\n- The best performance is achieved by retrieving the top 5 or 10 chunks. Retrieving more, up to 20 chunks, doesn‚Äôt offer additional benefits and can even degrade performance.\n- The deterioration in performance when adding more chunks could be due to the lost-in-the-middle phenomenon or the model being sidetracked by non-relevant information.\n\n# Conclusion\nAs we delved deep into understanding how retrieval augmentation and long-context extension interact when applied to leading language models fine-tuned for long-context question-answering and summarization tasks. Here are some things to be noted:\n- Boost in Performance with Retrieval: Implementing retrieval techniques significantly enhances the performance of both shorter 4K context language models and their longer 16K/32K context counterparts.\n- Efficiency of 4K Models with Retrieval: 4K context language models, when combined with retrieval augmentation, can achieve performance levels similar to 16K long context models. Plus, they have the added advantage of being faster during the inference process.\n- Best Model Performance: After enhancing with both context window extension and retrieval augmentation, the standout model, LLaMA2‚Äì70B-32k-ret (LLaMA2‚Äì70B-32k with retrieval), surpasses well-known models like GPT-3.5-turbo-16k and davinci-003.\n\n# References:\n- Retrieval meets long context, large language models.\n- Longbench: A bilingual, multitask benchmark for long context understanding.\n\nWe trust that this blog post on the review of the paper on retrieval augmentation with long-context LLMs has furnished you with meaningful insights. We‚Äôre keen to hear if your experiments align with our findings or present new perspectives ‚Äî divergent results always make for interesting discussions and further exploration.\n']","The primary focus of the QuALITY dataset in question answering is to emphasize thorough reading, with half of the questions designed to be challenging and require careful consideration.",simple,"[{'title': 'NVIDIA Research: RAG with Long Context LLMs', 'link': 'https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4', 'date': 'Oct 22, 2023', 'tags': ""['Llamaindex', 'LLM', 'Nvidia', 'Research Paper', 'Llama 2']""}]",True
46,What are the key features and objectives of the Llama 2 large language models?,"[' parameters for the large language models developed? Generated Answer: The two primary objectives achieved in this work are the development and release of Llama 2, a collection of pretrained and fine-tuned large language models (LLMs), and the optimization of these models for dialogue use cases. The range of parameters for the large language models developed is from 7 billion to 70 billion.Context-1: Llama 2 : Open Foundation and Fine-Tuned Chat Models Hugo Touvron‚àóLouis Martin‚Ä†Kevin Stone‚Ä† Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic Sergey Edunov Thomas Scialom‚àó GenAI, Meta Abstract In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat , are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on ourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosed- source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs. ‚àóEqual contribution, corresponding authors: {tscialom, htouvron}@meta.com ‚Ä†Second author Contributions for all the authors can be found in Section A.1.arXiv:2307.09288v2 [cs.CL] 19 Jul 2023Context-2: (2021)alsoilluminatesthedifficultiestiedtochatbot-oriented LLMs, with concerns ranging from privacy to misleading expertise claims. Deng et al. (2023) proposes a taxonomic framework to tackle these issues, and Bergman et al. (2022) delves into the balance between potential positive and negative impacts from releasing dialogue models. InvestigationsintoredteamingrevealspecificchallengesintunedLLMs,withstudiesbyGangulietal.(2022) and Zhuoet al. (2023) showcasing a variety ofsuccessful attack typesand their effects onthe generation of harmful content. National security agencies and various researchers, such as (Mialon et al., 2023), have also raisedredflagsaroundadvancedemergentmodelbehaviors,cyberthreats,andpotentialmisuseinareaslike biological warfare. Lastly, broader societal issues like job displacement due to accelerated AI research and an over-reliance on LLMs leading to training data degradation are also pertinent considerations (Acemoglu andRestrepo,2018;AutorandSalomons,2018;Webb,2019;Shumailovetal.,2023). Wearecommittedto continuing our work engaging with the broader policy, academic, and industry community on these issues. 7 Conclusion Inthisstudy,wehaveintroduced Llama 2,anewfamilyofpretrainedandfine-tunedmodelswithscales of7billionto70billionparameters. Thesemodelshavedemonstratedtheircompetitivenesswithexisting open-source chat models, as well as competency that is equivalent to some proprietary models on evaluation setsweexamined,althoughtheystilllagbehindothermodelslikeGPT-4. Wemeticulouslyelaboratedonthe methodsandtechniquesappliedinachievingourmodels,withaheavyemphasisontheiralignmentwiththe principlesofhelpfulnessandsafety. Tocontributemoresignificantlytosocietyandfosterthepaceofresearch, wehaveresponsiblyopenedaccesst']","The key features and objectives of the Llama 2 large language models include the development and release of a collection of pretrained and fine-tuned large language models (LLMs) optimized for dialogue use cases, with a range of parameters from 7 billion to 70 billion. The models are designed to outperform open-source chat models on most benchmarks and are aligned with principles of helpfulness and safety.",simple,"[{'title': 'LlamaIndex: RAG Evaluation Showdown with GPT-4 vs. Open-Source Prometheus Model', 'link': 'https://www.llamaindex.ai/blog/llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277', 'date': 'Dec 15, 2023', 'tags': ""['Open Source', 'Llamaindex', 'Gpt 4', 'LLM', 'OpenAI']""}]",True
47,What functionalities does the Metaphor API provide for data agents?,"['retrieve_documents`.\n- Find Similar: This directly calls an endpoint offered by Metaphor, which will return a list of documents similar to a given URL.\n- Current Date: This is a convenience function that returns the current date. On its own it is unrelated to Metaphor‚Äôs API, but may be called beforehand to figure out the right date filters to pass to some of Metaphor‚Äôs endpoints.\n\nIn the next section, let‚Äôs walk through how a data agent can make use of these endpoints through various use cases.\n# Example Walkthrough\nLet‚Äôs walk through our example notebook showing how LlamaIndex data agents can be used with Metaphor.\nTesting the Metaphor Tools\nThe first step is to import the Metaphor tool spec.\n# Set up Metaphor tool\nfrom llama_hub.tools.metaphor.base import MetaphorToolSpec\nmetaphor_tool = MetaphorToolSpec(\napi_key=\'your-key\',\n)\n# convert tool spec to a list of tools\nmetaphor_tool_list = metaphor_tool.to_tool_list()\nfor tool in metaphor_tool_list:\nprint(tool.metadata.name)\nIn this walkthrough, we make use of all of the tools. But you‚Äôre free to pick and choose to use specific tools if you want to define a more custom workflow and restrict the agent action space.\nWe can play around with the set of tools before defining our agent. All of our Metaphor tools make use of the `AutoPrompt` option where Metaphor will pass a query through an LLM to refine and improve the query.\nExample input:\nmetaphor_tool.search(\'machine learning transformers\', num_results=3)\nExample output:\n[{\'title\': \'On the potential of Transformers in Reinforcement Learning\',\n\'url\': \'https://lorenzopieri.com/rl_transformers/\',\n\'id\': \'ysJlYSgeGW3l4zyOBoSGcg\'},\n{\'title\': \'Transformers: Attention in Disguise\',\n\'url\': \'https://www.mihaileric.com/posts/transformers-attention-in-disguise/\',\n\'id\': \'iEYMai5rS9k0hN5_BH0VZg\'},\n{\'title\': \'Transformers in Computer Vision: Farewell Convolutions!\',\n\'url\': \'https://towardsdatascience.com/transformers-in-computer-vision-farewell-convolutions-f083da6ef8ab?gi=a1d0a9a2896c\',\n\'id\': \'kX1Z89DdjSvBrH1S1XLvwg\'}]\nThe notebook also contains examples of us playing around with the other endpoints: retrieve_documents, find_similar, search_and_retrieve_documents.\nSetting up an OpenAI Function Calling Agent with Metaphor\nWe can create an agent with access to all of the above tools and start testing it out:\nfrom llama_index.agent import OpenAIAgent\n# We don\'t give the Agent our unwrapped retrieve document tools, instead passing the wrapped tools\nagent = OpenAIAgent.from_tools(\n  metaphor_tool_list,\n  verbose=True,\n)\nThat‚Äôs it in terms of setup! Let‚Äôs try giving an example query:\nprint(agent.chat(\'What are the best restaurants in toronto?""))\nWe walk through the execution trace of this agent to see how it is interacting with the Metaphor tool.\n=== Calling Function ===\nCalling function: search with args: {\n  ""query"": ""best restaurants in Toronto""\n}\n[Metaphor Tool] Autoprompt string: Here\'s a link to the best restaurant in Toronto:\nGot output: [{\'title\': \'Via Allegro Ristorante - Toronto Fine Dining Restaurant\', \'url\': \'https://viaallegroristorante.com/\', \'id\': \'EVlexzJh-lzkVr4tb2y_qw\'}, {\'title\': \'The Senator ‚Äì Home\', \'url\': \'https://thesenator.com/\', \'id\': \'dA3HVr5P8E0Bs7nH2gH7ZQ\'}, {\'title\': \'Home - The Rushton\', \'url\': \'https://therushton.com/\', \'id\': \'6Je-igG-i-ApqISC5XX']","The Metaphor API provides functionalities such as retrieving documents, finding similar documents, and searching for documents. It also includes convenience functions like getting the current date and allows data agents to utilize various endpoints for different use cases.",simple,"[{'title': 'LlamaIndex + Metaphor: Towards Automating Knowledge Work with LLMs', 'link': 'https://www.llamaindex.ai/blog/llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f', 'date': 'Aug 21, 2023', 'tags': ""['Search', 'LLM', 'Agents', 'NLP', 'Llamaindex']""}]",True
48,What are the advantages of using a local LLM for data privacy?,"['lama-1.1B-Chat-v1.0.Q5_K_M.llamafile\n### Kick the tires\nNow, your llamafile should be ready to go. First, you can check which version of the llamafile library was used to build the llamafile binary you should downloaded:\n./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile --version\n\nllamafile v0.7.0\nThis post was written using a model built with `llamafile v0.7.0`. If your llamafile displays a different version and some of the steps below don\'t work as expected, please post an issue on the llamafile issue tracker.\n\nThe easiest way to use your llamafile is via its built-in chat interface. In a terminal, run\n./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile\nYour browser should open automatically and display a chat interface. (If it doesn\'t, just open your browser and point it at http://localhost:8080). When you\'re done chatting, return to your terminal and hit `Control-C` to shut down llamafile. If you\'re running these commands inside a notebook, just interrupt the notebook kernel to stop the llamafile.\n\nIn the rest of this walkthrough, we\'ll be using the llamafile\'s built-in inference server instead of the browser interface. The llamafile\'s server provides a REST API for interacting with the TinyLlama LLM via HTTP. Full server API documentation is available here. To start the llamafile in server mode, run:\n./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile --server --nobrowser --embedding\n### Summary: Download and run a llamafile\n# 1. Download the llamafile-ized model\nwget https://huggingface.co/Mozilla/TinyLlama-1.1B-Chat-v1.0-llamafile/resolve/main/TinyLlama-1.1B-Chat-v1.0.F16.llamafile\n\n# 2. Make it executable (you only need to do this once)\nchmod +x TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile\n\n# 3. Run in server mode\n./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile --server --nobrowser --embedding\n## Build a research assistant using LlamaIndex and llamafile\nNow, we\'ll show how to use LlamaIndex with your llamafile to build a research assistant to help you learn about some topic of interest--for this post, we chose homing pigeons. We\'ll show how to prepare your data, index into a vector store, then query it.\n\nOne of the nice things about running an LLM locally is privacy. You can mix both ""public data"" like Wikipedia pages and ""private data"" without worrying about sharing your data with a third party. Private data could include e.g. your private notes on a topic or PDFs of classified content. As long as you use a local LLM (and a local vector store), you won\'t have to worry about leaking data. Below, we\'ll show how to combine both types of data. Our vector store will include Wikipedia pages, an Army manual on caring for homing pigeons, and some brief notes we took while we were reading about this topic.\n\nTo get started, download our example data:\nmkdir data\n\n# Download \'The Homing Pigeon\' manual from Project Gutenberg\nwget https://www.gutenberg.org/cache/epub/55084/pg55084.txt -O data/The_Homing_Pigeon.txt\n\n# Download some notes on homing pigeons\nwget https://gist.githubusercontent.com/k8si/edf5a7ca2cc3bef7dd3d3e2ca42812de/raw/24955ee9df819e21975b1dd817938c1bfe955634/homing_pigeon_notes.md -O data/homing_pigeon_notes.md\nNext, we\'ll need to install LlamaIndex and a few of its integrations:\n# Install llama-index\npip install llama-index-core\n# Install llamafile']","One of the advantages of using a local LLM for data privacy is that you can mix both public data and private data without worrying about sharing your data with a third party. This means you can use private notes or classified content without the risk of leaking data, as everything is stored and processed locally.",simple,"[{'title': 'Using LlamaIndex and llamafile to build a local, private research assistant', 'link': 'https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant', 'date': 'May 14, 2024', 'tags': ""['llamafile']""}]",True
49,What are the challenges associated with using Llama 2 for text-to-SQL tasks?,"['Llama 2 is a huge milestone in the advancement of open-source LLMs. The biggest model and its finetuned variants sit at the top of the Hugging Face Open LLM Leaderboard. Multiple benchmarks show that it is approaching GPT-3.5 (or in some cases even surpassing it) in terms of performance. All of this means that open-source LLMs are an increasingly viable and reliable option for use in complex LLM applications, from RAG systems to agents.\n# Context: Llama-2‚Äì7B is Not Good at Text-to-SQL\nA downside of the smallest Llama 2 model (7B parameters), however, is that it‚Äôs not very good at generating SQL, making it impractical for structured analytics use cases. As an example, we tried prompting Llama 2 to generate the correct SQL statement given the following prompt template:\nYou are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables. \n\nYou must output the SQL query that answers the question.\n\n### Input:\n{input}\n\n### Context:\n{context}\n\n### Response:\nHere we plugged in a sample entry from the sql-create-context dataset.\ninput: In 1981 which team picked overall 148?\ncontext: CREATE TABLE table_name_8 (team VARCHAR, year VARCHAR, overall_pick VARCHAR)\nMeanwhile, here is the generated output vs. correct output:\nGenerated output: SELECT * FROM `table_name_8` WHERE \'1980\' = YEAR AND TEAM = ""Boston Celtics"" ORDER BY OVERALL_PICK DESC LIMIT 1;\n\nCorrect output: SELECT team FROM table_name_8 WHERE year = 1981 AND overall_pick = ""148""\nThis is clearly not ideal. Unlike ChatGPT and GPT-4, Llama 2 does not reliably produce well-formatted and correct SQL outputs.\nThis is exactly where fine-tuning comes in ‚Äî given a proper corpus of text-to-SQL data, we can teach Llama 2 to be better at generating SQL outputs from natural language. At a high-level, fine-tuning involves modifying the weights of the model in some capacity. There are different ways to finetune models, from updating all parameters of the network, to a subset of the parameters, to only finetuning additional parameters (e.g. how LoRA works).\nOnce the model is finetuned, it can still be plugged into a downstream LLM application. That is exactly what this tutorial aims to show. It is a step more involved than our existing tutorials which have primarily focused on ‚Äúin-context learning‚Äù and ‚Äúretrieval-augmentation‚Äù use cases ‚Äî freezing the model itself but focusing on the orchestration of data into the input prompt. Finetuning can have a high learning curve and also require a lot of compute. This tutorial makes it as easy as possible to get started.\n# Tutorial Overview\nIn this tutorial, we show you how you can finetune Llama 2 on a text-to-SQL dataset, and then use it for structured analytics against any SQL database using the capabilities of LlamaIndex.\nHere is the stack that we use:\n- b-mc2/sql-create-context from Hugging Face datasets as the training dataset\n- OpenLLaMa open_llama_7b_v2 as the base model\n- PEFT for efficient finetuning\n- Modal for handling all cloud compute/orchestration for finetuning. And also for the excellent reference doppel-bot repo.\n- LlamaIndex for text-to-SQL inference against any SQL database.\n\nSpecial mention to the awesome Llama 2 tutorial from Anyscale that helped to inspire this project.\nAll of our materials can be found in our Github repo: https://github.com/run-llama/modal_finetune_sql (again emphasizing that this is adapted from doppel-bot). Also, the full tutorial can be found in our Jupyter notebook guide. Make sure to check it out!\nAs mentioned above, performing finetuning does require quite a few steps. Our goal is to make this as straightforward as possible to follow and use out of the box. We don‚Äôt cover all the nitty gritty detailsof Modal, PEFT, the finetuning procedure itself, etc. but we do give a rough overview.\nThere are also certainly higher-level APIs that we could‚Äôve used (e.g. OpenAI, Lamini) in order to achieve this task. There‚Äôs plenty of room for followup tutorials to cover these topics!\n## Step 1: Loading Training Data for Finetuning L']","The challenges associated with using Llama 2 for text-to-SQL tasks include its poor performance in generating SQL queries, particularly with the smallest model (7B parameters), which makes it impractical for structured analytics use cases. Unlike other models like ChatGPT and GPT-4, Llama 2 does not reliably produce well-formatted and correct SQL outputs.",simple,"[{'title': 'Easily Finetune Llama 2 for Your Text-to-SQL Applications', 'link': 'https://www.llamaindex.ai/blog/easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d', 'date': 'Aug 17, 2023', 'tags': ""['Llamaindex', 'Fine Tuning', 'Large Language Models', 'AI', 'NLP']""}]",True
50,How does time-based filtering boost vector similarity searches in RAG with Timescale Vector?,"[' import StorageContext\n\nindex = VectorStoreIndex.from_vector_store(ts_vector_store)\nquery_engine = index.as_query_engine(vector_store_kwargs = ({""start_date"": start_dt, ""end_date"":end_dt}))\n\nquery_str = ""What\'s new with TimescaleDB functions? When were these changes made and by whom?""\nresponse = query_engine.query(query_str)\nprint(str(response))\nWe asked the LLM a question about our gitlog, namely, ‚ÄúWhat‚Äôs new with TimescaleDB functions. When were these changes made and by whom?‚Äù\nHere‚Äôs the response we get, which synthesizes the nodes returned from semantic search with time-based filtering on the Timescale VectorStore:\nTimescaleDB functions have undergone changes recently. These changes include the addition of several GUCs (Global User Configuration) that allow for enabling or disabling major TimescaleDB features. Additionally, a compatibility layer has been added for the ""_timescaledb_internal"" functions, which were moved into the ""_timescaledb_functions"" schema to enhance schema security. These changes were made by Dmitry Simonenko and Sven Klemm. The specific dates of these changes are August 3, 2023, and August 29, 2023, respectively.\nThis is a simple example of a powerful concept ‚Äî using time-based context retrieval in your RAG applications can help provide more relevant answers to your users. This time-based context retrieval can be helpful to any dataset with a natural language and time component. Timescale Vector uniquely enables this thanks to its efficient time-based similarity search capabilities, and taking advantage of it in your LlamaIndex application is easy thanks to the Timescale Vector integration.\n# Resources and next steps\nNow that you‚Äôve learned how Timescale Vector can help you power better AI applications with PostgreSQL, it‚Äôs your turn to dive in. Take the next step in your learning journey by following one of the tutorials or reading one of the blog posts in the resource set below:\n- Up and Running Tutorial: learn how to use Timescale Vector in LlamaIndex using a real-world dataset. You‚Äôll learn how to use Timescale Vector as a Vectorstore, Retriever, and QueryEngine and perform time-based similarity search on vectors.\n- Timescale Vector explainer: learn more about the internals of Timescale Vector.\n- Timescale Vector website: learn more about Timescale Vector and Timescale‚Äôs AI Launch Week.\n\nüéâ And a reminder: LlamaIndex Users get Timescale Vector free for 90 days\nWe‚Äôre giving LlamaIndex users an extended 90-day trial of Timescale Vector. This makes it easy to test and develop your applications on Timescale Vector, as you won‚Äôt be charged for any cloud PostgreSQL databases you spin up during your trial period. Try Timescale Vector for free today.\n', 'Authors: Avthar Sewrathan, Matvey Arye, Jerry Liu, Yi Ding\nIntroducing the Timescale Vector integration for LlamaIndex. Timescale Vector enables LlamaIndex developers to build better AI applications with PostgreSQL as their vector database: with faster vector similarity search, efficient time-based search filtering, and the operational simplicity of a single, easy-to-use cloud PostgreSQL database for not only vector embeddings but an AI application‚Äôs relational and time-series data too.\nPostgreSQL is the world‚Äôs most loved database, according to the Stack Overflow 2023 Developer Survey. And for a good reason: it‚Äôs been battle-hardened by production use for over three decades, it‚Äôs robust and reliable, and it has a rich ecosystem of tools, drivers, and connectors.\nAnd while pgvector, the open-source extension for vector data on PostgreSQL, is a wonderful extension (and all its features are offered as part of Timescale Vector), it is just one piece of the puzzle in providing a production-grade experience for AI application developers on PostgreSQL. After speaking with numerous developers at nimble startups and established industry giants, we saw the need to enhance pgvector to cater to the performance and operational needs of developers building AI applications.\nHere‚Äôs the TL;DR on how Timescale Vector helps you build better AI applications with LlamaIndex:\n- Faster similarity search on millions of vectors: Thanks to the introduction of a new search index inspired by the DiskANN algorithm, Timescale Vector achieves 3X faster search speed at ~99% recall than a specialized database and outperforms all existing PostgreSQL search indexes by between 39.39% and 1,590.33% on a dataset of one million OpenAI embeddings. Plus, enabling product quantization yields a 10x index space savings compared to pgvector. Timescale Vector also offers pgvector‚Äôs Hierarchical Navigable Small Worlds (HNSW) and Inverted File Flat (IVFFlat) indexing algorithms.\n- Efficient similarity search with time-based filtering: Timescale Vector optimizes time-based vector search queries, leveraging the automatic time-based partitioning and indexing of Timescale‚Äôs hypertables to efficiently find recent embeddings, constrain vector search by a time range or document age, and store and retrieve large language model (LLM) response and chat history with ease. Time-based semantic search also enables you to use Retrieval Augmented Generation (RAG) with time-based context retrieval to give users more useful LLM responses.\n- Simplified AI infra stack: By combining vector embeddings, relational data, and time-series data in one PostgreSQL database, Timescale Vector eliminates the operational complexity that comes with managing multiple database systems at scale.\n- Simplified metadata handling and multi-attribute filtering: Developers can leverage all PostgreSQL data types to store and filter metadata and JOIN vector search results with relational data for more contextually relevant responses. In future releases, Timescale Vector will further optimize rich multi-attribute filtering, enabling even faster similarity searches when filtering on metadata.\n\nOn top of these innovations for vector workloads, Timescale Vector provides a robust, production-ready cloud PostgreSQL platform with flexible pricing, enterprise-grade security, and free expert support.\nIn the rest of this post, we‚Äôll dive deeper (with code!) into the unique capabilities Timescale Vector enables for developers wanting to use PostgreSQL as their vector database with LlamaIndex:\n- Faster similarity search with DiskANN, HNSW and IVFFlat index types.\n- Efficient similarity search when filtering vectors by time.\n- Retrieval Augmented Generation (RAG) with time-based context retrieval.\n\n(If you want to jump straight to the code, explore this tutorial).\nüéâ LlamaIndex Users Get 3 Months of Timescale Vector for Free\nWe‚Äôre giving LlamaIndex users an extended 90-day trial of Timescale Vector. This makes it easy to test and develop your applications on Timescale Vector, as you won‚Äôt be charged for any cloud PostgreSQL databases you spin up during your trial period. Try Timescale Vector for free today.\n# Faster Vector Similarity Search in PostgreSQL\nTimescale Vector speeds up Approximate Nearest Neighbor (ANN) search on large-scale vector datasets, enhancing pgvector with a state-of-the-art ANN index inspired by the DiskANN algorithm. Timescale Vector also offers pgvector‚Äôs HNSW and IVFFlat indexing algorithms, giving developers the flexibility to choose the right index for their use case.\nOur performance benchmarks using the ANN benchmarks suite show that Timescale Vector achieves between 39.43% and 1,590.33% faster search speed at ~99% recall than all existing PostgreSQL search indexes, and 3X faster']","Time-based filtering boosts vector similarity searches in RAG with Timescale Vector by optimizing time-based vector search queries. It leverages automatic time-based partitioning and indexing of Timescale‚Äôs hypertables to efficiently find recent embeddings, constrain vector search by a time range or document age, and store and retrieve large language model (LLM) response and chat history with ease. This allows for more relevant and contextually appropriate responses to users.",multi_context,"[{'title': 'Timescale Vector x LlamaIndex: Making PostgreSQL a Better Vector Database for AI Applications', 'link': 'https://www.llamaindex.ai/blog/timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0', 'date': 'Sep 27, 2023', 'tags': ""['Llamaindex', 'NLP', 'LLM', 'AI', 'Timescaledb']""}, {'title': 'Timescale Vector x LlamaIndex: Making PostgreSQL a Better Vector Database for AI Applications', 'link': 'https://www.llamaindex.ai/blog/timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0', 'date': 'Sep 27, 2023', 'tags': ""['Llamaindex', 'NLP', 'LLM', 'AI', 'Timescaledb']""}]",True
51,How does structured output extraction boost multi-modal text-image integration?,"['(model=""models/gemini-ultra"")\n# Multi-modal Model\nFull Notebook Guide Here\nIn this notebook, we test out the gemini-pro-vision variant that features multi-modal inputs. It contains the following features:\n- supports both complete and chat capabilities\n- supports streaming and async\n- Supports feeding in multiple images in addition to text in the completion endpoint\n- Future work: multi-turn chat interleaving text and images is supported within our abstraction, but is not yet enabled for gemini-pro-vision.\n\nLet‚Äôs walk through a concrete example. Let‚Äôs say we are given a picture of the following scene:\nScene from a street in New York City\nWe can then initialize our Gemini Vision model, and ask it a question: ‚ÄúIdentify the city where this photo was taken‚Äù:\nfrom llama_index.multi_modal_llms.gemini import GeminiMultiModal\nfrom llama_index.multi_modal_llms.generic_utils import (\n    load_image_urls,\n)\n\nimage_urls = [\n    ""&lt;https://storage.googleapis.com/generativeai-downloads/data/scene.jpg&gt;"",\n    # Add yours here!\n]\nimage_documents = load_image_urls(image_urls)\ngemini_pro = GeminiMultiModal(model=""models/gemini-pro"")\ncomplete_response = gemini_pro.complete(\n    prompt=""Identify the city where this photo was taken."",\n    image_documents=image_documents,\n)\nOur response is the following:\nNew York City\nWe can insert multiple images too. Here‚Äôs an example with an image of Messi and the Colosseum.\nimage_urls = [\n    ""&lt;https://www.sportsnet.ca/wp-content/uploads/2023/11/CP1688996471-1040x572.jpg&gt;"",\n    ""&lt;https://res.cloudinary.com/hello-tickets/image/upload/c_limit,f_auto,q_auto,w_1920/v1640835927/o3pfl41q7m5bj8jardk0.jpg&gt;"",\n]\nimage_documents_1 = load_image_urls(image_urls)\nresponse_multi = gemini_pro.complete(\n    prompt=""is there any relationship between those images?"",\n    image_documents=image_documents_1,\n)\nprint(response_multi)\n# Multi-Modal Use Cases (Structured Outputs, RAG)\nFull Notebook Guide Here\nWe‚Äôve created extensive resources about different multi-modal use cases, from structured output extraction to RAG.\nThanks to Haotian Zhang, we have examples for both these use cases with Gemini. Please see our extensive notebook guides for more details. In the meantime here‚Äôs the final results!\nStructured Data Extraction with Gemini Pro Vision\nScreenshot of a Google Maps Restaurant Listing\nOutput:\n(\'restaurant\', \'La Mar by Gaston Acurio\')\n(\'food\', \'South American\')\n(\'location\', \'500 Brickell Key Dr, Miami, FL 33131\')\n(\'category\', \'Restaurant\')\n(\'hours\', \'Open ‚ãÖ Closes 11 PM\')\n(\'price\', 4.0)\n(\'rating\', 4)\n(\'review\', \'4.4 (2,104)\')\n(\'description\', \'Chic waterfront find offering Peruvian & fusion fare, plus bars for cocktails, ceviche & anticucho.\')\n(\'nearby_tourist_places\', \'Brickell Key Park\')\nMulti-Modal RAG\nWe run our structured output extractor on multiple restaurant images, index these nodes, and then ask a question ‚ÄúRecommend a Orlando restaurant for me and its nearby tourist places‚Äù\nI recommend Mythos Restaurant in Orlando. It is an American restaurant located at 6000 Universal Blvd, Orlando, FL 32819, United States. It has a rating of 4 and a review score of 4.3 based on 2,115 reviews. The restaurant offers a mythic underwater-themed dining experience with a view of Universal Studios\' Inland Sea. It is located near popular tourist places such as Universal\'s Islands of Adventure, Skull Island: Reign of Kong, The Wizarding World of Harry Potter, Jurassic Park River Adventure, Hollywood Rip Ride Rockit, and Universal Studios Florida.\n# Semantic Retriever', '(co-authored by Jerry Liu, Haotian Zhang, Logan Markewich, and Laurie Voss @ LlamaIndex)\nToday is Google‚Äôs public release of its latest AI model, Gemini. We‚Äôre excited to be a day 1 launch partner for Gemini, with support immediately available in LlamaIndex today!\nAs of 0.9.15, LlamaIndex offers full support for all currently released and upcoming Gemini models (Gemini Pro, Gemini Ultra). We support both a ‚Äútext-only‚Äù Gemini variant with a text-in/text-out format as well as a multimodal variant that takes in both text and images as input, and outputs text. We‚Äôve made some fundamental multi-modal abstraction changes to support the Gemini multi-modal interface, which allows users to input multiple images along with text. Our Gemini integrations are also feature-complete: they support (non-streaming, streaming), (sync, async), and (text completion, chat message) formats ‚Äî 8 combinations in total.\nIn addition, we also support the brand-new Semantic Retriever API, which bundles storage, embedding models, retrieval, and LLM in a RAG pipeline. We show you how it can be used on its own, or decomposed+bundled with LlamaIndex components to create advanced RAG pipelines.\nHuge shoutout to the Google Labs and Semantic Retriever teams for helping us get setup with early access.\n- Google Labs: Mark McDonald, Josh Gordon, Arthur Soroken\n- Semantic Retriever: Lawrence Tsang, Cher Hu\n\nThe below sections contain a detailed walkthrough of both our brand-new Gemini and Semantic Retriever abstractions in LlamaIndex. If you don‚Äôt want to read that now, make sure you bookmark our detailed notebook guides below!\n- Gemini (text-only) Guide\n- Gemini (multi-modal) Guide\n- Semantic Retriever Guide\n\n# Gemini Release and Support\nThere‚Äôs been a ton of press around Gemini, which boasts impressive performance at a variety of benchmarks. The Ultra variants (which are not yet publicly available) outperform GPT-4 on benchmarks from MMLU to Big-Bench Hard to math and coding tasks. Their multimodal demos demonstrate joint image/text understanding from domains like scientific paper understanding to literature review.\nLet‚Äôs walk through examples of using Gemini in LlamaIndex. We walk through both the text model (from llama_index.llms import Gemini) as well as the multi-modal model (from llama_index.multi_modal_llms.gemini import GeminiMultiModal)\n# Text Model\nFull Notebook Guide Here\nWe start with the text model. In the code snippet below, we show a bunch of different configurations, from completion to chat to streaming to async.\nfrom llama_index.llms import Gemini\n\n# completion\nresp = Gemini().complete(""Write a poem about a magic backpack"")\n# chat\nmessages = [\n    ChatMessage(role=""user"", content=""Hello friend!""),\n    ChatMessage(role=""assistant"", content=""Yarr what is shakin\' matey?""),\n    ChatMessage(\n        role=""user"", content=""Help me decide what to have for dinner.""\n    ),\n]\nresp = Gemini().chat(messages)\n# streaming (completion)\nllm = Gemini()\nresp = llm.stream_complete(\n    ""The story of Sourcrust, the bread creature, is really interesting. It all started when...""\n)\n# streaming (chat)\nllm = Gemini()\nmessages = [\n    ChatMessage(role=""user"", content=""Hello friend!""),\n    ChatMessage(role=""assistant"", content=""Yarr what is shakin\' matey?""),\n    ChatMessage(\n        role=""user"", content=""Help me decide what to have for dinner.""\n    ),\n]\nresp = llm.stream_chat(messages)\n# async completion\nresp = await llm.acomplete(""Llamas are famous for "")\nprint(resp)\n# async streaming (completion)\nresp = await llm.astream_complete(""Llamas are famous for "")\nasync for chunk in resp:\n    print(chunk.text, end="""")\nThe Gemini class of course has parameters that can be set. This includes model_name, temperature, max_tokens, and generate_kwargs.\nAs an example, you can do:\nllm = Gemini']",The answer to given question is not present in context,multi_context,"[{'title': 'LlamaIndex + Gemini', 'link': 'https://www.llamaindex.ai/blog/llamaindex-gemini-8d7c3b9ea97e', 'date': 'Dec 13, 2023', 'tags': ""['Llamaindex', 'Gemini', 'LLM', 'AI', 'Multimodal']""}, {'title': 'LlamaIndex + Gemini', 'link': 'https://www.llamaindex.ai/blog/llamaindex-gemini-8d7c3b9ea97e', 'date': 'Dec 13, 2023', 'tags': ""['Llamaindex', 'Gemini', 'LLM', 'AI', 'Multimodal']""}]",True
52,What topics does the Llama 3 Model Cookbook cover for RAG setups and agents?,"['Hello LlamaIndex Community! ü¶ô\nWelcome to another thrilling weekly update from LlamaWorld! We\'re excited to bring you a variety of outstanding updates, including Cookbooks, demos, guides, and tutorials.\n\n## ü§©\xa0The highlights:\n- MistralAI\'s 8x22b Model Cookbook: Released cookbook for MistralAI\'s 8x22b model with detailed guidance on RAG, query routing, and tool applications. Docs, Tweet.\n- Llama 3 Model Cookbook: A comprehensive cookbook for Meta\'s Llama 3 model from simple prompt runs to complex RAG pipeline, agents and tools, accessible directly from Hugging Face. Docs, Tweet.\n- create-llama Llama 3 template: create-llama template for Meta\'s Llama 3 to quickly start building full-stack LLM applications using the nextjs-llama3 template with a single CLI command. Tweet.\n\n\n## ‚ú® Feature Releases and Enhancements:\n- We have released a cookbook for the latest MistralAI model, the powerful 8x22b, which sets a new standard for open models. The cookbook covers RAG, query routing, and tool use cases. Docs, Tweet.\n- We have released a cookbook for latest Meta\'s new Llama 3 model, available directly from Hugging Face. This guide covers everything from running basic prompts to setting up a full RAG pipeline, agents and tools. Docs, Tweet.\n- We have introduced a template for integrating Meta\'s Llama 3 in create-llama. Simply run npx create-llama and select the nextjs-llama3 template to build full-stack LLM application with Llama 3 in one CLI command. Tweet.\n\n\n## üé•\xa0Demos:\n- Open Source AI Diagram Generator by Rohan using LlamaIndex\'s Pydantic program with partial JSON parsing and Vercel AI SDK to generate and stream diagrams dynamically for an enhanced user experience.\n- DREAM: A Distributed RAG Experimentation Framework by Aishwarya Prabhat, featuring a full-stack blueprint for optimizing RAG setups in a distributed environment. This setup includes Ray for computing, LlamaIndex for advanced techniques, Ragas for synthetic data, MinIO, MLflow, Project Jupyter, and ArgoCD.\n- Firecrawl from Mendable is an API service that crawls a given URL and converts its content, including all accessible subpages, into clean markdown format. It utilizes LlamaParse from LlamaIndex for PDF parsing.\n\n\n## üó∫Ô∏è Guides:\n- Guide to integrating Qdrant Hybrid Cloud with LlamaIndex, featuring JinaAI embeddings, MistralAI\'s Mixtral 8x7b, and our LlamaParse document parser.\n- Guide to building RAG using completely open and free components from Elastic, featuring Ollama and MistralAI, demonstrates how to assemble a RAG application with LlamaIndex using entirely free software.\n- Guide to Building a Code-Writing Agent: TechWithTimm demonstrated how to create an agent that writes code by reading your documentation. Learn how to set up local LLMs with Ollama, parse documentation using LlamaParse, build an agent, and teach it to write code.\n- Guide to Fine-tuning Embedding Models for RAG with LoRA by Mariboo demonstrates how to enhance Hugging Face models using LlamaIndex\'s finetuning techniques, including steps from quantization to fine-tuning with QLoRA.\n\n\n## ‚úçÔ∏è Tutorials:\n- Khye Wei\'s tutorial from Microsoft demonstrates how to use LlamaIndex with Azure\'s AI Search to create powerful RAG applications, including Hybrid Search, Query Rewriting, and SubQuestionQuery Engine.\n- Hanane Dupouy\'s tutorial on Building a Finance Agent with LlamaIndex to query public companies with tools for looking up stock prices, summarizing financial news, and plotting stock data, all streamlined through LlamaIndex\'s ReAct agent and API abstractions.\n- Andy Singal\'s tutorial on Building a ColBERT-powered Retrieval Agent with Memory demonstrates how to enhance a RAG pipeline with ""state"" storage for a more personalized, conversational assistant using LlamaIndex\'s custom agent and query pipeline abstractions.\n- Mariboo‚Äôs tutorial on Fine-tuning Embedding Models for RAG with LoRA using LlamaIndex\'s finetuning abstractions.\n\n', ""Hello, LlamaIndex members! ü¶ô\nWelcome to another thrilling weekly update from LlamaUniverse! We're excited to present a variety of outstanding updates, including Anthropic's Function Calling, Cookbooks, RankLLM, Guides, Tutorials, and much more.\n\n## ü§©\xa0The highlights:\n- Anthropic's Claude Function Calling Agent: Enhance QA/RAG and workflow automation with advanced tool calling in an agent framework. Notebook, Tweet.\n- RankLLM Integration: RankLLM is an open-source LLM collection for reranking, surpassing GPT-4 based alternatives is now integrated with LlamaIndex. Notebook, Tweet.\n- LlamaIndex + MistralAI Cookbook Series: Launched a cookbook series with MistralAI for building diverse RAG applications, from basic to advanced, with distinctive methods and abstractions. Cookbooks, Tweet\n\n\n## ‚ú® Feature Releases and Enhancements:\n- We have introduced the Anthropic‚Äôs Claude Function Calling Agent, leveraging advanced tool calling capabilities within an agent framework for enhanced QA/RAG and workflow automation. Notebook, Tweet.\n- RankLLM (by Ronak Pradeep) integration with LlamaIndex - an open-source LLM collection fine-tuned for reranking, offering top-notch results and outperforming GPT-4 based rerankers. Notebook, Tweet.\n- We have launched the LlamaIndex + MistralAI Cookbook Series for creating a range of RAG applications, from simple setups to advanced agents, featuring unique abstractions and techniques. Cookbooks, Tweet\n- We launched create-llama for building full-stack RAG/agent applications with a single CLI command, akin to create-react-app, for a comprehensive chatbot setup including tool use. Tweet.\n\n\n## üé•\xa0Demos:\n- AutoRAG by Marker-Inc-Korea: Streamline RAG pipeline optimization with an automated three-step process, from data preparation to evaluation and optimal pipeline adoption, enhancing the efficiency of the RAG pipeline using LlamaIndex.\n\n\n## üó∫Ô∏è Guides:\n- Guide to Building Advanced RAG with Temporal Filters: Learn how to enhance your RAG pipeline with time-based metadata for more effective financial report analysis using LlamaIndex and KDB.AI vector store.\n- Guide to Adaptive RAG for dynamically selecting RAG strategies based on query complexity, enhancing efficiency across varying question types.\n\n\n## ‚úçÔ∏è Tutorials:\n- (Œªx.x)eranga‚Äôs tutorial on the step-by-step process for building RAG with local models (LlamaIndex, Ollama, HuggingFace Embeddings, ChromaDB) and wrapping it all in a Flask server.\n- Ivan Ilin‚Äôs video tutorial on iki.ai - an LLM-powered digital library, for organizing, and sharing information within teams or organizations.\n- Tutorial on scaling LLM Applications with Koyeb on deploying a full-stack RAG application globally without infrastructure setup, using Koyeb, LlamaIndex.TS, and TogetherAI.\n- Ankush Singal's tutorial on Building Multi-Document Agents with LlamaIndex covers advanced multi-document agent concepts, where documents serve as sub-agents enabling complex QA, semantic search, and summarization.\n- Rohan‚Äôs tutorial on building a Full-Stack RAG application that streams intermediate results to visual UI components with event queues and server-side events.\n- Hanane Dupouy's tutorial on building a Finance Agent using an LLM with Yahoo Finance and LlamaIndex abstractions to analyze financial data for publicly traded companies, covering everything from balance sheets to stock recommendations.\n\n\n## üé•\xa0Webinars:\n- Webinar with Daniel Huynh ****featuring LaVague - an agent that can navigate the web in your Jupyter/Colab notebook.\n- Webinar with Logan Kelly featuring CallSine that utilizes LlamaIndex abstractions and LLMs for personalized sales outreach.\n\n""]","The Llama 3 Model Cookbook covers everything from running basic prompts to setting up a full RAG pipeline, agents, and tools.",multi_context,"[{'title': 'LlamaIndex Newsletter 2024-04-23', 'link': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-23', 'date': 'Apr 23, 2024', 'tags': '[]'}, {'title': 'LlamaIndex Newsletter 2024-04-09', 'link': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-09', 'date': 'Apr 9, 2024', 'tags': '[]'}]",True
53,What benefits does an AI assistant bring to crisis counseling for improving counselor efficiency and meeting mental health service demand?,"[' to assess the stage of the conversation, given that there are recommended styles and questions for each stage (e.g. establishing rapport, risk assessment)\n- Closed-loop feedback cycle: Allow counselors to thumbs-up or thumbs-down selected responses, as a natural way to collect human feedback which can be used for further model or agent training\n\n## Conclusion: A Step Toward Efficient and Effective Crisis Care\nOur AI copilot for crisis counselors represents a significant step toward more efficient and effective crisis care. By automating administrative tasks, it frees up counselors to focus on their core mission of providing youth in crisis a safe place to talk. This not only enhances the quality of care provided but also addresses the pressing issue of counselor shortage by maximizing the impact of existing resources. As we continue to refine and expand this technology, we envision a future where crisis counseling is more accessible, responsive, and impactful for all those in need\u200b\u200b.\n## References\n- https://rag-a-thon.devpost.com/\n- https://devpost.com/software/counselor-copilot\n- https://www.mhanational.org/issues/state-mental-health-america\n- https://www.aamc.org/news/growing-psychiatrist-shortage-enormous-demand-mental-health-services\n- https://www.thetrevorproject.org/\n- https://github.com/zrizvi93/trevorhack\n\n', 'Co-authored by: Riya Jagetia,\xa0Tarun Malik,\xa0Divija N,\xa0Sharon Tan,\xa0Zehra Rizvi, Amanda Piyapanee\nAt the recent LlamaIndex RAG-a-thon [1], our team‚Äôs ‚ÄúCounselor Copilot‚Äù won 2nd prize in the Traditional track and 1st prize in the Datastax/AstraDB category. More details can be found on our DevPost [2] writeup.\n## Introduction\nAgainst the backdrop of growing strain on mental health services [3, 4], non-profit organizations like The Trevor Project [5] are a critical part of the care ecosystem. Focusing on helping LGBTQ+ youth who are contemplating suicide, The Trevor Project provides accessible crisis services including via TrevorText, an online chat service with trained volunteer counselors.\n## Problem: The Dual Challenge Faced by Crisis Counselors\nHowever, TrevorText counselors face significant challenges. Not only is there high demand for counselors during busy times like holidays and night shifts, but also, counselors have to juggle a number of administrative tasks such as sifting through forms, responding to messages across multiple chats, and locating relevant local resources. This not only increases the risk of counselors burning out but also hampers their ability to provide timely and effective care.\nIn light of these challenges, there‚Äôs a pressing need for innovative solutions to bridge the gap between the demand and supply of crisis services.\nWhile our hackathon project focused on augmenting TrevorText, our product can be easily extended to general crisis chat alternatives as well.\n## The Winning Solution: An AI Copilot for Crisis Counselors\nCounselor Copilot is a real-time assistant for crisis counselors that takes into account contact context and chat history to suggest replies so that counselors can focus on what they do best: providing care. There is no prompting that is needed from counselors; the copilot works seamlessly in the background.\nFurther, the copilot never directly replies to contacts; instead, replies are suggested and can be edited.\nCounselor copilot takes into account contact context and chat history to provide real-time reply suggestions to the counselors\nSpecifically, the copilot automates counselor tasks that include but are not limited to:\n- Retrieving and synthesizing contact data from complex PDFs in real-time. This also provides counselors context on their contacts when conversations are initiated.\n- Assessing from the chat context if emergency intervention is required. If so, suggesting escalation to a supervisor.\n- Using existing resources and guidelines from the organization to suggest appropriate replies.\n\n\n4. Searching for location-specific resources for contacts, and quickly sharing those resources via email.\n\n5. Completing case forms in a CRM for contacts, including summarizing the interaction.\n\nWhile these tasks are important and necessary, they pull attention away from conversations with youth in crisis and take up precious time.\nWith Counselor copilot, these tasks are completed when they are required and without any prompting from counselors, providing more bandwidth for counselors and ultimately leading to higher-quality conversations with patients.\nBelow is a demo of our solution:\n## How we built it\nWhen the chat is initiated, the Counselor Copilot gets the contact‚Äôs data from the CRM, which is stored in complex PDFs. We used LlamaParse to extract relevant contact data in real-time and then provide a summary of that data to counselors as context at the beginning of each conversation.\n\nFurther, we used a LlamaIndex ReAct Agent to monitor the conversation and ‚Äî based on the chat history and contact context ‚Äî deploy the right tool. Tools at the ReAct Agent‚Äôs disposal include:\n- Escalating the conversation to a supervisor\n- Suggesting a response and related resources based on The Trevor Project‚Äôs guidelines\n- Searching the web for location-specific resources and sending the resources to the contact\n\nFor tool #2, we created a vector database that contains The Trevor Project‚Äôs documents, which highlight key guidelines for counselors based on different scenarios and situations that they may face. We used RAG to retrieve resources relevant to the conversation, and GPT4 to draft a response for the counselor based on those resources, both of which are essential due to the sensitive nature of the conversation.\nLastly, we used the conversation content to fill out a form with key Salesforce fields (e.g. name, age, city, state), as well as to summarize the conversation.\n## Possible Extensions\nWe‚Äôre excited by the potential for others to build on our work [6] and extend Counselor Copilot further. Some ideas include:\n- Reduce costs and improve quality of suggested responses: Fine-tune a state-of-the-art open-source LLM on extracts of chat conversations conducted by counselors\n- More targeted conversation management: Add a tool for the agent']","The AI assistant, Counselor Copilot, brings several benefits to crisis counseling, including automating administrative tasks, which allows counselors to focus on providing care. This enhances the quality of care and addresses the counselor shortage by maximizing the impact of existing resources. By suggesting replies based on contact context and chat history, it improves counselor efficiency and enables more timely and effective responses to youth in crisis.",multi_context,"[{'title': 'Bridging the Gap in Crisis Counseling: Introducing Counselor Copilot', 'link': 'https://www.llamaindex.ai/blog/bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3', 'date': 'Feb 24, 2024', 'tags': ""['Llamaindex', 'Mental Health', 'Retrieval Augmented', 'Generative Ai Use Cases', 'LLM']""}, {'title': 'Bridging the Gap in Crisis Counseling: Introducing Counselor Copilot', 'link': 'https://www.llamaindex.ai/blog/bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3', 'date': 'Feb 24, 2024', 'tags': ""['Llamaindex', 'Mental Health', 'Retrieval Augmented', 'Generative Ai Use Cases', 'LLM']""}]",True
54,What do LlamaParse and LlamaCloud contribute to data ingestion and parsing for RAG systems and vector DBs?,"['ex.ai/. While you‚Äôre waiting for official LlamaCloud access, anyone can immediately start using our LlamaParse APIs.\nWe‚Äôre shipping a lot of features in the next few weeks. We look forward to seeing the context-augmented LLM applications that you can build on top of LlamaCloud! üöÄü¶ô\nFAQ\nHave you got some examples of how to use LlamaCloud?\nWe sure do! One of the strengths of LlamaCloud is how easily the endpoints integrate into your existing code. Our llamacloud-demo repo has lots of examples from getting started to running evaluations.\nIs this competitive with vector databases?\nNo. LlamaCloud is focused primarily on data parsing and ingestion, which is a complementary layer to any vector storage provider. The retrieval layer is orchestration on top of an existing storage system. LlamaIndex open-source integrates with 40+ of the most popular vector databases, and we are integrating LlamaCloud with storage providers based on customer requests.\n', ', from storage to compute.\nDataStax\nDatastax has incorporated LlamaParse into their RAGStack to bring a privacy-preserving out-of-the-box RAG solution for enterprises: ""Last week one of our customers Imprompt has launched a pioneering \'Chat-to-Everything\' platform leveraging RAGStack powered by LlamaIndex to enhance their enterprise offerings while prioritizing privacy."" said Davor Bonaci, CTO and executive vice president, DataStax. ""We\'re thrilled to partner with LlamaIndex to bring a streamlined solution to market. By incorporating LlamaIndex into RAGStack, we are providing enterprise developers with a comprehensive Gen AI stack that simplifies the complexities of RAG implementation, while offering long-term support and compatibility assurance.‚Äù\nMongoDB\n‚ÄúMongoDB‚Äôs partnership with LlamaIndex allows for the ingestion of data into the MongoDB Atlas Vector database, and the retrieval of the index from Atlas via LlamaParse and LlamaCloud, enabling the development of RAG systems and other AI applications,‚Äù said Greg Maxson, Global Lead, AI Ecosystems at MongoDB. ‚ÄúNow, developers can abstract complexities associated with data ingestion, simplify RAG pipeline implementations, and more cost effectively develop large language model applications, ultimately accelerating generative AI app development and more quickly bringing apps to market.‚Äù\nQdrant\nAndr√© Zayarni, CEO of Qdrant, says ‚ÄúThe Qdrant team is excited to partner with LlamaIndex to combine the power of optimal data preprocessing, vectorization, and ingestion with Qdrant for a powerful fullstack RAG solution.‚Äù\nNVIDIA\nWe are also excited to collaborate with NVIDIA to integrate LlamaIndex with the NVIDIA AI Enterprise software platform for production AI: ‚ÄúLlamaCloud will help enterprises get generative AI applications from development into production with connectors that link proprietary data to the power of large language models,‚Äù said Justin Boitano, vice president, Enterprise and Edge Computing, NVIDIA. ‚ÄúPairing LlamaCloud with NVIDIA AI Enterprise can accelerate the end-to-end LLM pipeline ‚Äî including data processing, embedding creation, indexing, and model inference on accelerated computing across clouds, data centers and out to the edge.‚Äù\n# FAQ\nIs this competitive with vector databases?\nNo. LlamaCloud is focused primarily on data parsing and ingestion, which is a complementary layer to any vector storage provider. The retrieval layer is orchestration on top of an existing storage system. LlamaIndex open-source integrates with 40+ of the most popular vector databases, and we are working hard to do the following:\n- Integrate LlamaCloud with storage providers of existing design partners\n- Make LlamaCloud available in a more ‚Äúself-serve‚Äù manner.\n\n# Next Steps\nAs mentioned in the above sections, LlamaParse is available for public preview starting today with a usage cap. LlamaCloud is in a private preview mode; we are offering access to a limited set of enterprise design partners. If you‚Äôre interested come talk to us!\nLlamaParse: Repo, Cookbook, Contact Us\nLlamaCloud: Contact Us\n']","LlamaParse and LlamaCloud contribute to data ingestion and parsing for RAG systems by simplifying the complexities associated with data ingestion and enabling the development of RAG systems and other AI applications. LlamaCloud focuses primarily on data parsing and ingestion, serving as a complementary layer to any vector storage provider, while LlamaParse allows for the ingestion of data into vector databases like MongoDB Atlas and facilitates the retrieval of the index from Atlas.",multi_context,"[{'title': 'LlamaCloud - Built for Enterprise LLM App Builders', 'link': 'https://www.llamaindex.ai/blog/llamacloud-built-for-enterprise-llm-app-builders', 'date': 'Jul 9, 2024', 'tags': ""['Llamacloud']""}, {'title': 'Introducing LlamaCloud and LlamaParse', 'link': 'https://www.llamaindex.ai/blog/introducing-llamacloud-and-llamaparse-af8cedf9006b', 'date': 'Feb 20, 2024', 'tags': ""['Llamaindex', 'LLM', 'AI', 'Retrieval Augmented', 'Data']""}]",True
55,What's the process for launching a full-stack LlamaIndex app with Next.js or Express?,"['It‚Äôs a llama on a ship, geddit?\nLast week we released create-llama, a command-line tool to generate a full-stack LlamaIndex application for Retrieval-Augmented Generation (RAG). The response was fantastic, so we‚Äôll be following up with more templates and more features. We also wanted to show you just how easy it is to get your generated app all the way to production. So here‚Äôs a step by step guide, for each of the three backends we currently support: Next.js serverless, Express, and Python.\n# Next.js backend\nThe serverless full-stack Next.js application is the simplest version to deploy as you only have one artifact to deploy. Because it‚Äôs a Next.js app we‚Äôll be deploying to Vercel, the home of Next.js.\n## Step 1: run create-llama\n\nFirst run create-llama to generate your app. We strongly recommend generating a new OpenAI API key and supplying it at generation time (create-llama apps can be customized to use other LLMs but that‚Äôs out of scope for this tutorial).\n## Step 2: create a GitHub repository and push your app to it\n\nThe easiest way to deploy on Vercel is from a linked GitHub repository. Your generated app is already set up as a git repo, so all you have to do after creating a new empty repo is follow the instructions to push it up. This should give you a repo that looks a bit like this:\n\n## Step 3: import your repo into Vercel\nSelect the option to create a new project from a git repo:\n\nand select the repo you just created after authorizing:\n\n## Step 4: configure your project\nBecause this is a Next.js app and this is Vercel, there‚Äôs very little you need to do! The only thing you need to remember is to click ‚Äúenvironment variables‚Äù and create a variable called OPENAI_API_KEY with your key.\n\n## Step 5: Deploy!\nThat‚Äôs it! Deploying a Next.js app to Vercel is pretty easy.\n\nYour deployed app should look like this:\n\nCongratulations, you‚Äôve deployed a full-stack RAG application!\n# Express backend\nIf you chose to generate an Express backend with a Next.js frontend instead, let‚Äôs get you into production with those. We‚Äôll be deploying both frontend and backend to Render, a fantastic service for both static sites and dynamic web applications.\nThere will be 3 big things to do here:\n- Deploy the static frontend\n- Deploy the backend, and give the frontend permission to call it\n- Tell the frontend where the backend is located\n\nWe promise you‚Äôll be production in no time.\n## Step 1: run create-llama\nThis is just like the same step in Next.js\n## Step 2: push the code to a new GitHub repo\n\nLike Vercel, the easiest way to push a site to production is from a linked git repo. Your generated app already has a git repo initiated with frontend and backend folders, so you can go ahead and push them both to a single GitHub repository just as in the Next.js backend.\n## Step 3: Start a new static site\n\nWe‚Äôll be deploying your frontend first as a static site. After authorizing, select the repository where you pushed your frontend and backend; we‚Äôll specify that we‚Äôre deploying the frontend in the next step.\n\n## Step 4: configure your static site\n\nThere are several changes you need to make to the default configuration to successfully publish your static frontend:\n- Name your site something memorable, it will become the URL of your site once it‚Äôs deployed\n- Set your root directory to frontend\n- Set your build command to npm install; npm run build\n- Set your publish directory to out\n- Finally, click ‚ÄúAdvanced‚Äù and set an environment variable called NODE_VERSION to 20 . The default on Render is a much older version of Node so don‚Äôt skip this step!\n\n## Step 5: deploy your static frontend\n\nClick the Deploy button and watch your site build! You should now have a site live at a URL something like frontend-name-you-picked.onrender.com .\n## Step 6: start a new web service\n', 'Introducing create-llama, the easiest way to get started with LlamaIndex!\nUpdate 2023‚Äì11‚Äì20: we now have a guide to deploying your create-llama apps!\n\nWant to use the power of LlamaIndex to load, index and chat with your data using LLMs like GPT-4? It just got a lot easier! We‚Äôve created a simple to use command-line tool that will generate a full-stack app just for you ‚Äî just bring your own data! To get started, run:\nnpx create-llama\nThe app will then ask you a series of questions about what kind of app you want. You‚Äôll need to supply your own OpenAI API key (or you can customize it to use a different LLM), and make a few decisions.\n# How does it get my data?\nThe generated app has a data folder where you can put as many files as you want; the app will automatically index them at build time and after that you can quickly chat with them. If you‚Äôre using LlamaIndex.TS as the back-end (see below), you‚Äôll be able to ingest PDF, text, CSV, Markdown, Word and HTML files. If you‚Äôre using the Python backend, you can read even more types, including audio and video files!\n# Technical details\nThe front-end it generates is a Next.js application, with your choice of shadcn/ui or vanilla HTML and CSS for styling.\nFor the back-end, you have 3 options:\n- Next.js: if you select this option, you‚Äôll have a full stack Next.js application that you can deploy to a host like Vercel in just a few clicks. This uses LlamaIndex.TS, our TypeScript library.\n- Express: if you want a more traditional Node.js application you can generate an Express backend. This also uses LlamaIndex.TS.\n- Python FastAPI: if you select this option you‚Äôll get a backend powered by the llama-index python package, which you can deploy to a service like Render or fly.io.\n\nThere are a couple of other questions you‚Äôll be asked:\n- Streaming or non-streaming: if you‚Äôre not sure, you‚Äôll probably want a streaming backend.\n- SimpleChatEngine or ContextChatEngine : the ContextChatEngine is the one that uses your data. If you just want to chat with GPT, you can use the SimpleChatEngine.\n\n# Go forth and customize!\nOnce you‚Äôve got your app up and running, you can customize it to your heart‚Äôs content! By default, for cost reasons, the app will use GPT-3.5-Turbo. If you‚Äôd like to use GPT-4 you can configure that by modifying the file app/api/chat/llamaindex-stream.ts (in the Next.js backend) or you can configure it to use a different LLM entirely! LlamaIndex has integrations with dozens of LLMs, both APIs and local.\n']","To launch a full-stack LlamaIndex app with Next.js, follow these steps: 1. Run create-llama to generate your app, ensuring you supply an OpenAI API key. 2. Create a GitHub repository and push your app to it. 3. Import your repo into Vercel and create a new project from the git repo. 4. Configure your project by setting an environment variable called OPENAI_API_KEY. 5. Deploy your app. For an Express backend, the steps are similar: 1. Run create-llama to generate your app. 2. Push the code to a new GitHub repo. 3. Start a new static site for the frontend. 4. Configure your static site with necessary settings. 5. Deploy your static frontend. 6. Start a new web service for the backend.",multi_context,"[{'title': 'Shipping your Retrieval-Augmented Generation app to production with create-llama', 'link': 'https://www.llamaindex.ai/blog/shipping-your-retrieval-augmented-generation-app-to-production-with-create-llama-7bbe43b6287d', 'date': 'Nov 20, 2023', 'tags': ""['LLM', 'Llamaindex', 'Deployment', 'Rag', 'AI']""}, {'title': 'create-llama, a command line tool to generate LlamaIndex apps', 'link': 'https://www.llamaindex.ai/blog/create-llama-a-command-line-tool-to-generate-llamaindex-apps-8f7683021191', 'date': 'Nov 14, 2023', 'tags': ""['Llamaindex', 'LLM', 'AI', 'JavaScript', 'Typescript']""}]",True
56,What's the purpose of the Multi-Doc SEC 10Q Dataset from Docugami and LlamaIndex for RAG eval?,"['What‚Äôs up, Llama Followers ü¶ô,\nWe‚Äôre excited to bring you another week packed with the latest updates, features, exciting community demos, insightful tutorials, guides, and webinars. This week, don‚Äôt miss our special holiday workshop on 12/21, where we‚Äôll dive into innovative LLM + RAG use cases with Google Gemini team.\nGot a groundbreaking project, compelling article, or captivating video? We‚Äôre all ears! Reach out to us at news@llamaindex.ai. Remember to subscribe to our newsletter via our website to get all these exciting developments straight to your inbox.\nü§© First, the highlights:\n- Google Gemini Partnership: Now offering day 1 support for Gemini API on LlamaIndex, complete with comprehensive cookbooks for advanced RAG capabilities. Tweet.\n- MistralAI Integrations: Introduced day-0 integrations with MistralAI LLMs and Embedding model for building RAG solutions on LlamaIndex. Notebook, Tweet.\n- Docugami Multi-Doc Llama Dataset: Launched the Multi-Doc SEC 10Q Dataset by Taqi Jaffri, offering a range of question complexities for advanced RAG research. Docs, Tweet.\n- Proposition-Based Retrieval: Implemented a new retrieval unit based on propositions, enhancing QA performance with LLMs. Docs, Tweet.\n- RAG Pipeline Enhancement Guide: Introduced a guide featuring modules like Routing, Query-Rewriting, and Agent Reasoning for more complex QA over documents. Docs.\n\n‚ú® Feature Releases and Enhancements:\n- We launched a partnership with Google Gemini, offering day 1 support for the Gemini API on LlamaIndex, including full-feature support for Gemini (text and multi-modal) and Semantic Retriever API, complemented by three comprehensive cookbooks: Gemini LLM, Gemini Multi-modal, and Semantic Retriever API, promising advanced RAG capabilities and multi-modal integrations. Tweet.\n- We introduced day-0 integrations with the MistralAI LLMs (mistral-tiny, mistral-small, mistral-medium) and the MistralAI Embedding model for building RAG solutions with LlamaIndex both on Python and Typescript versions. Notebook, Tweet.\n- We launched the COVID-QA dataset on LlamaHub, a human-annotated, substantial set of 300+ QA pairs about COVID from various web articles, complete with source URLs for easy integration into RAG pipelines, offering ample scope for improvement. Docs, Tweet.\n- We launched a new multi-modal template in Create-llama, enabling image input and output generation using the latest GPT-4-vision model from OpenAI, expanding possibilities for diverse use cases. Docs, Tweet.\n- We have introduced Proposition-Based Retrieval in LlamaIndex: Implementing a new retrieval unit based on propositions, as introduced in the ‚ÄòDense X Retrieval‚Äô paper, enhancing QA performance with LLMs by indexing propositions and linking to the underlying text. Docs, Tweet.\n- We partnered with Docugami to launch a new Multi-Doc SEC 10Q Dataset by Taqi Jaffri, aimed at advancing QA datasets for RAG evaluation. This dataset offers a range of question complexities: Single-Doc, Single-Chunk RAG; Single-Doc, Multi-Chunk RAG; and Multi-Doc RAG, addressing the need for more intricate datasets in RAG research. Docs, Tweet.\n- We launched a SharePoint data loader, enabling direct integration of SharePoint files into LLM/RAG pipelines. Docs, Tweet.\n\nüëÄ Community Demos:\n- MemoryCache: Mozilla‚Äôs new experimental project that curates your online experience into a private, on-device RAG application using PrivateGPT_AI and LlamaIndex, enhancing personal knowledge management while maintaining privacy. Website, Repo.\n- OpenBB Finance showcases its enhanced chat widget feature in Terminal Pro, utilizing LlamaIndex‚Äôs data chunking combined with Cursor AI for improved large context management and accuracy. Tweet\n- AI Chatbot Starter (from the DataStax team), a web server powered by AstraDB and LlamaIndex, allows easy setup for chatting over web documentation. It can be used as a standalone service or integrated into full-stack applications, with simple credential setup and document ingestion. Repo, Tweet.\n- Na2SQL (by Harshad) to ****Build an End-to-End SQL Analyst App on Streamlit featuring interactive database viewing, SQL query displays, and integration with Llama Index. Blog, Repo.\n- LionAGI (by Ocean Li) is an']","The purpose of the Multi-Doc SEC 10Q Dataset from Docugami and LlamaIndex is to advance QA datasets for RAG evaluation, offering a range of question complexities such as Single-Doc, Single-Chunk RAG; Single-Doc, Multi-Chunk RAG; and Multi-Doc RAG, addressing the need for more intricate datasets in RAG research.",multi_context,"[{'title': 'LlamaIndex Newsletter 2023‚Äì12‚Äì19', 'link': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-12-19-2965a2d03726', 'date': 'Dec 19, 2023', 'tags': ""['Llamaindex', 'Rag', 'Newsletter', 'AI', 'LLM']""}]",True
57,"What are Llama 2's SQL query limits for structured analytics, and how might fine-tuning with OpenLLM and LlamaIndex improve it?","['Over the past year, Large Language Models (LLMs) like GPT-4 have not only transformed how we interact with machines but also have redefined the possibilities within the realm of natural language processing (NLP). A notable trend in this evolution is the increasing popularity of open-source LLMs like Llama 2, Falcon, OPT and Yi. Some may prefer them over their commercial counterparts in terms of accessibility, data security and privacy, customization potential, cost, and vendor dependency. Among the tools gaining increasing traction in the LLM space are OpenLLM and LlamaIndex ‚Äî two powerful platforms that, when combined, unlock new use cases for building AI-driven applications.\nOpenLLM is an open-source platform for deploying and operating any open-source LLMs in production. Its flexibility and ease of use make it an ideal choice for AI application developers seeking to harness the power of LLMs. You can easily fine-tune, serve, deploy, and monitor LLMs in a wide range of creative and practical applications.\nLlamaIndex provides a comprehensive framework for managing and retrieving private and domain-specific data. It acts as a bridge between the extensive knowledge of LLMs and the unique, contextual data needs of specific applications.\nOpenLLM‚Äôs support for a diverse range of open-source LLMs and LlamaIndex‚Äôs ability to seamlessly integrate custom data sources provide great customization for developers in both communities. This combination allows them to create AI solutions that are both highly intelligent and properly tailored to specific data contexts, which is very important for query-response systems.\nIn this blog post, I will explain how you can leverage the combined strengths of OpenLLM and LlamaIndex to build an intelligent query-response system. This system can understand, process, and respond to queries by tapping into a custom corpus.\n# Setting up the environment\nThe first step is to create a virtual environment in your machine, which helps prevent conflicts with other Python projects you might be working on. Let‚Äôs just call it llamaindex-openllm and activate it.\npython -m venv llamaindex-openllm\nsource llamaindex-openllm/bin/activate\nInstall the required packages. This command installs OpenLLM with the optional vllm component (I will explain it later).\npip install ""openllm[vllm]"" llama-index llama-index-llms-openllm llama-index-embeddings-huggingface\nFor handling requests, you need to have an LLM server. Here, I use the following command to start a Llama 2 7B local server at http://localhost:3000. Feel free to choose any model that fits your needs. If you already have a remote LLM server, you can skip this step.\nopenllm start meta-llama/Llama-2-7b-chat-hf --backend vllm\nOpenLLM automatically selects the most suitable runtime implementation for the model. For models with vLLM support, OpenLLM uses vLLM by default. Otherwise, it falls back to PyTorch. vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. According to this report, you can achieve 23x LLM inference throughput while reducing P50 latency using vLLM.\nNote: To use the vLLM backend, you need a GPU with at least the Ampere architecture (or newer) and CUDA version 11.8. This demo uses a machine with an Ampere A100‚Äì80G GPU. If your machine has a compatible GPU, you can also choose vLLM. Otherwise, simply install the standard OpenLLM package (pip install openllm) in the previous command.\n# v1: Creating a simple completion service\nBefore building a query-response system, let‚Äôs get familiar with the integration of OpenLLM and LlamaIndex and use it to create a simple completion service.\nThe integration offers two APIs for interactions with LLMs:\n1. OpenLLM: This can be used to initiate a local LLM server directly without the need for starting a separate one using commands like openllm start. Here‚Äôs how you can use it:\nfrom llama_index.llms.openllm import OpenLLM\nllm = OpenLLM(\'meta-llama/Llama-2-7b-chat-hf\')\n2. OpenLLMAPI: This can be used to interact with a server hosted elsewhere, like the Llama 2 7B model I started previously.\nLet‚Äôs try the complete endpoint and see if the Llama 2 7B model is able to tell what OpenLLM is by completing the sentence ‚ÄúOpenLLM is an open source tool for‚Äù', 'Llama 2 is a huge milestone in the advancement of open-source LLMs. The biggest model and its finetuned variants sit at the top of the Hugging Face Open LLM Leaderboard. Multiple benchmarks show that it is approaching GPT-3.5 (or in some cases even surpassing it) in terms of performance. All of this means that open-source LLMs are an increasingly viable and reliable option for use in complex LLM applications, from RAG systems to agents.\n# Context: Llama-2‚Äì7B is Not Good at Text-to-SQL\nA downside of the smallest Llama 2 model (7B parameters), however, is that it‚Äôs not very good at generating SQL, making it impractical for structured analytics use cases. As an example, we tried prompting Llama 2 to generate the correct SQL statement given the following prompt template:\nYou are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables. \n\nYou must output the SQL query that answers the question.\n\n### Input:\n{input}\n\n### Context:\n{context}\n\n### Response:\nHere we plugged in a sample entry from the sql-create-context dataset.\ninput: In 1981 which team picked overall 148?\ncontext: CREATE TABLE table_name_8 (team VARCHAR, year VARCHAR, overall_pick VARCHAR)\nMeanwhile, here is the generated output vs. correct output:\nGenerated output: SELECT * FROM `table_name_8` WHERE \'1980\' = YEAR AND TEAM = ""Boston Celtics"" ORDER BY OVERALL_PICK DESC LIMIT 1;\n\nCorrect output: SELECT team FROM table_name_8 WHERE year = 1981 AND overall_pick = ""148""\nThis is clearly not ideal. Unlike ChatGPT and GPT-4, Llama 2 does not reliably produce well-formatted and correct SQL outputs.\nThis is exactly where fine-tuning comes in ‚Äî given a proper corpus of text-to-SQL data, we can teach Llama 2 to be better at generating SQL outputs from natural language. At a high-level, fine-tuning involves modifying the weights of the model in some capacity. There are different ways to finetune models, from updating all parameters of the network, to a subset of the parameters, to only finetuning additional parameters (e.g. how LoRA works).\nOnce the model is finetuned, it can still be plugged into a downstream LLM application. That is exactly what this tutorial aims to show. It is a step more involved than our existing tutorials which have primarily focused on ‚Äúin-context learning‚Äù and ‚Äúretrieval-augmentation‚Äù use cases ‚Äî freezing the model itself but focusing on the orchestration of data into the input prompt. Finetuning can have a high learning curve and also require a lot of compute. This tutorial makes it as easy as possible to get started.\n# Tutorial Overview\nIn this tutorial, we show you how you can finetune Llama 2 on a text-to-SQL dataset, and then use it for structured analytics against any SQL database using the capabilities of LlamaIndex.\nHere is the stack that we use:\n- b-mc2/sql-create-context from Hugging Face datasets as the training dataset\n- OpenLLaMa open_llama_7b_v2 as the base model\n- PEFT for efficient finetuning\n- Modal for handling all cloud compute/orchestration for finetuning. And also for the excellent reference doppel-bot repo.\n- LlamaIndex for text-to-SQL inference against any SQL database.\n\nSpecial mention to the awesome Llama 2 tutorial from Anyscale that helped to inspire this project.\nAll of our materials can be found in our Github repo: https://github.com/run-llama/modal_finetune_sql (again emphasizing that this is adapted from doppel-bot). Also, the full tutorial can be found in our Jupyter notebook guide. Make sure to check it out!\nAs mentioned above, performing finetuning does require quite a few steps. Our goal is to make this as straightforward as possible to follow and use out of the box. We don‚Äôt cover all the nitty gritty detailsof Modal, PEFT, the finetuning procedure itself, etc. but we do give a rough overview.\nThere are also certainly higher-level APIs that we could‚Äôve used (e.g. OpenAI, Lamini) in order to achieve this task. There‚Äôs plenty of room for followup tutorials to cover these topics!\n## Step 1: Loading Training Data for Finetuning L']","Llama 2's SQL query limits for structured analytics include its inability to reliably produce well-formatted and correct SQL outputs, as demonstrated by its poor performance in generating SQL statements from natural language prompts. Fine-tuning with OpenLLM and LlamaIndex can improve this by modifying the model's weights using a proper corpus of text-to-SQL data, thereby teaching Llama 2 to generate better SQL outputs.",multi_context,"[{'title': 'Building An Intelligent Query-Response System with LlamaIndex and OpenLLM', 'link': 'https://www.llamaindex.ai/blog/building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf', 'date': 'Jan 3, 2024', 'tags': ""['Openllm', 'Llamaindex', 'AI', 'Bentoml', 'Open Source']""}, {'title': 'Easily Finetune Llama 2 for Your Text-to-SQL Applications', 'link': 'https://www.llamaindex.ai/blog/easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d', 'date': 'Aug 17, 2023', 'tags': ""['Llamaindex', 'Fine Tuning', 'Large Language Models', 'AI', 'NLP']""}]",True
58,How do vector embeddings aid time-based similarity searches in a Timescale Vector store with node metadata?,"['an Sreethar\ndate: 2023-09-5 21:03:21+0850\n\nTue Sep 5 21:03:21 2023 +0530 Lakshmi Narayanan Sreethar Fix segfault in set_integer_now_func When an invalid function oid is passed to set_integer_now_func, it finds out that the function oid is invalid but before throwing the error, it calls ReleaseSysCache on an invalid tuple causing a segfault. Fixed that by removing the invalid call to ReleaseSysCache.  Fixes #6037\n# Create vector embeddings for the text of each node\nNext, we‚Äôll create vector embeddings of the content of each node so that we can perform similarity searches on the text associated with each node. We‚Äôll use the `OpenAIEmbedding` model to create the embeddings.\n# Create embeddings for nodes\nfrom llama_index.embeddings import OpenAIEmbedding\nembedding_model = OpenAIEmbedding()\n\nfor node in nodes:\n   node_embedding = embedding_model.get_text_embedding(\n       node.get_content(metadata_mode=""all"")\n   )\n   node.embedding = node_embedding\n# Load nodes into Timescale Vector vector store\nNext, we‚Äôll create a `TimescaleVectorStore` instance and add the nodes we created to it.\n# Create a timescale vector store and add the newly created nodes to it\nts_vector_store = TimescaleVectorStore.from_params(\n   service_url=TIMESCALE_SERVICE_URL,\n   table_name=""li_commit_history"",\n   time_partition_interval= timedelta(days=7),\n)\nts_vector_store.add(nodes)\nTo take advantage of Timescale Vector‚Äôs efficient time-based search, we need to specify the `time_partition_interval` argument when instantiating a Timescale Vector vector store. This argument represents the length of each interval for partitioning the data by time. Each partition will consist of data that falls within the specified length of time.\nIn the example above, we use seven days for simplicity, but you can pick whatever value makes sense for the queries used by your application ‚Äî for example, if you query recent vectors frequently, you might want to use a smaller time delta like one day, or if you query vectors over a decade-long time period, then you might want to use a larger time delta like six months or one year. As a rule of thumb, common queries should touch only a couple of partitions and at the same time your full dataset should fit within a 1000 partitions, but don‚Äôt stress too much ‚Äî the system is not very sensitive to this value.\n# Similarity search with time filters\nNow that we‚Äôve loaded our nodes that contain vector embeddings data and metadata into a Timescale Vector vector store, and enabled automatic time-based partitioning on the table our vectors and metadata are stored in, we can query our vector store with time-based filters as follows:\n# Query the vector database\nvector_store_query = VectorStoreQuery(query_embedding = query_embedding, similarity_top_k=5)\n\n# Time filter variables for query\nstart_dt = datetime(2023, 8, 1, 22, 10, 35) # Start date = 1 August 2023, 22:10:35\nend_dt = datetime(2023, 8, 30, 22, 10, 35) # End date = 30 August 2023, 22:10:35\n\n# return most similar vectors to query between start date and end date date range\n# returns a VectorStoreQueryResult object\nquery_result = ts_vector_store.query(vector_store_query, start_date = start_dt, end_date = end_dt)\nLet‚Äôs take a look at the date and contents of the nodes returned by our query:\n# for each node in the query result, print the node metadata date\nfor node in query_result.nodes:\n   print(""-"" * 80)\n   print(node.metadata[""date""])\n   print(node.get_content(metadata_mode=""all""))\n--------------------------------------------------------------------------------\n2023-08-3 14:30:23+0500\ncommit:  7aeed663b9c0f337b530fd6cad47704a51a9b2ec\nauthor: Dmitry Simonenko\ndate: 2023-08-3 14:30:23+0500', ' import StorageContext\n\nindex = VectorStoreIndex.from_vector_store(ts_vector_store)\nquery_engine = index.as_query_engine(vector_store_kwargs = ({""start_date"": start_dt, ""end_date"":end_dt}))\n\nquery_str = ""What\'s new with TimescaleDB functions? When were these changes made and by whom?""\nresponse = query_engine.query(query_str)\nprint(str(response))\nWe asked the LLM a question about our gitlog, namely, ‚ÄúWhat‚Äôs new with TimescaleDB functions. When were these changes made and by whom?‚Äù\nHere‚Äôs the response we get, which synthesizes the nodes returned from semantic search with time-based filtering on the Timescale VectorStore:\nTimescaleDB functions have undergone changes recently. These changes include the addition of several GUCs (Global User Configuration) that allow for enabling or disabling major TimescaleDB features. Additionally, a compatibility layer has been added for the ""_timescaledb_internal"" functions, which were moved into the ""_timescaledb_functions"" schema to enhance schema security. These changes were made by Dmitry Simonenko and Sven Klemm. The specific dates of these changes are August 3, 2023, and August 29, 2023, respectively.\nThis is a simple example of a powerful concept ‚Äî using time-based context retrieval in your RAG applications can help provide more relevant answers to your users. This time-based context retrieval can be helpful to any dataset with a natural language and time component. Timescale Vector uniquely enables this thanks to its efficient time-based similarity search capabilities, and taking advantage of it in your LlamaIndex application is easy thanks to the Timescale Vector integration.\n# Resources and next steps\nNow that you‚Äôve learned how Timescale Vector can help you power better AI applications with PostgreSQL, it‚Äôs your turn to dive in. Take the next step in your learning journey by following one of the tutorials or reading one of the blog posts in the resource set below:\n- Up and Running Tutorial: learn how to use Timescale Vector in LlamaIndex using a real-world dataset. You‚Äôll learn how to use Timescale Vector as a Vectorstore, Retriever, and QueryEngine and perform time-based similarity search on vectors.\n- Timescale Vector explainer: learn more about the internals of Timescale Vector.\n- Timescale Vector website: learn more about Timescale Vector and Timescale‚Äôs AI Launch Week.\n\nüéâ And a reminder: LlamaIndex Users get Timescale Vector free for 90 days\nWe‚Äôre giving LlamaIndex users an extended 90-day trial of Timescale Vector. This makes it easy to test and develop your applications on Timescale Vector, as you won‚Äôt be charged for any cloud PostgreSQL databases you spin up during your trial period. Try Timescale Vector for free today.\n']","Vector embeddings aid time-based similarity searches in a Timescale Vector store by allowing the system to create a representation of the content of each node, which can then be queried for similarity. The embeddings are stored alongside metadata, including timestamps, enabling efficient time-based partitioning and filtering during queries. This allows users to retrieve the most relevant vectors based on both content similarity and specific time intervals.",multi_context,"[{'title': 'Timescale Vector x LlamaIndex: Making PostgreSQL a Better Vector Database for AI Applications', 'link': 'https://www.llamaindex.ai/blog/timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0', 'date': 'Sep 27, 2023', 'tags': ""['Llamaindex', 'NLP', 'LLM', 'AI', 'Timescaledb']""}, {'title': 'Timescale Vector x LlamaIndex: Making PostgreSQL a Better Vector Database for AI Applications', 'link': 'https://www.llamaindex.ai/blog/timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0', 'date': 'Sep 27, 2023', 'tags': ""['Llamaindex', 'NLP', 'LLM', 'AI', 'Timescaledb']""}]",True
59,What tool in LlamaIndex aids in data management and semantic searches?,"[' conversation history and dumped into the LLM context window, then we may run into context window limitations.\nLlamaIndex offers tool abstractions to help deal with this. Our LoadAndSearchToolSpec wraps any given tool that may return a large amount of data, and it splits it into two tools: a load tool that will dynamically store the data in an index, and a search tool that allows for search over that index.\nOn the Metaphor side, this is also where we define a search_and_retrieve_documents endpoint that combines search and retrieve. This allows the agent to make a single query to retrieve a large number of documents, which when combined with the LoadAndSearchToolSpec will get directly stored within an index. If the agent were to call search and retrieve separately, then it would both take longer and consume more tokens to write the search results to conversation history, and then passing that into the prompt again to call retrieve over all document IDs.\nCreating the LoadAndSearchToolSpec:\nfrom llama_index.tools.tool_spec.load_and_search.base import LoadAndSearchToolSpec\n# The search_and_retrieve_documents tool is the third in the tool list, as seen above\nwrapped_retrieve = LoadAndSearchToolSpec.from_defaults(\n  metaphor_tool_list[2],\n)\nNow let‚Äôs walk through a full execution example:\n# Just pass the wrapped tools and the get_date utility\nagent = OpenAIAgent.from_tools(\n  [*wrapped_retrieve.to_tool_list(), metaphor_tool_list[4]],\n  verbose=True,\n)\nprint(agent.chat(\'Can you summarize everything published in the last month regarding news on superconductors\'))\nThe output here shows that the agent calls multiple tools in succession to get the right answer.\n=== Calling Function ===\nCalling function: current_date with args: {}\nGot output: 2023-08-20\n========================\n=== Calling Function ===\nCalling function: search_and_retrieve_documents with args: {\n  ""query"": ""superconductors"",\n  ""start_published_date"": ""2023-07-20"",\n  ""end_published_date"": ""2023-08-20""\n}\n[Metaphor Tool] Autoprompt: ""Here is an interesting article about superconductors:\nGot output: Content loaded! You can now search the information using read_search_and_retrieve_documents\n========================\n=== Calling Function ===\nCalling function: read_search_and_retrieve_documents with args: {\n  ""query"": ""superconductors""\n}\nGot output: \nSuperconductors are materials that can perfectly conduct electricity. They are used in a variety of applications, such as particle accelerators, nuclear fusion devices, MRI machines, and maglev trains. However, so far, no superconductor has been proven to work at ambient pressures and temperatures. On July 22, scientists in South Korea published research claiming to have solved this problem with a material called LK-99, which has an electrical resistivity that drops to near zero at 30 degrees Celsius (86 degrees Fahrenheit).\n========================\nIn the last month, there have been developments in the field of superconductors. Scientists in South Korea have published research on a material called LK-99, which has the ability to conduct electricity with near-zero resistance at a temperature of 30 degrees Celsius (86 degrees Fahrenheit). This breakthrough could potentially lead to the development of superconductors that work at ambient pressures and temperatures, opening up new possibilities for various applications such as particle accelerators, nuclear fusion devices, MRI machines, and maglev trains.\nThe agent used the get_date tool to determine the current month, and then applied the filters in Metaphor based on publication date when calling search. It then loaded the documents using retrieve_documents and read them using read_retrieve_documents.\n# Conclusion\nAs shown above, the integration between LlamaIndex data agents + Metaphor search has the potential to bypass existing limitations with LLMs and even RAG systems. We‚Äôre excited to continue exploring this further in future blog posts.\nWe encourage you to play around with the notebook ‚Äî make sure to check it out!\nResources:\n- Notebook: https://github.com/emptycrown/llama-hub/blob/main/llama_hub/tools/notebooks/metaphor.ipynb\n- LlamaHub: https://llamahub.ai/l/tools-metaphor\n- Metaphor: https://platform.metaphor.systems/\n- Metaphor API Docs: https://docs.metaphor', '(co-authored by Jerry Liu, CEO of LlamaIndex, Jeffrey Wang, co-founder at Metaphor, and Adam Hoffman, Software Engineer at Hypotenuse Labs)\nWe‚Äôre incredibly excited to launch an integration between LlamaIndex and Metaphor: combine the capabilities of LlamaIndex data agents with Metaphor as a native LLM search tool to enable knowledge workers capable of answering any question over any data, no matter how recent or complex.\nWe provide a deeper overview of Metaphor and the LlamaIndex integration below. We also walk through our example notebook to showcase how they can be combined.\n\n# Background/Context\nState-of-the art large language models (LLMs) such as ChatGPT, GPT-4, Claude 2 have incredible reasoning capabilities that unlock a wide variety of use cases ‚Äî from insight extraction to question-answering to general workflow automation. Yet they are limited in their abilities to retrieve contextually relevant information. A popular stack that has emerged is to setup a retrieval-augmented generation (RAG) system, which combines LLMs with external storage solutions over a static knowledge source. Frameworks such as LlamaIndex provide a variety of tools to setup both simple and complex RAG systems.\nYet even this is not the complete picture. LLMs should ideally be able to dynamically search and retrieve information from the external world, not just depend on a static source of knowledge. This would allow them to fulfill a more general set of tasks and not only perform search/retrieval, but perform actions as well.\nTo do this well, we need two core components:\n- General abstractions that allow LLMs to intelligently perform various tasks over your data, in both a ‚Äúread‚Äù and ‚Äúwrite‚Äù fashion\n- A good search engine tailored for LLM use\n\nLlamaIndex data agent abstractions help to satisfy the first core component. A complete data agent consists of both a reasoning loop as well as a set of Tools. These tools can be interfaces for search/retrieval or more generally any external API. Given a query, the agent will execute its reasoning loop and dynamically figure out the set of Tools it will need to fulfill the task at hand.\nData agents have access to a rich set of Tools offered on LlamaHub ‚Äî these range from Gmail API, to a SQL db API, to a basic tool in the form of Bing search. We‚Äôve shown that they are capable of e2e tasks from sending emails, scheduling meetings, to automating custom support insight extraction. Yet there has never been a tool tailored for LLM use.\n# Overview of Metaphor\nThe Metaphor API is designed to connect your LLM to the internet. It allows you to perform fully neural, highly semantic searches over the Internet and also get clean, HTML content from the results.\nMetaphor was trained to predict links on the internet, given how people talk about things on the Internet. For example, someone might post about a great article they read like this:\nFound an amazing article I read about the history of Rome‚Äôs architecture: [LINK]\nBy training a model to predict these links given how people talk about them, the end result is a totally different way to search the internet ‚Äî search as if you‚Äôre about to share the link you want. While a little unintuitive at first, searching this way can return extremely high quality results. But for the purposes of LlamaIndex, you won‚Äôt need to worry about this because by default, queries will be converted into Metaphor prompts.\nWhy would you use Metaphor Search over Bing/Google? There are 3 main reasons:\n- You can search fully semantically, for instance with feelings or complex descriptors.\n- You can search only for the type of entity that you want. Companies, articles, people.\n- You can find content that Google simply doesn‚Äôt surface well, maybe because keywords aren‚Äôt the right tool or maybe just because Google doesn‚Äôt care about returning good results for that type of content.\n\nTo learn more, you can read the full Metaphor API blog post.\n# Integration Details\nThe Metaphor Tool Spec in LlamaHub is an API interface that consists of 5 tools that an agent can use.\n- Search: The entrypoint to Metaphor ‚Äî allows an agent to pass a natural language query that will then be passed to the Metaphor search engine. This endpoint also contains some additional parameters, such as the number of results, domains to include/exclude, and a date filter.\n- Retrieve Documents: This will retrieve the content of a set of documents given IDs. These ids are returned as part of the results from the search endpoint above.\n- Search and Retrieve Documents: This is a convenience endpoint that combines the functionality of `search` and `']","The LoadAndSearchToolSpec in LlamaIndex aids in data management and semantic searches by wrapping tools that can return large amounts of data, allowing for dynamic storage in an index and enabling search over that index.",multi_context,"[{'title': 'LlamaIndex + Metaphor: Towards Automating Knowledge Work with LLMs', 'link': 'https://www.llamaindex.ai/blog/llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f', 'date': 'Aug 21, 2023', 'tags': ""['Search', 'LLM', 'Agents', 'NLP', 'Llamaindex']""}, {'title': 'LlamaIndex + Metaphor: Towards Automating Knowledge Work with LLMs', 'link': 'https://www.llamaindex.ai/blog/llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f', 'date': 'Aug 21, 2023', 'tags': ""['Search', 'LLM', 'Agents', 'NLP', 'Llamaindex']""}]",True
60,How does linking an IAM OIDC provider to EKS aid in deploying RabbitMQ and setting up a worker service?,"[' Make sure to write down the URL printed at the end! If you forget, you can get the URL in the EKS page on AWS. You‚Äôll want the external IP for the load balancer.\n# Step 3: Deploying RabbitMQ\nRabbitMQ is where we will queue documents to be ingested. RabbitMQ is a message broker system that allows for powerful yet simple queuing of tasks. Since some ingestion tasks (like metadata extraction, embeddings) can be slow, the more naive approach of a REST API would leave connections open while data is processed. Instead, using a queue allows us to quickly upload data and offload processing to scalable message consumer(s). It also allows us to add parallelism with ease, where in our system, each Document object is processed independently by a consumer.\nDeploying RabbitMQ on EKS was a little tricky, but using the RabbitMQ operator installed with krew, many things are abstracted away.\nFirst, you need to create your cluster. For whatever reason, this didn‚Äôt work unless I also specified the zones\neksctl create cluster \\\n  --name mqCluster \\\n  --zones us-east-1a,us-east-1b,us-east-1c,us-east-1d,us-east-1f\nSince RabbitMQ needs storage, and each replica needs to share the same storage, we should give our cluster permission to provision and use EBS for storage. This was a frustrating step to figure out since most existing guides skip this detail!\neksctl utils associate-iam-oidc-provider \\\n  --cluster=mqCluster \\\n  --region us-east-1 \\\n  --approve\nsleep 5\neksctl create iamserviceaccount \\\n    --name ebs-csi-controller-sa \\\n    --namespace kube-system \\\n    --cluster mqCluster \\\n    --role-name AmazonEKS_EBS_CSI_DriverRole \\\n    --role-only \\\n    --attach-policy-arn arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy \\\n    --approve\nsleep 5\neksctl create addon \\\n  --name aws-ebs-csi-driver \\\n  --cluster mqCluster \\\n  --service-account-role-arn arn:aws:iam::$(aws sts get-caller-identity --query Account --output text):role/AmazonEKS_EBS_CSI_DriverRole \\\n  --force\nFrom there, we can install the RabbitMQ operator and create our deployment\nkubectl apply -f &lt;https://github.com/rabbitmq/cluster-operator/releases/latest/download/cluster-operator.yml&gt;\nsleep 5\nkubectl apply -f rabbitmqcluster.yaml\nsleep 5\necho ""RabbitMQ URL is: $(kubectl get svc production-rabbitmqcluster -o jsonpath=\'{.status.loadBalancer.ingress[0].hostname}\')""\nAs usual, the code for all this can be found in the git repo.\nNOTE: Make sure to write down the URL printed at the end! If you forget, you can get the URL in the EKS page on AWS. You‚Äôll want the external IP for the load balancer.\nYou can monitor your RabbitMQ queues by visiting ‚Äú<rabbitmq_url>:15672‚Äù and signing in with ‚Äúguest‚Äù/‚Äùguest‚Äù.\n# Step 4: Deploying IngestionPipeline Workers\nThis is where the real meat of work comes in. We need to create a consumer that will endlessly pull from our RabbitMQ queue, ingest data with the help of TEI, and then put that data into our vector db.\nTo do this, we can make a FastAPI server that does two things\n- Starts a thread to consume from our queue\n- Starts a webserver, to enable us to specify a readiness check, and gives us room to add more features in the future (i.e. probing queue status, logs, etc.)\n\nFirst, we write our code, as you can see in worker.py\nThen, we dockerize our app by creating a simple Dockerfile and running:\ndocker build -t <image_name> .\ndocker tag <image_name>:latest <image_name>:<version>\ndocker push <image_name>:<version>\nWith our app dockerized, we can complete the worker-de', 'ployment.yaml file by filling in\n- Our embeddings URL under TEI_URL\n- Our rabbit-mq URL under RABBITMQ_URL\n- Our image name under container image\n- Our cluster details (in this case, a weaviate URL and API key)\n\nWith the yaml file complete, now we can properly deploy the worker\neksctl create cluster --name mq-workers --zones us-east-1a,us-east-1b,us-east-1c,us-east-1d,us-east-1f\nsleep 5\nkubectl create -f ./worker-deployment.yaml\nsleep 5\nkubectl create -f ./worker-service.yaml\n# Step 5: Making a User-Facing Lambda Function\nOur lambda function will rely on a single external dependency ‚Äî pika ‚Äî which is used to communicate with RabbitMQ.\nCreate a python file called lambda_function.py with the following code:\nimport pika\nimport json\n\ndef lambda_handler(event, context):\n    try:\n        body = json.loads(event.get(\'body\', \'{}\'))\n    except:\n        body = event.get(\'body\', {})\n        \n    user = body.get(\'user\', \'\')\n    documents = body.get(\'documents\', [])\n    if not user or not documents:\n        return {\n            \'statusCode\': 400,\n            \'body\': json.dumps(\'Missing user or documents\')\n        }\n    \n    credentials = pika.PlainCredentials(""guest"", ""guest"")\n    parameters = pika.ConnectionParameters(\n        host=""hostname.amazonaws.com"", \n        port=5672, \n        credentials=credentials\n    )\n    \n    connection = pika.BlockingConnection(parameters=parameters)\n    channel = connection.channel()\n    channel.queue_declare(queue=\'etl\')\n\n    for document in documents:\n        data = {\n            \'user\': user,\n            \'documents\': [document]\n        }\n        channel.basic_publish(\n            exchange="""", \n            routing_key=\'etl\', \n            body=json.dumps(data)\n        )\n\n    return {\n        \'statusCode\': 200,\n        \'body\': json.dumps(\'Documents queued for ingestion\')\n    }\nThe function above processes incoming requests, and publishes each document as a single message in our rabbitmq cluster.\nTo deploy a lambda file with dependencies, we need to create a zip of our lambda function + all dependencies. To do this, we can create a requirements.txt file with our dependencies and run:\npip install -r requirements.txt -t .\nzip -r9 ../ingestion_lambda.zip . -x ""*.git*"" ""*setup.sh*"" ""*requirements.txt*"" ""*.zip*""\nWith our code and zip file in hand, head over to the Lambda AWS page in your browser.\n- Select Create function\n- Give it a name, select a python runtime (I used Python 3.11)\n- Click Create function at the bottom\n- In the code editor, you‚Äôll see an Upload from button ‚Äî click that, and upload your zip file\n- Click test, give the test a name, and paste the following JSON\n\n{\n    ""body"": {""user"": ""Test"", ""documents"": [{""text"": ""test""}]}\n}\nOnce the test works, the Deploy button will not be grayed out, and you can click it.\nYour public URL will be listed in the upper right pane under Function URL ‚Äî this is the URL you can use']","Linking an IAM OIDC provider to EKS aids in deploying RabbitMQ and setting up a worker service by allowing the cluster to have permission to provision and use EBS for storage, which is necessary for RabbitMQ to function properly. This step is crucial as it ensures that the storage requirements for RabbitMQ are met, enabling the deployment to proceed smoothly.",multi_context,"[{'title': 'Scaling LlamaIndex with AWS and Hugging Face', 'link': 'https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716', 'date': 'Jan 2, 2024', 'tags': ""['AWS', 'Llamaindex', 'Etl', 'Data Processing', 'Hugging Face']""}, {'title': 'Scaling LlamaIndex with AWS and Hugging Face', 'link': 'https://www.llamaindex.ai/blog/scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716', 'date': 'Jan 2, 2024', 'tags': ""['AWS', 'Llamaindex', 'Etl', 'Data Processing', 'Hugging Face']""}]",True
61,What features does Langfuse offer for prompt management and observability in RAG apps?,"[' management and evaluation\nThis integration makes it easy to get started with Tracing. If your application ends up growing into using custom logic or other frameworks/packages, all Langfuse integrations are fully interoperable.\n\nWe have also built additional features to version control and collaborate on prompts (langfuse prompt management), track experiments, and evaluate production traces. For RAG specifically, we collaborated with the RAGAS team and it‚Äôs easy to run their popular eval suite on traces captured with Langfuse (see cookbook).\n## Get started\nThe easiest way to get started is to follow the cookbook and check out the docs.\n## Feedback? Ping us\nWe‚Äôd love to hear any feedback. Come join us on our community discord or add your thoughts to this GitHub thread.\n\n', 'This is a guest post from the team at Langfuse\n\nThere are so many different ways to make RAG work for a use case. What vector store to use? What retrieval strategy to use? LlamaIndex makes it easy to try many of them without having to deal with the complexity of integrations, prompts and memory all at once.\n\nInitially, we at Langfuse worked on complex RAG/agent applications and quickly realized that there is a new need for observability and experimentation to tweak and iterate on the details. In the end, these details matter to get from something cool to an actually reliable RAG application that is safe for users and customers. Think of this: if there is a user session of interest in your production RAG application, how can you quickly see whether the retrieved context for that session was actually relevant or the LLM response was on point?\n\nThus, we started working on Langfuse.com (GitHub) to establish an open source LLM engineering platform with tightly integrated features for tracing, prompt management, and evaluation. In the beginning we just solved our own and our friends‚Äô problems. Today we are at over 1000 projects which rely on Langfuse, and 2.3k stars on GitHub. You can either self-host Langfuse or use the cloud instance maintained by us.\n\nWe are thrilled to announce our new integration with LlamaIndex today. This feature was highly requested by our community and aligns with our project\'s focus on native integration with major application frameworks. Thank you to everyone who contributed and tested it during the beta phase!\n## The challenge\nWe love LlamaIndex, since the clean and standardized interface abstracts a lot of complexity away. Let‚Äôs take this simple example of a VectorStoreIndex and a ChatEngine.\nfrom llama_index.core import SimpleDirectoryReader\nfrom llama_index.core import VectorStoreIndex\n\ndocuments = SimpleDirectoryReader(""./data"").load_data()\n\nindex = VectorStoreIndex.from_documents(documents)\n\nchat_engine = index.as_chat_engine()\n\nprint(chat_engine.chat(""What problems can I solve with RAG?""))\nprint(chat_engine.chat(""How do I optimize my RAG application?""))\nIn just 3 lines we loaded our local documents, added them to an index and initialized a ChatEngine with memory. Subsequently we had a stateful conversation with the chat_engine.\n\nThis is awesome to get started, but we quickly run into questions like:\n- ‚ÄúWhat context is actually retrieved from the index to answer the questions?‚Äù\n- ‚ÄúHow is chat memory managed?‚Äù\n- ‚ÄúWhich steps add the most latency to the overall execution? How to optimize it?‚Äù\n\n## One-click OSS observability to the rescue\n\nWe integrated Langfuse to be a one-click integration with LlamaIndex using the global callback manager.\n\nPreparation\n- Install the community package (pip install llama-index-callbacks-langfuse)\n- Copy/paste the environment variables from the Langfuse project settings to your Python project: \'LANGFUSE_SECRET_KEY\', \'LANGFUSE_PUBLIC_KEY\' and \'LANGFUSE_HOST\'\n\n\nNow, you only need to set the global langfuse handler:\nfrom llama_index.core import set_global_handler\n\nset_global_handler(""langfuse"")\nAnd voil√°, with just two lines of code you get detailed traces for all aspects of your RAG application in Langfuse. They automatically include latency and usage/cost breakdowns.\n\n## Group multiple chat threads into a session\nWorking with lots of teams building GenAI/LLM/RAG applications, we‚Äôve continuously added more features that are useful to debug and improve these applications. One example is session tracking for conversational applications to see the traces in context of a full message thread.\n\nTo activate it, just add an id that identifies the session as a trace param before calling the chat_engine.\nfrom llama_index.core import global_handler\n\nglobal_handler.set_trace_params(\n  session_id=""your-session-id""\n)\n\nchat_engine.chat(""What did he do growing up?"")\nchat_engine.chat(""What did he do at USC?"")\nchat_engine.chat(""How old is he?"")\nThereby you can see all these chat invocations grouped into a session view in Langfuse Tracing:\n\nNext to sessions, you can also track individual users or add tags and metadata to your Langfuse traces.\n## Trace more complex applications and use other Langfuse features for prompt']","Langfuse offers features for prompt management and observability in RAG applications, including version control and collaboration on prompts, tracking experiments, and evaluating production traces. It also provides detailed traces for all aspects of RAG applications, including latency and usage/cost breakdowns, and allows for session tracking to see traces in the context of full message threads.",multi_context,"[{'title': 'One-click Open Source RAG Observability with Langfuse', 'link': 'https://www.llamaindex.ai/blog/one-click-open-source-rag-observability-with-langfuse', 'date': 'Mar 18, 2024', 'tags': ""['LLM', 'Observability']""}, {'title': 'One-click Open Source RAG Observability with Langfuse', 'link': 'https://www.llamaindex.ai/blog/one-click-open-source-rag-observability-with-langfuse', 'date': 'Mar 18, 2024', 'tags': ""['LLM', 'Observability']""}]",True
62,How does document store metadata enhance auto-retrieval with SQL and vector data?,"[' document store metadata though; that way we don‚Äôt need to spend an extra LLM call in the auto-retrieval step inferring the right metadata filters.\n\n# Experiments\nSo how well does this work? It works surprisingly well across a broad range of queries, from queries that can leverage both structured data and unstructured data to queries that are specific to a structured data collection or unstructured data collection.\n## Setup\nOur experiment setup is very simple. We have a SQL table called city_stats which contains the city, population, and country of three different cities: Toronto, Tokyo, and Berlin.\nWe also use a Pinecone index to store Wikipedia articles corresponding to the three cities. Each article is chunked up and stored as a separate ‚ÄúNode‚Äù object; each chunk also contains a title metadata attribute containing the city name.\nWe then derive the VectorIndexAutoRetriever and RetrieverQueryEngine from the Pinecone vector index.\nfrom llama_index.indices.vector_store.retrievers import VectorIndexAutoRetriever\nfrom llama_index.vector_stores.types import MetadataInfo, VectorStoreInfo\nfrom llama_index.query_engine.retriever_query_engine import RetrieverQueryEngine\n\n\nvector_store_info = VectorStoreInfo(\n    content_info=\'articles about different cities\',\n    metadata_info=[\n        MetadataInfo(\n            name=\'city\', \n            type=\'str\', \n            description=\'The name of the city\'),\n    ]\n)\nvector_auto_retriever = VectorIndexAutoRetriever(vector_index, vector_store_info=vector_store_info)\n\nretriever_query_engine = RetrieverQueryEngine.from_args(\n    vector_auto_retriever, service_context=service_context\n)\nYou can also get the SQL query engine as follows\nsql_query_engine = sql_index.as_query_engine()\nBoth the SQL query engine and vector query engine can be wrapped as QueryEngineTool objects.\nsql_tool = QueryEngineTool.from_defaults(\n    query_engine=sql_query_engine,\n    description=(\n        \'Useful for translating a natural language query into a SQL query over a table containing: \'\n        \'city_stats, containing the population/country of each city\'\n    )\n)\nvector_tool = QueryEngineTool.from_defaults(\n    query_engine=query_engine,\n    description=f\'Useful for answering semantic questions about different cities\',\n)\nFinally, we can define our SQLAutoVectorQueryEngine\nquery_engine = SQLAutoVectorQueryEngine(\n    sql_tool,\n    vector_tool,\n    service_context=service_context\n)\n## Results\nWe run some example queries.\nQuery 1\nquery_engine.query(\n  \'Tell me about the arts and culture of the city with the highest population\'\n)\nIntermediate steps:\n\nFinal Response:\nTokyo is the city with the highest population, with 13.96 million people. It is a vibrant city with a rich culture and a wide variety of art forms. From traditional Japanese art such as calligraphy and woodblock prints to modern art galleries and museums, Tokyo has something for everyone. There are also many festivals and events throughout the year that celebrate the city\'s culture and art. Some popular festivals and events in Tokyo include the Sann≈ç Festival at Hie Shrine, the Sanja Festival at Asakusa Shrine, and the biennial Kanda Festivals. These events often feature parades with elaborately decorated floats and thousands of people. Additionally, an enormous fireworks display over the Sumida River takes place annually on the last Saturday of July, attracting over a million viewers. During spring, when cherry blossoms bloom, many residents gather in Ueno Park, Inokashira Park, and the Shinjuku Gyoen National Garden for picnics under the blossoms.\nThis query runs through the full flow of the SQLAutoVectorQueryEngine . It first queries the SQL database for the city with the highest population (‚ÄúTokyo‚Äù), and then queries the vector database with the new query. The results are combined into a final response.\nQuery 2\nquery_engine.query(""Tell me about the history of Berlin"")\nIntermediate Steps:\n\nFinal Response:\nBerlin', ' example query: ‚ÄúTell me about the arts and culture of the city with the highest population.‚Äù\nThe ‚Äúproper‚Äù way to answer this question is roughly as follows:\n- Query the structured table for the city with the highest population.\n\nSELECT city, population FROM city_stats ORDER BY population DESC LIMIT 1\n- Convert the original question into a more detailed question: ‚ÄúTell me about the arts and culture of Tokyo.‚Äù\n- Ask the new question over your vector database.\n- Use the original question + intermediate queries/responses to SQL db and vector db to synthesize the answer.\n\nLet‚Äôs think about some of the high-level implications of such a sequence:\n- Instead of doing embedding search (and optionally metadata filters) to retrieve relevant context, we want to somehow have a SQL query as a first ‚Äúretrieval‚Äù step.\n- We want to make sure that we can somehow ‚Äújoin‚Äù the results from the SQL query with the context stored in the vector database. There is no existing language to ‚Äújoin‚Äù information between a SQL and vector database. We will have to implement this behavior ourselves.\n- Neither data source can answer this question on its own. The structured table only contains population information. The vector database contains city information but no easy way to query for the city with the maximum population.\n\n# A Query Engine to Combine Structured Analytics and Semantic Search\nWe have created a brand-new query engine ( SQLAutoVectorQueryEngine ) that can query, join, sequence, and combine both structured data from both your SQL database and unstructured data from your vector database in order to synthesize the final answer.\nThe SQLAutoVectorQueryEngine is initialized through passing in a SQL query engine ( GPTNLStructStoreQueryEngine ) as well as a query engine that uses our vector store auto-retriever module ( VectorIndexAutoRetriever ). Both the SQL query engine and vector query engines are wrapped as ‚ÄúTool‚Äù objects containing a name and description field.\nReminder: the VectorIndexAutoRetriever takes in a natural language query as input. Given some knowledge of the metadata schema of the vector database, the auto retriever first infers the other necessary query parameters to pass in (e.g. top-k value, and metadata filters), and executes a query against the vector database with all the query parameters.\nDiagram of the flow for SQLAutoVectorQueryEngine\nDuring query-time, we run the following steps:\n- A selector prompt (similarly used in our RouterQueryEngine , see guide) first chooses whether we should query the SQL database or the vector database. If it chooses to use the vector query engine, then the rest of the function execution is the same as querying the RetrieverQueryEngine with VectorIndexAutoRetriever .\n- If it chooses to query the SQL database, it will execute a text-to-SQL query operation against the database, and (optionally) synthesize a natural language output.\n- A query transformation is run, to convert the original question into a more detailed question given the results from the SQL query. For instance if the original question is ‚ÄúTell me about the arts and culture of the city with the highest population.‚Äù, and the SQL query returns Tokyo as the city with the highest population, then the new query is ‚ÄúTell me about the arts and culture of Tokyo.‚Äù The one exception is if the SQL query itself is enough to answer the original question; if it is, then function execution returns with the SQL query as the response.\n- The new query is then run through through the vector store query engine, which performs retrieval from the vector store and then LLM response synthesis. We enforce using a VectorIndexAutoRetriever module. This allows us to automatically infer the right query parameters (query string, top k, metadata filters), given the result of the SQL query. For instance, with the example above, we may infer the query to be something like query_str=""arts and culture"" and filters={""title"": ""Tokyo""} .\n- The original question, SQL query, SQL response, vector store query, and vector store response are combined into a prompt to synthesize the final answer.\n\nTaking a step back, here are some general comments about this approach:\n- Using our auto-retrieval module is our way of simulating a join between the SQL database and vector database. We effectively use the results from our SQL query to determine the parameters to query the vector database with.\n- This also implies that there doesn‚Äôt need to be an explicit mapping between the items in the SQL database and the metadata in the vector database, since we can rely on the LLM being able come up with the right query for different items. It would be interesting to model explicit relationships between structured tables and']","Document store metadata enhances auto-retrieval by allowing the system to avoid additional LLM calls during the auto-retrieval step, as it can directly infer the right metadata filters needed for queries. This integration of structured data from SQL and unstructured data from the vector database enables more efficient and accurate retrieval of relevant information.",multi_context,"[{'title': 'Combining Text-to-SQL with Semantic Search for Retrieval Augmented Generation', 'link': 'https://www.llamaindex.ai/blog/combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b', 'date': 'May 28, 2023', 'tags': ""['Artificial Intelligence', 'Machine Learning', 'NLP', 'Data Science', 'Database']""}, {'title': 'Combining Text-to-SQL with Semantic Search for Retrieval Augmented Generation', 'link': 'https://www.llamaindex.ai/blog/combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b', 'date': 'May 28, 2023', 'tags': ""['Artificial Intelligence', 'Machine Learning', 'NLP', 'Data Science', 'Database']""}]",True
63,What to consider for a vector store and node parser in RAG for better doc processing?,"['_params = {\'chunk_size\': chunk_size, \'chunk_overlap\': chunk_overlap}\n        \n    elif parser_type == ""SemanticSplitterNodeParser"":\n        if \'embed_model\' not in st.session_state:\n            st.warning(""Please select an embedding model first."")\n            return None, None\n        \n        embed_model = st.session_state[\'embed_model\']\n        buffer_size = st.number_input(""Buffer Size"", min_value=1, value=1)\n        breakpoint_percentile_threshold = st.number_input(""Breakpoint Percentile Threshold"", min_value=0, max_value=100, value=95)\n        parser = SemanticSplitterNodeParser(buffer_size=buffer_size, breakpoint_percentile_threshold=breakpoint_percentile_threshold, embed_model=embed_model)\n        parser_params = {\'buffer_size\': buffer_size, \'breakpoint_percentile_threshold\': breakpoint_percentile_threshold}\n        \n    elif parser_type == ""TokenTextSplitter"":\n        chunk_size = st.number_input(""Chunk Size"", min_value=1, value=1024)\n        chunk_overlap = st.number_input(""Chunk Overlap"", min_value=0, value=20)\n        parser = TokenTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n        parser_params = {\'chunk_size\': chunk_size, \'chunk_overlap\': chunk_overlap}\n\n    # Save the parser type and parameters to the session state\n    st.session_state[\'node_parser_type\'] = parser_type\n    st.session_state[\'node_parser_params\'] = parser_params\n    \n    return parser, parser_type\nBelow the node parser selection, I have also included a preview of the first node of the text after splitting/parsing, just to give the users an idea of how the chunking is actually happening based the selected node parser and the relevant parameters.\n## select_response_synthesis_method\nThis function allows users to choose how the RAG pipeline synthesizes responses. I have included varioud response synthesis methods supported by Llamaindex including refine, tree_summarize, compact, simple_summarize, accumulate and compact_accumulate.\nUsers can click on the more information link to get more details about response synthesis and the different types.\n\ndef select_response_synthesis_method():\n    st.header(""Choose Response Synthesis Method"")\n    col1, col2 = st.columns([4,1])\n    with col2:\n        st.markdown(""""""\n                    [More Information](https://docs.llamaindex.ai/en/stable/module_guides/querying/response_synthesizers/response_synthesizers.html)\n                    """""")\n    response_modes = [\n        ""refine"",\n        ""tree_summarize"",  \n        ""compact"", \n        ""simple_summarize"", \n        ""accumulate"", \n        ""compact_accumulate""\n    ]\n    selected_mode = st.selectbox(""Select Response Mode"", response_modes, on_change=reset_pipeline_generated)\n    response_mode = selected_mode\n    return response_mode, selected_mode\n## select_vector_store\nEnables users to choose a vector store, which is a critical component for', '    st.markdown(F""Embedding Model: {embed_model.model_name}"")\n        st.markdown(F""Embed Batch Size: {embed_model.embed_batch_size}"")\n        st.markdown(F""Embed Batch Size: {embed_model.max_length}"")\n\n\n    return embed_model, selected_model\n## select_node_parser Function\nThis function allows users to choose a node parser, which is instrumental in breaking down documents into manageable chunks or nodes, facilitating better handling and processing. I have included some of the most commonly used node parsers supported by Llamaindex, which include SentenceSplitter, CodeSplitter, SemanticSplitterNodeParser, TokenTextSplitter, HTMLNodeParser, JSONNodeParser and MarkdownNodeParser.\n\ndef select_node_parser():\n    st.header(""Choose Node Parser"")\n    col1, col2 = st.columns([4,1])\n    with col2:\n        st.markdown(""""""\n                    [More Information](https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/root.html)\n                    """""")\n    parser_types = [""SentenceSplitter"", ""CodeSplitter"", ""SemanticSplitterNodeParser"",\n                    ""TokenTextSplitter"", ""HTMLNodeParser"", ""JSONNodeParser"", ""MarkdownNodeParser""]\n    parser_type = st.selectbox(""Select Node Parser"", parser_types, on_change=reset_pipeline_generated)\n    \n    parser_params = {}\n    if parser_type == ""HTMLNodeParser"":\n        tags = st.text_input(""Enter tags separated by commas"", ""p, h1"")\n        tag_list = tags.split(\',\')\n        parser = HTMLNodeParser(tags=tag_list)\n        parser_params = {\'tags\': tag_list}\n        \n    elif parser_type == ""JSONNodeParser"":\n        parser = JSONNodeParser()\n        \n    elif parser_type == ""MarkdownNodeParser"":\n        parser = MarkdownNodeParser()\n        \n    elif parser_type == ""CodeSplitter"":\n        language = st.text_input(""Language"", ""python"")\n        chunk_lines = st.number_input(""Chunk Lines"", min_value=1, value=40)\n        chunk_lines_overlap = st.number_input(""Chunk Lines Overlap"", min_value=0, value=15)\n        max_chars = st.number_input(""Max Chars"", min_value=1, value=1500)\n        parser = CodeSplitter(language=language, chunk_lines=chunk_lines, chunk_lines_overlap=chunk_lines_overlap, max_chars=max_chars)\n        parser_params = {\'language\': language, \'chunk_lines\': chunk_lines, \'chunk_lines_overlap\': chunk_lines_overlap, \'max_chars\': max_chars}\n        \n    elif parser_type == ""SentenceSplitter"":\n        chunk_size = st.number_input(""Chunk Size"", min_value=1, value=1024)\n        chunk_overlap = st.number_input(""Chunk Overlap"", min_value=0, value=20)\n        parser = SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n        parser']","The context mentions that users can choose a vector store, which is a critical component for document processing in the RAG pipeline. Additionally, it discusses the selection of a node parser, which is instrumental in breaking down documents into manageable chunks or nodes, facilitating better handling and processing. Various node parsers supported by Llamaindex include SentenceSplitter, CodeSplitter, SemanticSplitterNodeParser, TokenTextSplitter, HTMLNodeParser, JSONNodeParser, and MarkdownNodeParser.",multi_context,"[{'title': 'RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex', 'link': 'https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089', 'date': 'Feb 2, 2024', 'tags': ""['Rag', 'No Code', 'Llamaindex', 'OpenAI', 'Code Generation']""}, {'title': 'RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex', 'link': 'https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089', 'date': 'Feb 2, 2024', 'tags': ""['Rag', 'No Code', 'Llamaindex', 'OpenAI', 'Code Generation']""}]",True
64,What can Helmet AI do to improve leadership decisions in fast-changing markets?,"['We had an awesome time at the Berkeley Hackathon two weeks ago (6/17‚Äì6/18). The attendance stats were impressive:\n- 1200 hackers\n- 262 submitted projects\n- 2 real-life llamas ü¶ô\n\nLlamaIndex sponsored a ‚ÄúBest Knowledge-Intensive LLM App‚Äù prize series at the hackathon. The criteria was an app that leveraged a knowledge base of custom data to build innovative new application experiences.\nWe announced three prize winners along with an honorable mention. We are excited to feature each project in a special highlight below. In each highlight, the creators describe the project mission and what it solves, the implementation+tech stack, challenges, and future directions. Check it out! üëá\n# First Prize Winner: Helmet AI\nCreators: Jaiveer Singh, Devin Mui, Ethan Mehta, Manav Rathod\nDevpost: https://devpost.com/software/helmet-ai\n\n## Introduction\nIn today‚Äôs rapidly evolving business landscape, staying ahead of the competition is paramount for success. However, the deluge of information and the ever-changing market dynamics can make it challenging for business leaders to make informed decisions. In this blog post, we introduce Helmet AI, a cutting-edge market intelligence tool designed to empower leadership teams with real-time insights and a competitive edge. Join us as we explore the capabilities, technology stack, and future prospects of Helmet AI.\n## Unveiling Helmet AI\n\nHelmet AI is an innovative market intelligence tool that harnesses the power of advanced technologies to provide leaders with actionable insights and an unparalleled understanding of the global business landscape. With its context-aware Ingestion Engine and Insight Extractor powered by OpenAI‚Äôs GPT models, Helmet AI offers a comprehensive solution for tracking breaking news, uncovering hidden relationships, and extracting valuable, personalized insights from vast amounts of data. For ease of use, Helmet AI displays these insights in a familiar, Twitter-like ‚ÄúFeed‚Äù interface. Additionally, Helmet AI offers a Chat interface for users to ask questions about a particular news story to Helmet‚Äôs knowledgeable chat agent.\n## Key Features and Technology Stack\nContext-Aware Ingestion Engine:\n- Helmet AI‚Äôs Ingestion Engine continuously monitors the vast landscape of breaking news and global events. By leveraging techniques such as subscribing to RSS feeds for up to date news data and processing documents with LlamaIndex and LangChain, the engine builds a complete understanding of real-time events and their implications on various user profiles. Embeddings are stored in a Pinecone Vector Database.\n\nInsight Extractor with OpenAI‚Äôs GPT Models:\n- The Insight Extractor component of Helmet AI utilizes the power of OpenAI‚Äôs GPT models to identify and concisely explain the intricate relationships between seemingly disparate topics surfaced in your feed. By transforming raw data into actionable insights with intelligent explanations, leaders can make informed decisions based on an understanding of market trends and complex dynamics.\n\nScalable Infrastructure:\n- Helmet AI is built on Azure‚Äôs robust infrastructure, utilizing a range of services such as App Services, a PostgreSQL Database, and Github Actions for orchestrating Deployments. The implementation also incorporates GraphQL for efficient data retrieval and processing.\n\n\n## Challenges Overcome and Accomplishments\nDuring the development of Helmet AI, our team encountered various challenges, including integrating MindsDB with Azure and overcoming limitations with Gmail authentication. However, we were able to overcome these obstacles and successfully implemented Helmet AI in just 36 hours during the Berkeley AI Hackathon. Additionally, we established a seamless deployment process using GitHub Actions, automating manual service orchestration. The experience was particularly rewarding for the first-time hackers on the team.\n## Key Learnings\nThroughout the development process, our team gained valuable insights. We discovered the importance of setting up deployment flows early on to reduce stress during crunch time. Embracing best practices in software engineering proved crucial. Furthermore, we realized the potential of leveraging advanced language models as implicit knowledge graphs, expanding their applications beyond traditional embeddings.\n## Future Prospects\nLooking ahead, Helmet AI aims to scale up the Ingestion Engine to handle the entirety of the web, leveraging technologies like AnyScale. The team plans to collaborate with enterprise business development teams to initiate pilot programs and gather feedback for further refinement. With a solid foundation in place, Helmet AI hopes to have an impact on the way leaders gather insights and make strategic decisions.\n## Conclusion\nHelmet AI represents a solid attempt at a game-changing solution for business leaders seeking to stay ahead in today‚Äôs fast-paced business world. By leveraging cutting-edge technologies, including AI-powered insight extraction and explanation and real-time data analysis, Helmet AI empowers leaders to confidently navigate market challenges and seize emerging opportunities. As the tool continues to evolve and expand its capabilities, the future of market intelligence']","Helmet AI improves leadership decisions in fast-changing markets by providing real-time insights and a comprehensive understanding of the global business landscape. It utilizes advanced technologies, including a context-aware Ingestion Engine and an Insight Extractor powered by OpenAI‚Äôs GPT models, to track breaking news, uncover hidden relationships, and extract valuable insights from vast amounts of data. This enables leaders to make informed decisions based on an understanding of market trends and complex dynamics.",multi_context,"[{'title': 'Special Feature: Berkeley Hackathon Projects (LlamaIndex Prize Winners)', 'link': 'https://www.llamaindex.ai/blog/special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0', 'date': 'Jun 30, 2023', 'tags': ""['Hackathons', 'AI', 'Large Language Models', 'Berkeley', 'NLP']""}]",True
65,How does starting a Jupyter notebook help with data loading and model finetuning in Modal?,"['LaMa\nThe first step here is to open up the Jupyter notebook. The notebook is organized into a series of runnable scripts that each perform the steps needed to load data.\nOur code uses Modal for every step of the orchestration, and Modal is best used on top of the Python scripts themselves. That is why a lot of these cells don‚Äôt contain Python blocks of their own.\nFirst we use Modal to load in the b-mc2/sql-create-context dataset. This is a simple task that just loads in the dataset and formats it into a .jsonl file.\nmodal run src.load_data_sql --data-dir ""data_sql""\nAs we can see, under the hood the task is quite straightforward:\n# Modal stubs allow our function to run remotely\n@stub.function(\n    retries=Retries(\n        max_retries=3,\n        initial_delay=5.0,\n        backoff_coefficient=2.0,\n    ),\n    timeout=60 * 60 * 2,\n    network_file_systems={VOL_MOUNT_PATH.as_posix(): output_vol},\n    cloud=""gcp"",\n)\ndef load_data_sql(data_dir: str = ""data_sql""):\n    from datasets import load_dataset\n\n    dataset = load_dataset(""b-mc2/sql-create-context"")\n\n    dataset_splits = {""train"": dataset[""train""]}\n    out_path = get_data_path(data_dir)\n\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n\n    for key, ds in dataset_splits.items():\n        with open(out_path, ""w"") as f:\n            for item in ds:\n                newitem = {\n                    ""input"": item[""question""],\n                    ""context"": item[""context""],\n                    ""output"": item[""answer""],\n                }\n                f.write(json.dumps(newitem) + ""\\n"")\n## Step 2: Run Finetuning Script\nThe next step is to run our finetuning script on the parsed dataset.\nmodal run src.finetune_sql --data-dir ""data_sql"" --model-dir ""model_sql""\nThe finetuning script performs the following steps.\nSplits the dataset into training and validation splits\ntrain_val = data[""train""].train_test_split(test_size=val_set_size, shuffle=True, seed=42)\ntrain_data = train_val[""train""].shuffle().map(generate_and_tokenize_prompt)\nval_data = train_val[""test""].shuffle().map(generate_and_tokenize_prompt)\nFormats each split into tuples of (input prompt, label): The input query and context are formatted into the same input prompt. The input prompt is then tokenized, and the labels are set to the exact same as the input prompt ‚Äî this allows the model to train on next-token prediction.\ndef generate_and_tokenize_prompt(data_point):\n  full_prompt = generate_prompt_sql(\n      data_point[""input""],\n      data_point[""context""],\n      data_point[""output""],\n  )\n  tokenized_full_prompt = tokenize(full_prompt)\n  if not train_on_inputs:\n      raise NotImplementedError(""not implemented yet"")\n  return tokenized_full_prompt\nThe input prompt is the exact same as what was given at the top of this blog.\nWhen the finetuning script is run, the model is saved in the remote cloud directory specified by model_dir (which is set to a default value if not']","Starting a Jupyter notebook helps with data loading and model finetuning in Modal by organizing runnable scripts that perform the necessary steps to load data and run the finetuning script. The notebook allows users to execute these scripts sequentially, facilitating the orchestration of tasks using Modal, which is designed to work on top of Python scripts.",multi_context,"[{'title': 'Easily Finetune Llama 2 for Your Text-to-SQL Applications', 'link': 'https://www.llamaindex.ai/blog/easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d', 'date': 'Aug 17, 2023', 'tags': ""['Llamaindex', 'Fine Tuning', 'Large Language Models', 'AI', 'NLP']""}]",True
66,How do voice activation and design improve C3 Voice Assistant's accessibility for those with typing issues?,"['\n      const utterance = new SpeechSynthesisUtterance(text);\n\n      const voices = window.speechSynthesis.getVoices();\n      if (voices.length &gt; 0) {\n        utterance.voice = voices[3]; // You can change this to select different voices\n      }\n\n      utterance.onstart = () =&gt; {\n        console.log(""TTS starts speaking"");\n        setShowSpinner(false);\n      };\n\n      utterance.onend = () =&gt; {\n        setAppState(""idle"");\n        if (wakeWordRecognitionRef.current) {\n          wakeWordRecognitionRef.current.start(); // Restart wake word listener after speaking\n        }\n      };\n      synthRef.current.speak(utterance);\n    }\n## 5. UI Rendering\nThe return statement of the App function contains the JSX code for rendering the app\'s UI. This includes buttons for starting/stopping the voice interaction, a display area for the transcript, and a chat sidebar showing the history of interactions.\nBy combining voice recognition, LLM integration, and speech synthesis, this frontend component provides a comprehensive and accessible interface for interacting with the C3 Voice Assistant.\n# Backend Server Setup\n- Initialize Create-Llama: Run npx create-llama@latest in your terminal.\n- Follow the prompts to set up a Python FastAPI backend, which we can be integrated with our frontend.\n- Use poetry install and poetry shell to prepare the environment.\n- Create a .env file with OPENAI_API_KEY=<openai_api_key>.\n- Generate Embeddings (optional): If a ./data directory exists, run python app/engine/generate.py.\n- Execute python main.pyto start the server.\n- Test the API: Use curl --location \'localhost:8000/api/chat\' --header \'Content-Type: application/json\' --data \'{ ""messages"": [{ ""role"": ""user"", ""content"": ""Hello"" }] }\' to test.\n- Modify API behavior in app/api/routers/chat.py. The server supports CORS for all origins, alterable with the ENVIRONMENT=prod setting.\n\n# Integration\nOnce the backend server is set up, integrating it with the frontend is straightforward. Simply update the fetchResponseFromLLM function in your frontend\'s App.js to call the backend server URL. This change ensures that when the frontend makes a request, it communicates with your newly configured backend, thus effectively integrating the two components.\n# Final Thoughts\nWrapping up, the C3 Voice Assistant isn‚Äôt just a tech showcase; it‚Äôs a stride towards democratizing AI. It‚Äôs about making powerful AI tools, like LLMs and RAG, accessible and user-friendly. This project is more than lines of code ‚Äî it‚Äôs a push to break down tech barriers and empower everyone.\nYour thoughts and feedback are invaluable ‚Äî let‚Äôs make AI more accessible together!\nLink to Github Repo: Frontend and Backend\nConnect with Me on LinkedIn\nLinkedin Post\n', '# Introduction\nThe C3 Voice Assistant is my latest project aimed at making Large Language Model (LLM) and Retrieval-Augmented Generation (RAG) applications more accessible. This voice-activated assistant caters to a broad audience, including those facing typing challenges or accessibility issues.\n# Features\n- Voice Activation: Initiated by saying ‚ÄúC3.‚Äù Alternatively, users can click the blue ring to activate the listening mode of the app. The wake word ‚ÄúC3‚Äù is configurable and you can choose any other word.\n- Universal Accessibility: Ideal for users preferring voice commands or facing typing challenges.\n- LLM Integration: Capable of general queries and document-specific inquiries (e.g., Nvidia‚Äôs FY 2023 10K report).\n- User-Friendly Interface: The interface of the AI voice assistant is designed for simplicity and ease of use, focusing on voice chat interactions. It features a minimalistic and user-friendly React.js layout. Additionally, there is a convenient sidebar that displays the entire chat history in text format, allowing users to review and reflect on their interactions with the AI.\n\n\n# The Tech Stack\nThe app is built on a robust and flexible tech stack that ensures a smooth, reliable, and efficient user experience. Here‚Äôs an overview:\n- Frontend: The user interface is a custom application developed using React.js. It‚Äôs designed to be minimalistic yet highly functional, prioritizing ease of use and accessibility.\n- Backend: The server-side operations are powered by Python Flask. I‚Äôve utilized the innovative ‚Äòcreate-llama‚Äô feature from LlamaIndex, which significantly streamlines the development process.\n- Hosting: For a seamless performance, the frontend of the C3 Voice Assistant is hosted on Vercel. The backend, on the other hand, is deployed on Render, ensuring efficient management and operation of server-side tasks.\n\n# Building the Frontend\n\nThe frontend, built with React.js, focuses on user interaction and accessibility. The App.js script incorporates features like wake word recognition, speech-to-text conversion, state management, and dynamic UI elements like speech bubbles and spinners.\n## 1. Component and State Initialization\nThis section sets up the React component and initializes various states, such as appState to track the current state of the app (idle, listening, speaking), and transcript to store the text transcribed from user speech.\nimport React, { useState, useRef, useEffect } from ""react"";\nimport ""./App.css"";\n\nconst App = () =&gt; {\n  const [appState, setAppState] = useState(""idle"");\n  const [transcript, setTranscript] = useState("""");\n  // Additional state and ref declarations...\n};\n## 2. Speech Recognition Setup\nIn this useEffect hook, two speech recognition instances are initialized: one for detecting the wake word ‚ÄúC3‚Äù and another for the main speech recognition. This setup ensures that the app starts listening for commands when ‚ÄúC3‚Äù is mentioned.\nYou can easily swap ‚ÄúC3‚Äù with any other wake word of your choice.\nuseEffect(() =&gt; {\n    // Wake word listener setup\n    const WakeWordSpeechRecognition =\n      window.SpeechRecognition || window.webkitSpeechRecognition;\n    if (WakeWordSpeechRecognition &amp;&amp; !wakeWordRecognitionRef.current) {\n      wakeWordRecognitionRef.current = new WakeWordSpeechRecognition();\n      wakeWordRecognitionRef.current.continuous = true;\n      wakeWordRecognitionRef.current.interimResults = false;\n\n      wakeWordRecognitionRef.current.onresult = (event) =&gt; {\n        const transcript = event.results[event.results.length - 1][0].transcript\n          .trim()\n          .toLowerCase();\n        if (transcript.includes(""c3"")) {\n          toggleRecording(); // Start the main speech recognition process\n        }\n      };\n\n      wakeWordRecognitionRef.current.start();\n    }\n\n    // Main speech recognition setup\n    const SpeechRecognition =\n      window.']","Voice activation in the C3 Voice Assistant is initiated by saying 'C3' or by clicking a button, making it accessible for users who may have typing challenges. The design focuses on a user-friendly interface that prioritizes simplicity and ease of use, allowing users to interact with the assistant through voice commands rather than text input. This combination of voice activation and a minimalistic design enhances accessibility for a broad audience, including those facing typing challenges.",multi_context,"[{'title': 'AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5 (Deployed in Prod on Vercel and Render)', 'link': 'https://www.llamaindex.ai/blog/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a', 'date': 'Jan 14, 2024', 'tags': ""['Llamaindex', 'Voice Assistant', 'OpenAI', 'Accessibility', 'Inclusive Ai']""}, {'title': 'AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5 (Deployed in Prod on Vercel and Render)', 'link': 'https://www.llamaindex.ai/blog/ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a', 'date': 'Jan 14, 2024', 'tags': ""['Llamaindex', 'Voice Assistant', 'OpenAI', 'Accessibility', 'Inclusive Ai']""}]",True
67,What benefits does the AI copilot provide crisis counselors for admin tasks and youth communication in high-demand scenarios?,"[' to assess the stage of the conversation, given that there are recommended styles and questions for each stage (e.g. establishing rapport, risk assessment)\n- Closed-loop feedback cycle: Allow counselors to thumbs-up or thumbs-down selected responses, as a natural way to collect human feedback which can be used for further model or agent training\n\n## Conclusion: A Step Toward Efficient and Effective Crisis Care\nOur AI copilot for crisis counselors represents a significant step toward more efficient and effective crisis care. By automating administrative tasks, it frees up counselors to focus on their core mission of providing youth in crisis a safe place to talk. This not only enhances the quality of care provided but also addresses the pressing issue of counselor shortage by maximizing the impact of existing resources. As we continue to refine and expand this technology, we envision a future where crisis counseling is more accessible, responsive, and impactful for all those in need\u200b\u200b.\n## References\n- https://rag-a-thon.devpost.com/\n- https://devpost.com/software/counselor-copilot\n- https://www.mhanational.org/issues/state-mental-health-america\n- https://www.aamc.org/news/growing-psychiatrist-shortage-enormous-demand-mental-health-services\n- https://www.thetrevorproject.org/\n- https://github.com/zrizvi93/trevorhack\n\n', 'Co-authored by: Riya Jagetia,\xa0Tarun Malik,\xa0Divija N,\xa0Sharon Tan,\xa0Zehra Rizvi, Amanda Piyapanee\nAt the recent LlamaIndex RAG-a-thon [1], our team‚Äôs ‚ÄúCounselor Copilot‚Äù won 2nd prize in the Traditional track and 1st prize in the Datastax/AstraDB category. More details can be found on our DevPost [2] writeup.\n## Introduction\nAgainst the backdrop of growing strain on mental health services [3, 4], non-profit organizations like The Trevor Project [5] are a critical part of the care ecosystem. Focusing on helping LGBTQ+ youth who are contemplating suicide, The Trevor Project provides accessible crisis services including via TrevorText, an online chat service with trained volunteer counselors.\n## Problem: The Dual Challenge Faced by Crisis Counselors\nHowever, TrevorText counselors face significant challenges. Not only is there high demand for counselors during busy times like holidays and night shifts, but also, counselors have to juggle a number of administrative tasks such as sifting through forms, responding to messages across multiple chats, and locating relevant local resources. This not only increases the risk of counselors burning out but also hampers their ability to provide timely and effective care.\nIn light of these challenges, there‚Äôs a pressing need for innovative solutions to bridge the gap between the demand and supply of crisis services.\nWhile our hackathon project focused on augmenting TrevorText, our product can be easily extended to general crisis chat alternatives as well.\n## The Winning Solution: An AI Copilot for Crisis Counselors\nCounselor Copilot is a real-time assistant for crisis counselors that takes into account contact context and chat history to suggest replies so that counselors can focus on what they do best: providing care. There is no prompting that is needed from counselors; the copilot works seamlessly in the background.\nFurther, the copilot never directly replies to contacts; instead, replies are suggested and can be edited.\nCounselor copilot takes into account contact context and chat history to provide real-time reply suggestions to the counselors\nSpecifically, the copilot automates counselor tasks that include but are not limited to:\n- Retrieving and synthesizing contact data from complex PDFs in real-time. This also provides counselors context on their contacts when conversations are initiated.\n- Assessing from the chat context if emergency intervention is required. If so, suggesting escalation to a supervisor.\n- Using existing resources and guidelines from the organization to suggest appropriate replies.\n\n\n4. Searching for location-specific resources for contacts, and quickly sharing those resources via email.\n\n5. Completing case forms in a CRM for contacts, including summarizing the interaction.\n\nWhile these tasks are important and necessary, they pull attention away from conversations with youth in crisis and take up precious time.\nWith Counselor copilot, these tasks are completed when they are required and without any prompting from counselors, providing more bandwidth for counselors and ultimately leading to higher-quality conversations with patients.\nBelow is a demo of our solution:\n## How we built it\nWhen the chat is initiated, the Counselor Copilot gets the contact‚Äôs data from the CRM, which is stored in complex PDFs. We used LlamaParse to extract relevant contact data in real-time and then provide a summary of that data to counselors as context at the beginning of each conversation.\n\nFurther, we used a LlamaIndex ReAct Agent to monitor the conversation and ‚Äî based on the chat history and contact context ‚Äî deploy the right tool. Tools at the ReAct Agent‚Äôs disposal include:\n- Escalating the conversation to a supervisor\n- Suggesting a response and related resources based on The Trevor Project‚Äôs guidelines\n- Searching the web for location-specific resources and sending the resources to the contact\n\nFor tool #2, we created a vector database that contains The Trevor Project‚Äôs documents, which highlight key guidelines for counselors based on different scenarios and situations that they may face. We used RAG to retrieve resources relevant to the conversation, and GPT4 to draft a response for the counselor based on those resources, both of which are essential due to the sensitive nature of the conversation.\nLastly, we used the conversation content to fill out a form with key Salesforce fields (e.g. name, age, city, state), as well as to summarize the conversation.\n## Possible Extensions\nWe‚Äôre excited by the potential for others to build on our work [6] and extend Counselor Copilot further. Some ideas include:\n- Reduce costs and improve quality of suggested responses: Fine-tune a state-of-the-art open-source LLM on extracts of chat conversations conducted by counselors\n- More targeted conversation management: Add a tool for the agent']","The AI copilot provides crisis counselors with several benefits for administrative tasks and youth communication in high-demand scenarios. It automates administrative tasks such as retrieving and synthesizing contact data, assessing if emergency intervention is required, suggesting appropriate replies based on existing resources and guidelines, searching for location-specific resources, and completing case forms in a CRM. By handling these tasks seamlessly in the background, the copilot allows counselors to focus more on providing care, ultimately leading to higher-quality conversations with youth in crisis.",multi_context,"[{'title': 'Bridging the Gap in Crisis Counseling: Introducing Counselor Copilot', 'link': 'https://www.llamaindex.ai/blog/bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3', 'date': 'Feb 24, 2024', 'tags': ""['Llamaindex', 'Mental Health', 'Retrieval Augmented', 'Generative Ai Use Cases', 'LLM']""}, {'title': 'Bridging the Gap in Crisis Counseling: Introducing Counselor Copilot', 'link': 'https://www.llamaindex.ai/blog/bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3', 'date': 'Feb 24, 2024', 'tags': ""['Llamaindex', 'Mental Health', 'Retrieval Augmented', 'Generative Ai Use Cases', 'LLM']""}]",True
68,"What upgrades in Llama 2's design boost its dialogue skills, and how do they address chatbot LLM safety and performance issues?","[' parameters for the large language models developed? Generated Answer: The two primary objectives achieved in this work are the development and release of Llama 2, a collection of pretrained and fine-tuned large language models (LLMs), and the optimization of these models for dialogue use cases. The range of parameters for the large language models developed is from 7 billion to 70 billion.Context-1: Llama 2 : Open Foundation and Fine-Tuned Chat Models Hugo Touvron‚àóLouis Martin‚Ä†Kevin Stone‚Ä† Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic Sergey Edunov Thomas Scialom‚àó GenAI, Meta Abstract In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat , are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on ourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosed- source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs. ‚àóEqual contribution, corresponding authors: {tscialom, htouvron}@meta.com ‚Ä†Second author Contributions for all the authors can be found in Section A.1.arXiv:2307.09288v2 [cs.CL] 19 Jul 2023Context-2: (2021)alsoilluminatesthedifficultiestiedtochatbot-oriented LLMs, with concerns ranging from privacy to misleading expertise claims. Deng et al. (2023) proposes a taxonomic framework to tackle these issues, and Bergman et al. (2022) delves into the balance between potential positive and negative impacts from releasing dialogue models. InvestigationsintoredteamingrevealspecificchallengesintunedLLMs,withstudiesbyGangulietal.(2022) and Zhuoet al. (2023) showcasing a variety ofsuccessful attack typesand their effects onthe generation of harmful content. National security agencies and various researchers, such as (Mialon et al., 2023), have also raisedredflagsaroundadvancedemergentmodelbehaviors,cyberthreats,andpotentialmisuseinareaslike biological warfare. Lastly, broader societal issues like job displacement due to accelerated AI research and an over-reliance on LLMs leading to training data degradation are also pertinent considerations (Acemoglu andRestrepo,2018;AutorandSalomons,2018;Webb,2019;Shumailovetal.,2023). Wearecommittedto continuing our work engaging with the broader policy, academic, and industry community on these issues. 7 Conclusion Inthisstudy,wehaveintroduced Llama 2,anewfamilyofpretrainedandfine-tunedmodelswithscales of7billionto70billionparameters. Thesemodelshavedemonstratedtheircompetitivenesswithexisting open-source chat models, as well as competency that is equivalent to some proprietary models on evaluation setsweexamined,althoughtheystilllagbehindothermodelslikeGPT-4. Wemeticulouslyelaboratedonthe methodsandtechniquesappliedinachievingourmodels,withaheavyemphasisontheiralignmentwiththe principlesofhelpfulnessandsafety. Tocontributemoresignificantlytosocietyandfosterthepaceofresearch, wehaveresponsiblyopenedaccesst', 'Llama 2 is a huge milestone in the advancement of open-source LLMs. The biggest model and its finetuned variants sit at the top of the Hugging Face Open LLM Leaderboard. Multiple benchmarks show that it is approaching GPT-3.5 (or in some cases even surpassing it) in terms of performance. All of this means that open-source LLMs are an increasingly viable and reliable option for use in complex LLM applications, from RAG systems to agents.\n# Context: Llama-2‚Äì7B is Not Good at Text-to-SQL\nA downside of the smallest Llama 2 model (7B parameters), however, is that it‚Äôs not very good at generating SQL, making it impractical for structured analytics use cases. As an example, we tried prompting Llama 2 to generate the correct SQL statement given the following prompt template:\nYou are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables. \n\nYou must output the SQL query that answers the question.\n\n### Input:\n{input}\n\n### Context:\n{context}\n\n### Response:\nHere we plugged in a sample entry from the sql-create-context dataset.\ninput: In 1981 which team picked overall 148?\ncontext: CREATE TABLE table_name_8 (team VARCHAR, year VARCHAR, overall_pick VARCHAR)\nMeanwhile, here is the generated output vs. correct output:\nGenerated output: SELECT * FROM `table_name_8` WHERE \'1980\' = YEAR AND TEAM = ""Boston Celtics"" ORDER BY OVERALL_PICK DESC LIMIT 1;\n\nCorrect output: SELECT team FROM table_name_8 WHERE year = 1981 AND overall_pick = ""148""\nThis is clearly not ideal. Unlike ChatGPT and GPT-4, Llama 2 does not reliably produce well-formatted and correct SQL outputs.\nThis is exactly where fine-tuning comes in ‚Äî given a proper corpus of text-to-SQL data, we can teach Llama 2 to be better at generating SQL outputs from natural language. At a high-level, fine-tuning involves modifying the weights of the model in some capacity. There are different ways to finetune models, from updating all parameters of the network, to a subset of the parameters, to only finetuning additional parameters (e.g. how LoRA works).\nOnce the model is finetuned, it can still be plugged into a downstream LLM application. That is exactly what this tutorial aims to show. It is a step more involved than our existing tutorials which have primarily focused on ‚Äúin-context learning‚Äù and ‚Äúretrieval-augmentation‚Äù use cases ‚Äî freezing the model itself but focusing on the orchestration of data into the input prompt. Finetuning can have a high learning curve and also require a lot of compute. This tutorial makes it as easy as possible to get started.\n# Tutorial Overview\nIn this tutorial, we show you how you can finetune Llama 2 on a text-to-SQL dataset, and then use it for structured analytics against any SQL database using the capabilities of LlamaIndex.\nHere is the stack that we use:\n- b-mc2/sql-create-context from Hugging Face datasets as the training dataset\n- OpenLLaMa open_llama_7b_v2 as the base model\n- PEFT for efficient finetuning\n- Modal for handling all cloud compute/orchestration for finetuning. And also for the excellent reference doppel-bot repo.\n- LlamaIndex for text-to-SQL inference against any SQL database.\n\nSpecial mention to the awesome Llama 2 tutorial from Anyscale that helped to inspire this project.\nAll of our materials can be found in our Github repo: https://github.com/run-llama/modal_finetune_sql (again emphasizing that this is adapted from doppel-bot). Also, the full tutorial can be found in our Jupyter notebook guide. Make sure to check it out!\nAs mentioned above, performing finetuning does require quite a few steps. Our goal is to make this as straightforward as possible to follow and use out of the box. We don‚Äôt cover all the nitty gritty detailsof Modal, PEFT, the finetuning procedure itself, etc. but we do give a rough overview.\nThere are also certainly higher-level APIs that we could‚Äôve used (e.g. OpenAI, Lamini) in order to achieve this task. There‚Äôs plenty of room for followup tutorials to cover these topics!\n## Step 1: Loading Training Data for Finetuning L']",The upgrades in Llama 2's design that boost its dialogue skills include the optimization of the models for dialogue use cases and the implementation of safety improvements. These enhancements address chatbot LLM safety and performance issues by ensuring that the models outperform open-source chat models on most benchmarks and align with principles of helpfulness and safety.,multi_context,"[{'title': 'LlamaIndex: RAG Evaluation Showdown with GPT-4 vs. Open-Source Prometheus Model', 'link': 'https://www.llamaindex.ai/blog/llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277', 'date': 'Dec 15, 2023', 'tags': ""['Open Source', 'Llamaindex', 'Gpt 4', 'LLM', 'OpenAI']""}, {'title': 'Easily Finetune Llama 2 for Your Text-to-SQL Applications', 'link': 'https://www.llamaindex.ai/blog/easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d', 'date': 'Aug 17, 2023', 'tags': ""['Llamaindex', 'Fine Tuning', 'Large Language Models', 'AI', 'NLP']""}]",True
69,What dual fine-tuning boosts LLM data use & retriever relevance in RA-DIT?,"['## Introduction\nLarge Language Models (LLMs) improve performance by accessing external data for background knowledge tasks related. However, existing approaches require costly modifications during LM‚Äôs pre-training or integrating the data store after the model has been trained. On the downside, both strategies lead to suboptimal performance.\nTo address this problem an AI Research team at Meta has proposed a method called RA-DIT: RETRIEVAL-AUGMENTED DUAL INSTRUCTION TUNING that allows any LLM to be upgraded to include retrieval features.\nIn this blog post, we will explore RA-DIT capabilities to have better performance on Retrieval Augmentation Generation (RAG) through building the dataset and fine-tuning the models.\nThe RA-DIT approach involves two distinct fine-tuning steps:\n- Update a pre-trained LM to better use retrieved information.\n- Update the retriever to return more relevant results\n\n## How it works\nThe RA-DIT approach separately fine-tunes the LLM and the retriever. The LLM is updated to maximize the probability of the correct answer given the retrieval-augmented instructions, while the retriever is updated to minimize how much the document is semantically similar (relevant) to the query.\nBelow we are going through each step from generating the fine-tuning dataset, fine-tuning the language model for better predictions, and refining the retrieval search process.\n## Fine-tuning Dataset\nThe fine-tuning dataset is tailored to enhance the language model‚Äôs ability to leverage knowledge and boost its contextual awareness during prediction generation. Generating Q/A pairs, summarizing data, and incorporating chain-of-thought reasoning can lead to improved results when integrated with the models.\n\nFollowing our LamaIndex implementation, we retrieve the top_k nodes, generate Q/A pairs from the documents, and then augment the data. We use the Q/A pairs through the QueryResponseDataset module, which returns a (query, response) pair for the fine-tuning dataset. While the retrieval fine-tuning data set is created on Q/A pairs data.\n## Language Model Fine-tuning\nWith our fine-tuning dataset in hand, we can refine our LLM to achieve two main benefits: Adapt the LLM to better utilization of relevant background knowledge and train the LLM to produce accurate predictions even with incorrectly retrieved chunks, empowering the model to rely on its own knowledge.\n## Retriever Fine-tuning\n\nThe retriever is fine-tuned using the LM-Supervised Retrieval (LSR) method. In this approach, the LLM assesses the information fetched by the retriever. If the LLM finds the information misaligned with the given query, it sends feedback to the retriever. Using this feedback, the retriever refines its search process, ensuring it fetches data that the LLM can effectively use. This collaboration enhances the overall quality of the answers provided.\n## Evaluation\nTo assess the suggested method, the authors employed specific datasets and metrics. Let‚Äôs delve into each of these to grasp the experimental results better.\n## Metrics\nAn ‚Äúexact match‚Äù (EM) metric was used to measure how closely the model‚Äôs prediction matches the ground truth answer.\n## Dataset\nThe methodology was tested on two distinct tasks:\n- Knowledge-intensive tasks.\n- Commonsense reasoning.\n\nLet‚Äôs explore the datasets utilized for both of these tasks.\n## Knowledge-intensive dataset\nFor knowledge-intensive tasks the selected datasets predominantly focus on the model‚Äôs capacity to access, understand, and relay deep and specific knowledge. They encompass questions rooted in facts, general trivia, and complex domain-specific queries;\nThe datasets used are MMLU, Natural Questions (NQ), TriviaQA, and a subset of tasks from the KILT benchmark.\n## Commonsense reasoning dataset\nCommonsense reasoning datasets challenge the model‚Äôs ability to reason and infer based on general knowledge and everyday scenarios. They contain questions and scenarios that typically don‚Äôt rely on deep domain knowledge but rather on intuitive and general world understanding.\nThe datasets used are BoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC-E, ARC-C, OBQA.\nFor a better understanding of how these datasets were utilized you can check the paper for better understanding.\n## Results\nIn a comparative analysis of model performance on knowledge-intensive (Table 2 below) and commonsense reasoning tasks (Table 3 below), three models were considered:\n- LLAMA 65B\n- LLAMA 65B REPLUG (only retrieval augmentation)\n- RA-DIT 65B\n\n## Knowledge Intensive Tasks\nEvaluations are conducted in 0-shot, 5-shot, and 64']",The dual fine-tuning in RA-DIT boosts LLM data use by updating a pre-trained language model to better utilize retrieved information and updating the retriever to return more relevant results.,multi_context,"[{'title': 'Improving RAG effectiveness with Retrieval-Augmented Dual Instruction Tuning (RA-DIT)', 'link': 'https://www.llamaindex.ai/blog/improving-rag-effectiveness-with-retrieval-augmented-dual-instruction-tuning-ra-dit-01e73116655d', 'date': 'Oct 18, 2023', 'tags': ""['AI', 'Large Language Models', 'Machine Learning', 'Fine Tuning', 'Llamaindex']""}]",True
70,What are the benefits of fixed document embeddings with a linear adapter in the EmbeddingAdapterFinetuneEngine for retrieval performance?,"['We‚Äôve added capabilities in LlamaIndex allowing you to fine-tune a linear adapter on top of embeddings produced from any model (sentence_transformers, OpenAI, and more).\nThis allows you to transform your embedding representations into a new latent space that‚Äôs optimized for retrieval over your specific data and queries. This can lead to small increases in retrieval performance that in turn translate to better performing RAG systems.\nA nice bonus: you do not need to re-embed your documents by using this adapter! Simply transform the query instead.\nWe have a full end-to-end guide showing how you can generate a synthetic dataset, fine-tune the linear adapter, and evaluate its performance.\n# Context\nThe concept of fine-tuning your embedding model is powerful. In fact, we were inspired to both add a full example repository / blog post as well as native abstractions in LlamaIndex showing how you can fine-tune a sentence_transformers model over any unstructured text corpus (with our SentenceTransformersFinetuneEngine).\nHowever, this approach has some limitations:\n- The SentenceTransformersFinetuneEngine is limited to fine-tuning sentence_transformers models.\n- After finetuning the embedding model, you will need to re-embed your document corpus.\n\nDuring our Finetuning + RAG webinar last Friday, Jo (Vespa) mentioned the exact same problem: fine-tuning the embeddings model requires you to reindex your documents. However, his work with Vespa explored the concept of ‚Äúfreezing‚Äù document embeddings using a foundation model, and instead training a transformation on the query embedding.\nThis inspired us to explore a similar embedding fine-tuning approach that was simultaneously more general but also allowed us to freeze existing document embeddings.\n# Approach\nOur brand-new EmbeddingAdapterFinetuneEngine fine-tunes a linear adapter on top of query embeddings produced by any model. The linear adapter is simply a linear transformation that specifically transforms the query embedding while keeping document embeddings fixed.\nThe linear adapter can be used on top of any existing embeddings model: SBERT embeddings, OpenAI embeddings, Cohere embeddings, and more. As a result you can just plug this in on top of any embedding model that you‚Äôre already using!\n\nSince document embeddings are unchanged, this means that you can always fine-tune this linear adapter after you‚Äôve generated embeddings for your documents. You can choose to arbitrarily re-train this adapter on top of changing data distributions, without needing to re-embed all your documents.\n## Technical Details\nAs mentioned above, the linear adapter simply performs a linear transformation on top of the query embedding while keeping the Document embeddings fixed (with a weight matrix W + bias term b):\n\nAnd that‚Äôs it! If document embeddings can be represented as a (n x d) matrix D, where n is number of documents and d is the embedding dimension, then embedding similarity is just measured by\n\nThe linear adapter is trained using a similar loss term as the MultipleNegativesRankingLoss function in sentence_transformers ‚Äî given a batch of positive (question, context) examples, the function uses cross-entropy loss under the hood to penalize the ground-truth (question, context) pairs for being far apart and swapped pairs for being too close.\nAdditional Notes: We ended up writing the bulk of this fine-tuning logic in plain PyTorch, but taking heavy inspiration from the sentence_transformers source code. We couldn‚Äôt use sentence_transformers directly since we take in embeddings as inputs rather than raw text. You can take a look at some of our training code here.\n# Notebook Walkthrough\nIn this notebook walkthrough, we follow a similar set of steps as our previous blog post on embedding fine-tuning:\n- Generate a synthetic question-context dataset for both training and evaluation.\n- Fine-tuning our linear adapter on top of an existing model (e.g. SBERT)\n- Getting the embedding model, and evaluating it.\n\nAs with the previous post, we use the UBER and LYFT 10K as example data. We use Lyft to generate our training dataset and Uber to generate our evaluation dataset.\nThe full guide is here: https://gpt-index.readthedocs.io/en/latest/examples/finetuning/embeddings/finetune_embedding_adapter.html\n## Generate a Synthetic Dataset for Trraining and Evaluation\nWe use our helper abstractions, generate_qa_embedding_', 'UPDATE 9/10/2023: We‚Äôve included embedding finetuning abstractions into the LlamaIndex repo, so this repo is technically outdated! Please check out our embedding fine-tuning guides in the core documentation.\nWe‚Äôve created a comprehensive, end-to-end guide showing you how to fine-tune an embedding model to improve performance of Retrieval Augmented Generation (RAG) systems over any unstructured text corpus (no labels required!).\nThe result is a 5‚Äì10% performance increase in retrieval evaluation metrics ‚Äî our finetuned bge model almost reaches text-embedding-ada-002 levels of retrieval performance in terms of hit rate. This enables more accurate retrieval which leads to better RAG systems as a whole.\nThis tutorial is helpful to anyone building RAG systems:\n- If you‚Äôre new to finetuning, no problem! We have step by step notebooks walking through the key steps. Simply substitute the file links for your own data, and just run every cell.\n- Finetuning embedding models is lightweight and doesn‚Äôt require a GPU. These notebooks were tested on an M2 Macbook Pro.\n\nResources\n- Repo: https://github.com/run-llama/finetune-embedding\n- Notebooks: Dataset Generation, Finetuning, Evaluation\n\n# Background/Context\n## The Current RAG Stack\nRAG is a popular paradigm for connecting Large Language Models (LLMs) with an external source of data that was not present in its training corpus. It pairs a retrieval model over a knowledge bank with the LLM through its input prompt space. RAG stacks typically look like the following:\n- Indexing: Prepare a corpus of unstructured text, parse/chunk it. Then embed each chunk and put in a vector database.\n- Query-time: Retrieve context from the vector db using top-k embedding similarity lookup, and stuff context into the LLM input space.\n\n(Of course RAG can be much more advanced than this, and LlamaIndex provides tools for both simple and advanced RAG)\nUnfortunately RAG is easy to prototype by cobbling together the different components, but hard to productionize. The simple stack has many failure modes and oftentimes the issue lies with bad retrieval ‚Äî if the returned context is irrelevant to the query, then the capability of the LLM is irrelevant; the answer will always be bad.\n## How Can We Make Retrieval Better?\nWe can try more sophisticated retrieval algorithms (e.g. hybrid search, reranking).\nAn insight from our recent production RAG webinar, however, is that the embeddings themselves may not live in an optimal latent space for your data. Embeddings generated by pre-trained models may be close/far from each other based on the pre-training objective, but may not completely align with your own retrieval objective. For instance, if you‚Äôre building search over ML ArXiv papers, you may want the embeddings to align semantically with specific ML concepts (e.g. ‚ÄúLLMs‚Äù, ‚ÄúNLP‚Äù) and not filler words ‚ÄúThis paper is‚Ä¶‚Äù).\nFinetuning is a way to solve that. The concept of finetuning has become increasingly popular in the LLM space, with technological advancements as well as easy-to-use services.\nIn this tutorial, we focus on finetuning the embedding model. We show how finetuning the embedding model can lead to better retrieval performance.\n## Challenges/Considerations\nWhen you finetune embeddings, you need training examples. In the case of embeddings, this typically means that you have both ‚Äúpositive‚Äù and ‚Äúnegative‚Äù examples ‚Äî pairs of texts that should be close to each other and far from each other.\nAn issue is that we don‚Äôt have these positive or negative examples apriori. Given a dataset of unstructured text, is it possible to automatically generate these example pairs?\nWith LlamaIndex you can! We use LlamaIndex modules to automatically generate a set of questions from unstructured text chunks. These (question, chunk) pairs are then used as positive examples as training signals for the model (negative examples are randomly sampled across other chunks).\nThe next section shows a full walkthrough across all of our modules.\n# Walkthrough\nAt a high-level, we do the following:\n- Generating synthetic dataset for training and evaluation (Notebook)\n- Finetuning an opensource embedding model (Notebook)\n- Evaluating the embedding model (Notebook)\n\n## Generating synthetic dataset for training and evaluation\nThe key idea here is that we can leverage an LLM to generate hypothetical questions that are best answered by a given piece of']","The benefits of fixed document embeddings with a linear adapter in the EmbeddingAdapterFinetuneEngine for retrieval performance include the ability to fine-tune the linear adapter on top of query embeddings without needing to re-embed the document corpus. This allows for improved retrieval performance while keeping the document embeddings unchanged, enabling the adapter to be retrained on changing data distributions without the overhead of re-embedding all documents.",multi_context,"[{'title': 'Fine-Tuning a Linear Adapter for Any Embedding Model', 'link': 'https://www.llamaindex.ai/blog/fine-tuning-a-linear-adapter-for-any-embedding-model-8dd0a142d383', 'date': 'Sep 6, 2023', 'tags': ""['Fine Tuning', 'Embedding', 'Llamaindex', 'NLP', 'AI']""}, {'title': 'Fine-Tuning Embeddings for RAG with Synthetic Data', 'link': 'https://www.llamaindex.ai/blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971', 'date': 'Aug 25, 2023', 'tags': ""['Fine Tuning', 'LLM', 'Llamaindex', 'NLP', 'Embedding']""}]",True
71,What's the role of a structured dataset in fine-tuning linear adapters and evaluating retrieval metrics?,"['pairs , to generate our training and evaluation dataset. This function takes in any set of text nodes (chunks) and generates a structured dataset containing (question, context) pairs.\nfrom llama_index.finetuning import (\n    generate_qa_embedding_pairs,\n    EmbeddingQAFinetuneDataset,\n)\n\n# generate\ntrain_dataset = generate_qa_embedding_pairs(train_nodes)\nval_dataset = generate_qa_embedding_pairs(val_nodes)\n\n# save\ntrain_dataset.save_json(""train_dataset.json"")\nval_dataset.save_json(""val_dataset.json"")\n\n# load \ntrain_dataset = EmbeddingQAFinetuneDataset.from_json(""train_dataset.json"")\nval_dataset = EmbeddingQAFinetuneDataset.from_json(""val_dataset.json"")\n## Fine-tuning our Linear Adapter\nWe then fine-tune our linear adapter on top of an existing embedding model. We import our newEmbeddingAdapterFinetuneEngine abstraction, which takes in an existing embedding model and a set of training parameters.\nIn this example we use the bge-small-en sentence-transformers model, but we can also use any embedding model in LlamaIndex/LangChain.\nfrom llama_index.finetuning import EmbeddingAdapterFinetuneEngine\nfrom llama_index.embeddings import resolve_embed_model\nimport torch\n\nbase_embed_model = resolve_embed_model(""local:BAAI/bge-small-en"")\n# alternative: use OpenAI\n# from llama_index.embeddings import OpenAIEmbedding\n# openai = OpenAIEmbedding()\n\nfinetune_engine = EmbeddingAdapterFinetuneEngine(\n    train_dataset,\n    base_embed_model,\n    model_output_path=""&lt;model_output_path&gt;"",\n    epochs=4,\n    verbose=True,\n    # can optionally pass along any parameters that go into `train_model`\n    # optimizer_class=torch.optim.SGD,\n    # optimizer_params={""lr"": 0.01}\n)\nWe can then call fine-tune to kick off the fine-tuning job. Training a linear model is quite straightforward and doesn‚Äôt require heavy machinery ‚Äî this can easily run on a Macbook.\nfinetune_engine.finetune()\n## Getting the Embedding Model, and Evaluating it\nOnce the fine-tuning job is then, we can then fetch our embedding model.\nWe can either directly fetch it from our finetune_engine , or import our new LinearAdapterEmbeddingModel and construct it in a more manual fashion.\nOption 1:\nembed_model = finetune_engine.get_finetuned_model()\nOption 2:\nfrom llama_index.embeddings import LinearAdapterEmbeddingModel\n\nembed_model = LinearAdapterEmbeddingModel(base_embed_model, ""&lt;model_output_path&gt;"")\nThe next step is to evaluate it. We compare the fine-tuned model against the base model, as well as against text-embedding-ada-002.\nWe evaluate with two ranking metrics:\n- Hit-rate metric: For each (query, context) pair, we retrieve the top-k documents with the query. It‚Äôs a hit if the results contain the ground-truth context.\n- Mean Reciprocal Rank: A slightly more granular ranking metric that looks at the ‚Äúreciprocal rank‚Äù of the ground-truth context in the top-k retrieved set. The reciprocal rank is defined as 1/rank. Of course, if the results don‚Äôt contain the context, then the reciprocal rank is 0.\n\nSome additional comments:\n- We ran with 4 epochs over the Lyft documents\n- We used Adam as an optimizer with the default learning rate (we tried SGD and it didn‚Äôt work as well)\n\nResults\nQuantiative metrics (hit-rate and MRR) for ada, bge, and our fine-tuned model\nIn terms of hit-rate, the base model gets 78.7% hit-rate on the validation dataset, and the fine', 'We‚Äôve added capabilities in LlamaIndex allowing you to fine-tune a linear adapter on top of embeddings produced from any model (sentence_transformers, OpenAI, and more).\nThis allows you to transform your embedding representations into a new latent space that‚Äôs optimized for retrieval over your specific data and queries. This can lead to small increases in retrieval performance that in turn translate to better performing RAG systems.\nA nice bonus: you do not need to re-embed your documents by using this adapter! Simply transform the query instead.\nWe have a full end-to-end guide showing how you can generate a synthetic dataset, fine-tune the linear adapter, and evaluate its performance.\n# Context\nThe concept of fine-tuning your embedding model is powerful. In fact, we were inspired to both add a full example repository / blog post as well as native abstractions in LlamaIndex showing how you can fine-tune a sentence_transformers model over any unstructured text corpus (with our SentenceTransformersFinetuneEngine).\nHowever, this approach has some limitations:\n- The SentenceTransformersFinetuneEngine is limited to fine-tuning sentence_transformers models.\n- After finetuning the embedding model, you will need to re-embed your document corpus.\n\nDuring our Finetuning + RAG webinar last Friday, Jo (Vespa) mentioned the exact same problem: fine-tuning the embeddings model requires you to reindex your documents. However, his work with Vespa explored the concept of ‚Äúfreezing‚Äù document embeddings using a foundation model, and instead training a transformation on the query embedding.\nThis inspired us to explore a similar embedding fine-tuning approach that was simultaneously more general but also allowed us to freeze existing document embeddings.\n# Approach\nOur brand-new EmbeddingAdapterFinetuneEngine fine-tunes a linear adapter on top of query embeddings produced by any model. The linear adapter is simply a linear transformation that specifically transforms the query embedding while keeping document embeddings fixed.\nThe linear adapter can be used on top of any existing embeddings model: SBERT embeddings, OpenAI embeddings, Cohere embeddings, and more. As a result you can just plug this in on top of any embedding model that you‚Äôre already using!\n\nSince document embeddings are unchanged, this means that you can always fine-tune this linear adapter after you‚Äôve generated embeddings for your documents. You can choose to arbitrarily re-train this adapter on top of changing data distributions, without needing to re-embed all your documents.\n## Technical Details\nAs mentioned above, the linear adapter simply performs a linear transformation on top of the query embedding while keeping the Document embeddings fixed (with a weight matrix W + bias term b):\n\nAnd that‚Äôs it! If document embeddings can be represented as a (n x d) matrix D, where n is number of documents and d is the embedding dimension, then embedding similarity is just measured by\n\nThe linear adapter is trained using a similar loss term as the MultipleNegativesRankingLoss function in sentence_transformers ‚Äî given a batch of positive (question, context) examples, the function uses cross-entropy loss under the hood to penalize the ground-truth (question, context) pairs for being far apart and swapped pairs for being too close.\nAdditional Notes: We ended up writing the bulk of this fine-tuning logic in plain PyTorch, but taking heavy inspiration from the sentence_transformers source code. We couldn‚Äôt use sentence_transformers directly since we take in embeddings as inputs rather than raw text. You can take a look at some of our training code here.\n# Notebook Walkthrough\nIn this notebook walkthrough, we follow a similar set of steps as our previous blog post on embedding fine-tuning:\n- Generate a synthetic question-context dataset for both training and evaluation.\n- Fine-tuning our linear adapter on top of an existing model (e.g. SBERT)\n- Getting the embedding model, and evaluating it.\n\nAs with the previous post, we use the UBER and LYFT 10K as example data. We use Lyft to generate our training dataset and Uber to generate our evaluation dataset.\nThe full guide is here: https://gpt-index.readthedocs.io/en/latest/examples/finetuning/embeddings/finetune_embedding_adapter.html\n## Generate a Synthetic Dataset for Trraining and Evaluation\nWe use our helper abstractions, generate_qa_embedding_']","The role of a structured dataset in fine-tuning linear adapters and evaluating retrieval metrics is to provide (question, context) pairs that can be used for training the linear adapter on top of query embeddings. This structured dataset allows for the assessment of the fine-tuned model's performance through metrics such as hit-rate and Mean Reciprocal Rank (MRR).",multi_context,"[{'title': 'Fine-Tuning a Linear Adapter for Any Embedding Model', 'link': 'https://www.llamaindex.ai/blog/fine-tuning-a-linear-adapter-for-any-embedding-model-8dd0a142d383', 'date': 'Sep 6, 2023', 'tags': ""['Fine Tuning', 'Embedding', 'Llamaindex', 'NLP', 'AI']""}, {'title': 'Fine-Tuning a Linear Adapter for Any Embedding Model', 'link': 'https://www.llamaindex.ai/blog/fine-tuning-a-linear-adapter-for-any-embedding-model-8dd0a142d383', 'date': 'Sep 6, 2023', 'tags': ""['Fine Tuning', 'Embedding', 'Llamaindex', 'NLP', 'AI']""}]",True
72,What tool in LlamaIndex aids in data management and semantic searches?,"[' conversation history and dumped into the LLM context window, then we may run into context window limitations.\nLlamaIndex offers tool abstractions to help deal with this. Our LoadAndSearchToolSpec wraps any given tool that may return a large amount of data, and it splits it into two tools: a load tool that will dynamically store the data in an index, and a search tool that allows for search over that index.\nOn the Metaphor side, this is also where we define a search_and_retrieve_documents endpoint that combines search and retrieve. This allows the agent to make a single query to retrieve a large number of documents, which when combined with the LoadAndSearchToolSpec will get directly stored within an index. If the agent were to call search and retrieve separately, then it would both take longer and consume more tokens to write the search results to conversation history, and then passing that into the prompt again to call retrieve over all document IDs.\nCreating the LoadAndSearchToolSpec:\nfrom llama_index.tools.tool_spec.load_and_search.base import LoadAndSearchToolSpec\n# The search_and_retrieve_documents tool is the third in the tool list, as seen above\nwrapped_retrieve = LoadAndSearchToolSpec.from_defaults(\n  metaphor_tool_list[2],\n)\nNow let‚Äôs walk through a full execution example:\n# Just pass the wrapped tools and the get_date utility\nagent = OpenAIAgent.from_tools(\n  [*wrapped_retrieve.to_tool_list(), metaphor_tool_list[4]],\n  verbose=True,\n)\nprint(agent.chat(\'Can you summarize everything published in the last month regarding news on superconductors\'))\nThe output here shows that the agent calls multiple tools in succession to get the right answer.\n=== Calling Function ===\nCalling function: current_date with args: {}\nGot output: 2023-08-20\n========================\n=== Calling Function ===\nCalling function: search_and_retrieve_documents with args: {\n  ""query"": ""superconductors"",\n  ""start_published_date"": ""2023-07-20"",\n  ""end_published_date"": ""2023-08-20""\n}\n[Metaphor Tool] Autoprompt: ""Here is an interesting article about superconductors:\nGot output: Content loaded! You can now search the information using read_search_and_retrieve_documents\n========================\n=== Calling Function ===\nCalling function: read_search_and_retrieve_documents with args: {\n  ""query"": ""superconductors""\n}\nGot output: \nSuperconductors are materials that can perfectly conduct electricity. They are used in a variety of applications, such as particle accelerators, nuclear fusion devices, MRI machines, and maglev trains. However, so far, no superconductor has been proven to work at ambient pressures and temperatures. On July 22, scientists in South Korea published research claiming to have solved this problem with a material called LK-99, which has an electrical resistivity that drops to near zero at 30 degrees Celsius (86 degrees Fahrenheit).\n========================\nIn the last month, there have been developments in the field of superconductors. Scientists in South Korea have published research on a material called LK-99, which has the ability to conduct electricity with near-zero resistance at a temperature of 30 degrees Celsius (86 degrees Fahrenheit). This breakthrough could potentially lead to the development of superconductors that work at ambient pressures and temperatures, opening up new possibilities for various applications such as particle accelerators, nuclear fusion devices, MRI machines, and maglev trains.\nThe agent used the get_date tool to determine the current month, and then applied the filters in Metaphor based on publication date when calling search. It then loaded the documents using retrieve_documents and read them using read_retrieve_documents.\n# Conclusion\nAs shown above, the integration between LlamaIndex data agents + Metaphor search has the potential to bypass existing limitations with LLMs and even RAG systems. We‚Äôre excited to continue exploring this further in future blog posts.\nWe encourage you to play around with the notebook ‚Äî make sure to check it out!\nResources:\n- Notebook: https://github.com/emptycrown/llama-hub/blob/main/llama_hub/tools/notebooks/metaphor.ipynb\n- LlamaHub: https://llamahub.ai/l/tools-metaphor\n- Metaphor: https://platform.metaphor.systems/\n- Metaphor API Docs: https://docs.metaphor', '(co-authored by Jerry Liu, CEO of LlamaIndex, Jeffrey Wang, co-founder at Metaphor, and Adam Hoffman, Software Engineer at Hypotenuse Labs)\nWe‚Äôre incredibly excited to launch an integration between LlamaIndex and Metaphor: combine the capabilities of LlamaIndex data agents with Metaphor as a native LLM search tool to enable knowledge workers capable of answering any question over any data, no matter how recent or complex.\nWe provide a deeper overview of Metaphor and the LlamaIndex integration below. We also walk through our example notebook to showcase how they can be combined.\n\n# Background/Context\nState-of-the art large language models (LLMs) such as ChatGPT, GPT-4, Claude 2 have incredible reasoning capabilities that unlock a wide variety of use cases ‚Äî from insight extraction to question-answering to general workflow automation. Yet they are limited in their abilities to retrieve contextually relevant information. A popular stack that has emerged is to setup a retrieval-augmented generation (RAG) system, which combines LLMs with external storage solutions over a static knowledge source. Frameworks such as LlamaIndex provide a variety of tools to setup both simple and complex RAG systems.\nYet even this is not the complete picture. LLMs should ideally be able to dynamically search and retrieve information from the external world, not just depend on a static source of knowledge. This would allow them to fulfill a more general set of tasks and not only perform search/retrieval, but perform actions as well.\nTo do this well, we need two core components:\n- General abstractions that allow LLMs to intelligently perform various tasks over your data, in both a ‚Äúread‚Äù and ‚Äúwrite‚Äù fashion\n- A good search engine tailored for LLM use\n\nLlamaIndex data agent abstractions help to satisfy the first core component. A complete data agent consists of both a reasoning loop as well as a set of Tools. These tools can be interfaces for search/retrieval or more generally any external API. Given a query, the agent will execute its reasoning loop and dynamically figure out the set of Tools it will need to fulfill the task at hand.\nData agents have access to a rich set of Tools offered on LlamaHub ‚Äî these range from Gmail API, to a SQL db API, to a basic tool in the form of Bing search. We‚Äôve shown that they are capable of e2e tasks from sending emails, scheduling meetings, to automating custom support insight extraction. Yet there has never been a tool tailored for LLM use.\n# Overview of Metaphor\nThe Metaphor API is designed to connect your LLM to the internet. It allows you to perform fully neural, highly semantic searches over the Internet and also get clean, HTML content from the results.\nMetaphor was trained to predict links on the internet, given how people talk about things on the Internet. For example, someone might post about a great article they read like this:\nFound an amazing article I read about the history of Rome‚Äôs architecture: [LINK]\nBy training a model to predict these links given how people talk about them, the end result is a totally different way to search the internet ‚Äî search as if you‚Äôre about to share the link you want. While a little unintuitive at first, searching this way can return extremely high quality results. But for the purposes of LlamaIndex, you won‚Äôt need to worry about this because by default, queries will be converted into Metaphor prompts.\nWhy would you use Metaphor Search over Bing/Google? There are 3 main reasons:\n- You can search fully semantically, for instance with feelings or complex descriptors.\n- You can search only for the type of entity that you want. Companies, articles, people.\n- You can find content that Google simply doesn‚Äôt surface well, maybe because keywords aren‚Äôt the right tool or maybe just because Google doesn‚Äôt care about returning good results for that type of content.\n\nTo learn more, you can read the full Metaphor API blog post.\n# Integration Details\nThe Metaphor Tool Spec in LlamaHub is an API interface that consists of 5 tools that an agent can use.\n- Search: The entrypoint to Metaphor ‚Äî allows an agent to pass a natural language query that will then be passed to the Metaphor search engine. This endpoint also contains some additional parameters, such as the number of results, domains to include/exclude, and a date filter.\n- Retrieve Documents: This will retrieve the content of a set of documents given IDs. These ids are returned as part of the results from the search endpoint above.\n- Search and Retrieve Documents: This is a convenience endpoint that combines the functionality of `search` and `']","The LoadAndSearchToolSpec in LlamaIndex aids in data management and semantic searches by wrapping tools that can return large amounts of data, allowing for dynamic storage in an index and enabling search over that index.",multi_context,"[{'title': 'LlamaIndex + Metaphor: Towards Automating Knowledge Work with LLMs', 'link': 'https://www.llamaindex.ai/blog/llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f', 'date': 'Aug 21, 2023', 'tags': ""['Search', 'LLM', 'Agents', 'NLP', 'Llamaindex']""}, {'title': 'LlamaIndex + Metaphor: Towards Automating Knowledge Work with LLMs', 'link': 'https://www.llamaindex.ai/blog/llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f', 'date': 'Aug 21, 2023', 'tags': ""['Search', 'LLM', 'Agents', 'NLP', 'Llamaindex']""}]",True
73,What can create-llama do for full-stack app generation and backend integration?,"['It‚Äôs a llama on a ship, geddit?\nLast week we released create-llama, a command-line tool to generate a full-stack LlamaIndex application for Retrieval-Augmented Generation (RAG). The response was fantastic, so we‚Äôll be following up with more templates and more features. We also wanted to show you just how easy it is to get your generated app all the way to production. So here‚Äôs a step by step guide, for each of the three backends we currently support: Next.js serverless, Express, and Python.\n# Next.js backend\nThe serverless full-stack Next.js application is the simplest version to deploy as you only have one artifact to deploy. Because it‚Äôs a Next.js app we‚Äôll be deploying to Vercel, the home of Next.js.\n## Step 1: run create-llama\n\nFirst run create-llama to generate your app. We strongly recommend generating a new OpenAI API key and supplying it at generation time (create-llama apps can be customized to use other LLMs but that‚Äôs out of scope for this tutorial).\n## Step 2: create a GitHub repository and push your app to it\n\nThe easiest way to deploy on Vercel is from a linked GitHub repository. Your generated app is already set up as a git repo, so all you have to do after creating a new empty repo is follow the instructions to push it up. This should give you a repo that looks a bit like this:\n\n## Step 3: import your repo into Vercel\nSelect the option to create a new project from a git repo:\n\nand select the repo you just created after authorizing:\n\n## Step 4: configure your project\nBecause this is a Next.js app and this is Vercel, there‚Äôs very little you need to do! The only thing you need to remember is to click ‚Äúenvironment variables‚Äù and create a variable called OPENAI_API_KEY with your key.\n\n## Step 5: Deploy!\nThat‚Äôs it! Deploying a Next.js app to Vercel is pretty easy.\n\nYour deployed app should look like this:\n\nCongratulations, you‚Äôve deployed a full-stack RAG application!\n# Express backend\nIf you chose to generate an Express backend with a Next.js frontend instead, let‚Äôs get you into production with those. We‚Äôll be deploying both frontend and backend to Render, a fantastic service for both static sites and dynamic web applications.\nThere will be 3 big things to do here:\n- Deploy the static frontend\n- Deploy the backend, and give the frontend permission to call it\n- Tell the frontend where the backend is located\n\nWe promise you‚Äôll be production in no time.\n## Step 1: run create-llama\nThis is just like the same step in Next.js\n## Step 2: push the code to a new GitHub repo\n\nLike Vercel, the easiest way to push a site to production is from a linked git repo. Your generated app already has a git repo initiated with frontend and backend folders, so you can go ahead and push them both to a single GitHub repository just as in the Next.js backend.\n## Step 3: Start a new static site\n\nWe‚Äôll be deploying your frontend first as a static site. After authorizing, select the repository where you pushed your frontend and backend; we‚Äôll specify that we‚Äôre deploying the frontend in the next step.\n\n## Step 4: configure your static site\n\nThere are several changes you need to make to the default configuration to successfully publish your static frontend:\n- Name your site something memorable, it will become the URL of your site once it‚Äôs deployed\n- Set your root directory to frontend\n- Set your build command to npm install; npm run build\n- Set your publish directory to out\n- Finally, click ‚ÄúAdvanced‚Äù and set an environment variable called NODE_VERSION to 20 . The default on Render is a much older version of Node so don‚Äôt skip this step!\n\n## Step 5: deploy your static frontend\n\nClick the Deploy button and watch your site build! You should now have a site live at a URL something like frontend-name-you-picked.onrender.com .\n## Step 6: start a new web service\n', 'Introducing create-llama, the easiest way to get started with LlamaIndex!\nUpdate 2023‚Äì11‚Äì20: we now have a guide to deploying your create-llama apps!\n\nWant to use the power of LlamaIndex to load, index and chat with your data using LLMs like GPT-4? It just got a lot easier! We‚Äôve created a simple to use command-line tool that will generate a full-stack app just for you ‚Äî just bring your own data! To get started, run:\nnpx create-llama\nThe app will then ask you a series of questions about what kind of app you want. You‚Äôll need to supply your own OpenAI API key (or you can customize it to use a different LLM), and make a few decisions.\n# How does it get my data?\nThe generated app has a data folder where you can put as many files as you want; the app will automatically index them at build time and after that you can quickly chat with them. If you‚Äôre using LlamaIndex.TS as the back-end (see below), you‚Äôll be able to ingest PDF, text, CSV, Markdown, Word and HTML files. If you‚Äôre using the Python backend, you can read even more types, including audio and video files!\n# Technical details\nThe front-end it generates is a Next.js application, with your choice of shadcn/ui or vanilla HTML and CSS for styling.\nFor the back-end, you have 3 options:\n- Next.js: if you select this option, you‚Äôll have a full stack Next.js application that you can deploy to a host like Vercel in just a few clicks. This uses LlamaIndex.TS, our TypeScript library.\n- Express: if you want a more traditional Node.js application you can generate an Express backend. This also uses LlamaIndex.TS.\n- Python FastAPI: if you select this option you‚Äôll get a backend powered by the llama-index python package, which you can deploy to a service like Render or fly.io.\n\nThere are a couple of other questions you‚Äôll be asked:\n- Streaming or non-streaming: if you‚Äôre not sure, you‚Äôll probably want a streaming backend.\n- SimpleChatEngine or ContextChatEngine : the ContextChatEngine is the one that uses your data. If you just want to chat with GPT, you can use the SimpleChatEngine.\n\n# Go forth and customize!\nOnce you‚Äôve got your app up and running, you can customize it to your heart‚Äôs content! By default, for cost reasons, the app will use GPT-3.5-Turbo. If you‚Äôd like to use GPT-4 you can configure that by modifying the file app/api/chat/llamaindex-stream.ts (in the Next.js backend) or you can configure it to use a different LLM entirely! LlamaIndex has integrations with dozens of LLMs, both APIs and local.\n']","Create-llama is a command-line tool that generates a full-stack LlamaIndex application for Retrieval-Augmented Generation (RAG). It allows users to create applications with different backends, including Next.js, Express, and Python. The generated app can be easily deployed to platforms like Vercel or Render, and it supports customization with various LLMs, including the option to use an OpenAI API key. Additionally, it provides a data folder for users to index their files and chat with them using LLMs.",multi_context,"[{'title': 'Shipping your Retrieval-Augmented Generation app to production with create-llama', 'link': 'https://www.llamaindex.ai/blog/shipping-your-retrieval-augmented-generation-app-to-production-with-create-llama-7bbe43b6287d', 'date': 'Nov 20, 2023', 'tags': ""['LLM', 'Llamaindex', 'Deployment', 'Rag', 'AI']""}, {'title': 'create-llama, a command line tool to generate LlamaIndex apps', 'link': 'https://www.llamaindex.ai/blog/create-llama-a-command-line-tool-to-generate-llamaindex-apps-8f7683021191', 'date': 'Nov 14, 2023', 'tags': ""['Llamaindex', 'LLM', 'AI', 'JavaScript', 'Typescript']""}]",True
74,What dual vector searches are key for image-text integration in a multimodal RAG?,"[' of code is copied directly from the LlamaIndex multimodal cookbook. We begin by defining a multimodal LLM and the prompt template and then combine everything as a query engine.\nopenai_mm_llm = OpenAIMultiModal(\n    model=""gpt-4-vision-preview"", max_new_tokens=1500\n)\n\nqa_tmpl_str = (\n    ""Context information is below.\\n""\n    ""---------------------\\n""\n    ""{context_str}\\n""\n    ""---------------------\\n""\n    ""Given the context information and not prior knowledge, ""\n    ""answer the query.\\n""\n    ""Query: {query_str}\\n""\n    ""Answer: ""\n)\nqa_tmpl = PromptTemplate(qa_tmpl_str)\n\nquery_engine = index.as_query_engine(\n    multi_modal_llm=openai_mm_llm, text_qa_template=qa_tmpl\n)\nNow we can go ahead and test how well it performs.\nquery_str = ""How do vector RAG application work?""\nresponse = query_engine.query(query_str)\nprint(response)\nResponse\nGenerated response by an LLM.\nWe can also visualize which images the retrieval fetched and were used to help inform the final answer.\nImage input to LLM.\nThe LLM got two identical images as input, which just shows that I reuse some of my diagrams. However, I am pleasantly surprised by CLIP embeddings as they were able to retrieve he most relevant image out of the collection. In a more production setting, you might want to clean and deduplicate images, but that is beyond the scope of this article.\n## Conclusion\nLLMs are evolving faster than what we are historically used to and are spanning across multiple modalities. I firmly believe that by the end of the next year, LLMs will be soon able to comprehend videos, and be therefore able to pick up non-verbal cues while talking to you. On the other hand, we can use images as input to RAG pipeline and enhance the variety of information passed to an LLM, making responses better and more accurate. The multimodal RAG pipelines implementation with LlamaIndex and Neo4j is as easy as it gets.\nThe code is available on GitHub.\n']",The answer to given question is not present in context,multi_context,"[{'title': 'Multimodal RAG pipeline with LlamaIndex and Neo4j', 'link': 'https://www.llamaindex.ai/blog/multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206', 'date': 'Dec 18, 2023', 'tags': ""['Neo4j', 'LLM', 'Llamaindex', 'ChatGPT', 'Graph Database']""}]",True
75,How do LLMs integrate SQL data with document summaries for better info retrieval?,"['# Summary\nIn this article, we showcase a powerful new query engine ( SQLAutoVectorQueryEngine ) in LlamaIndex that can leverage both a SQL database as well as a vector store to fulfill complex natural language queries over a combination of structured and unstructured data. This query engine can leverage the expressivity of SQL over structured data, and join it with unstructured context from a vector database. We showcase this query engine on a few examples and show that it can handle queries that make use of both structured/unstructured data, or either.\nCheck out the full guide here: https://gpt-index.readthedocs.io/en/latest/examples/query_engine/SQLAutoVectorQueryEngine.html.\n# Context\nData lakes in enterprises typically encompass both structured and unstructured data. Structured data is typically stored in a tabular format in SQL databases, organized into tables with predefined schemas and relationships between entities. On the other hand, unstructured data found in data lakes lacks a predefined structure and does not fit neatly into traditional databases. This type of data includes text documents, but also other multimodal formats such as audio recordings, videos, and more.\nLarge Language Models (LLMs) have the ability to extract insights from both structured and unstructured data. There have been some initial tooling and stacks that have emerged for tackling both types of data:\n- Text-to-SQL (Structured data): Given a collection of tabular schemas, we convert natural language into a SQL statement which can then be executed against the database.\n- Semantic Search with a Vector Database (Unstructured Data): Store unstructured documents along with their embeddings in a vector database (e.g. Pinecone, Chroma, Milvus, Weaviate, etc.). During query-time, fetch the relevant documents by embedding similarity, and then put into the LLM input prompt to synthesize a response.\n\nEach of these stacks solves particular use cases.\n## Text-to-SQL Over Structured Data\nIn the structured setting, SQL is an extremely expressive language for operating over tabular data ‚Äî in the case of analytics, you can get aggregations, join information across multiple tables, sort by timestamp, and much more. Using the LLM to convert natural language to SQL can be thought as a program synthesis ‚Äúcheat code‚Äù ‚Äî just let the LLM compile to the right SQL query, and let the SQL engine on the database handle the rest!\nUse Case: Text-to-SQL queries are well-suited for analytics use cases where the answer can be found by executing a SQL statement. They are not suited for cases where you‚Äôd need more detail than what is found in a structured table, or if you‚Äôd need more sophisticated ways of determining relevance to the query beyond simple constructs like WHERE conditions.\nExample queries suited for Text-to-SQL:\n- ‚ÄúWhat is the average population of cities in North America‚Äù?\n- ‚ÄúWhat are the largest cities and populations in each respective continent?‚Äù\n\n## Semantic Search over Unstructured Data\nIn the unstructured setting, the behavior for retrieval-augmented generation systems is to first perform retrieval and then synthesis. During retrieval, we first look up the most relevant documents to the query by embedding similarity. Some vector stores support being able to handle additional metadata filters for retrieval. We can choose to manually specify the set of required filters, or have the LLM ‚Äúinfer‚Äù what the query string and metadata filters should be (see our auto-retrieval modules in LlamaIndex or LangChain‚Äôs self-query module).\nUse Case: Retrieval Augmented Generation is well suited for queries where the answer can be obtained within some sections of unstructured text data. Most existing vector stores (e.g. Pinecone, Chroma) do not offer a SQL-like interface; hence they are less suited for queries that involve aggregations, joins, sums, etc.\nExample queries suited for Retrieval Augmented Generation\n- ‚ÄúTell me about the historical museums in Berlin‚Äù\n- ‚ÄúWhat does Jordan ask from Nick on behalf of Gatsby?‚Äù\n\n## Combining These Two Systems\nFor some queries, we may want to make use of knowledge in both structured tables as well as vector databases/document stores in order to give the best answer to the query. Ideally this can give us the best of both worlds: the analytics capabilities over structured data, and semantic understanding over unstructured data.\nHere‚Äôs an example use case:\n- You have access to a collection of articles about different cities, stored in a vector database\n- You also have access to a structured table containing statistics for each city.\n\nGiven this data collection, let‚Äôs take an', 'In this blog post, we introduce a brand new LlamaIndex data structure: a Document Summary Index. We describe how it can help offer better retrieval performance compared to traditional semantic search, and also walk through an example.\n# Background\nOne of the core use cases of Large Language Models (LLMs) is question-answering over your own data. To do this, we pair the LLM with a ‚Äúretrieval‚Äù model that can perform information retrieval over a knowledge corpus, and perform response synthesis over the retrieved texts using the LLM. This overall framework is called Retrieval-Augmented Generation.\nMost users building LLM-powered QA systems today tend to do some form of the following:\n- Take source documents, split each one into text chunks\n- Store text chunks in a vector db\n- During query-time, retrieve text chunks by embedding similarity and/or keyword filters.\n- Perform response synthesis\n\nFor a variety of reasons, this approach provides limited retrieval performance.\n## Limitations of Existing Approaches\nThere are a few limitations of embedding retrieval using text chunks.\n- Text chunks lack global context. Oftentimes the question requires context beyond what is indexed in a specific chunk.\n- Careful tuning of top-k / similarity score thresholds. Make the value too small and you‚Äôll miss context. Make the value too big and cost/latency might increase with more irrelevant context.\n- Embeddings don‚Äôt always select the most relevant context for a question. Embeddings are inherently determined separately between text and the context.\n\nAdding keyword filters are one way to enhance the retrieval results. But that comes with its own set of challenges. We would need to adequately determine the proper keywords for each document, either manually or through an NLP keyword extraction/topic tagging model. Also we would need to adequately infer the proper keywords from the query.\n# Document Summary Index\nA diagram for the Document Summary Index\nWe propose a new index in LlamaIndex that will extract/index an unstructured text summary for each document. This index can help enhance retrieval performance beyond existing retrieval approaches. It helps to index more information than a single text chunk, and carries more semantic meaning than keyword tags. It also allows for a more flexible form of retrieval: we can do both LLM retrieval and embedding-based retrieval.\n## How It Works\nDuring build-time, we ingest each document, and use a LLM to extract a summary from each document. We also split the document up into text chunks (nodes). Both the summary and the nodes are stored within our Document Store abstraction. We maintain a mapping from the summary to the source document/nodes.\nDuring query-time, we retrieve relevant documents to the query based on their summaries, using the following approaches:\n- LLM-based Retrieval: We present sets of document summaries to the LLM, and ask the LLM to determine which documents are relevant + their relevance score.\n- Embedding-based Retrieval: We retrieve relevant documents based on summary embedding similarity (with a top-k cutoff).\n\nNote that this approach of retrieval for document summaries (even with the embedding-based approach) is different than embedding-based retrieval over text chunks. The retrieval classes for the document summary index retrieve all nodes for any selected document, instead of returning relevant chunks at the node-level.\nStoring summaries for a document also enables LLM-based retrieval. Instead of feeding the entire document to the LLM in the beginning, we can first have the LLM inspect the concise document summary to see if it‚Äôs relevant to the query at all. This leverages the reasoning capabilities of LLM‚Äôs which are more advanced than embedding-based lookup, but avoids the cost/latency of feeding the entire document to the LLM\n## Additional Insights\nDocument retrieval with summaries can be thought of as a ‚Äúmiddle ground‚Äù between semantic search and brute-force summarization across all docs. We look up documents based on summary relevance with the given query, and then return all *nodes* corresponding to the retrieved docs.\nWhy should we do this? This retrieval method gives user more context than top-k over a text-chunk, by retrieving context at a document-level. But, it‚Äôs also a more flexible/automatic approach than topic modeling; no more worrying about whether your text has the right keyword tags!\n# Example\nLet‚Äôs walk through an example that showcases the document summary index, over Wikipedia articles about different cities.\nThe rest of this guide showcases the relevant code snippets. You can find the full walkthrough here (and here‚Äôs the notebook link).\nWe can build the GPTDocumentSummaryIndex over a set of documents, and pass in a ResponseSynthesizer object to synthesize']","LLMs integrate SQL data with document summaries by using a new index in LlamaIndex called the Document Summary Index. This index extracts and indexes an unstructured text summary for each document, enhancing retrieval performance beyond existing approaches. During query-time, relevant documents are retrieved based on their summaries using LLM-based retrieval and embedding-based retrieval, allowing for a combination of structured SQL data and unstructured document summaries to provide better answers to queries.",multi_context,"[{'title': 'Combining Text-to-SQL with Semantic Search for Retrieval Augmented Generation', 'link': 'https://www.llamaindex.ai/blog/combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b', 'date': 'May 28, 2023', 'tags': ""['Artificial Intelligence', 'Machine Learning', 'NLP', 'Data Science', 'Database']""}, {'title': 'A New Document Summary Index for LLM-powered QA Systems', 'link': 'https://www.llamaindex.ai/blog/a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec', 'date': 'May 8, 2023', 'tags': ""['Information Retrieval', 'Large Language Models', 'Gpt 4', 'AI', 'Machine Learning']""}]",True
76,What insights arise from varying alpha values on Hit Rate and MRR in single vs. multiple document evaluations?,"['    metric_dicts.append(metric_dict)\n\n    full_df = pd.DataFrame(metric_dicts)\n\n    hit_rate = full_df[""hit_rate""].mean()\n    mrr = full_df[""mrr""].mean()\n    return hit_rate, mrr\n10. Retrieval Evaluation\nHere we do retrieval evaluation on different query types (datasets) and alpha values to understand which alpha will be suitable for which query type. You need to plug in the reranker accordingly to compute the retrieval evaluation with and without the reranker.\n# Asynchronous function to loop over datasets and alpha values and evaluate\nasync def main():\n    results_df = pd.DataFrame(columns=[\'Dataset\', \'Alpha\', \'Hit Rate\', \'MRR\'])\n\n    for dataset in datasets_single_document.keys():\n        for alpha in alpha_values:\n            eval_results = await evaluate_retriever(alpha, datasets_single_document[dataset])\n            hit_rate, mrr = calculate_metrics(eval_results)\n            new_row = pd.DataFrame({\'Dataset\': [dataset], \'Alpha\': [alpha], \'Hit Rate\': [hit_rate], \'MRR\': [mrr]})\n            results_df = pd.concat([results_df, new_row], ignore_index=True)\n\n    # Determine the grid size for subplots\n    num_rows = len(datasets_single_document) // 2 + len(datasets_single_document) % 2\n    num_cols = 2\n\n    # Plotting the results in a grid\n    fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, num_rows * 4), squeeze=False)  # Ensure axes is always 2D\n\n    for i, dataset in enumerate(datasets_single_document):\n        ax = axes[i // num_cols, i % num_cols]\n        dataset_df = results_df[results_df[\'Dataset\'] == dataset]\n        ax.plot(dataset_df[\'Alpha\'], dataset_df[\'Hit Rate\'], marker=\'o\', label=\'Hit Rate\')\n        ax.plot(dataset_df[\'Alpha\'], dataset_df[\'MRR\'], marker=\'o\', linestyle=\'--\', label=\'MRR\')\n        ax.set_xlabel(\'Alpha\')\n        ax.set_ylabel(\'Metric Value\')\n        ax.set_title(f\'{dataset}\')\n        ax.legend()\n        ax.grid(True)\n\n    # If the number of datasets is odd, remove the last (empty) subplot\n    if len(datasets_single_document) % num_cols != 0:\n        fig.delaxes(axes[-1, -1])  # Remove the last subplot if not needed\n\n    # Adjust layout to prevent overlap\n    plt.tight_layout()\n    plt.show()\n\n# Run the main function\nasyncio.run(main())\n# Analyze the results:\nHaving completed the implementation phase, we now turn our attention to analyzing the outcomes. We conducted two sets of experiments: one on a single document and another on multiple documents. These experiments varied in alpha values, types of user queries, and the inclusion or exclusion of a reranker. The accompanying graphs display the results, focusing on the Hit Rate and MRR (Mean Reciprocal Rank) as retrieval evaluation metrics.\nPlease keep in mind that following observations are specific to the datasets used in our study. We encourage you to conduct the experiment with your own documents and draw your relevant observations and conclusions.\n## With Single Document:\nWithout Reranker:\n\nWith Reranker:\n\n## With Multiple Documents:\nWithout Reranker:', 'UPDATE: The pooling method for the Jina AI embeddings has been adjusted to use mean pooling, and the results have been updated accordingly. Notably, the JinaAI-v2-base-en with bge-reranker-largenow exhibits a Hit Rate of 0.938202 and an MRR (Mean Reciprocal Rank) of 0.868539 and withCohereRerank exhibits a Hit Rate of 0.932584, and an MRR of 0.873689.\nWhen building a Retrieval Augmented Generation (RAG) pipeline, one key component is the Retriever. We have a variety of embedding models to choose from, including OpenAI, CohereAI, and open-source sentence transformers. Additionally, there are several rerankers available from CohereAI and sentence transformers.\nBut with all these options, how do we determine the best mix for top-notch retrieval performance? How do we know which embedding model fits our data best? Or which reranker boosts our results the most?\nIn this blog post, we‚Äôll use the Retrieval Evaluation module from LlamaIndex to swiftly determine the best combination of embedding and reranker models. Let\'s dive in!\nLet‚Äôs first start with understanding the metrics available in Retrieval Evaluation\n# Understanding Metrics in Retrieval Evaluation:\nTo gauge the efficacy of our retrieval system, we primarily relied on two widely accepted metrics: Hit Rate and Mean Reciprocal Rank (MRR). Let‚Äôs delve into these metrics to understand their significance and how they operate.\nHit Rate:\nHit rate calculates the fraction of queries where the correct answer is found within the top-k retrieved documents. In simpler terms, it‚Äôs about how often our system gets it right within the top few guesses.\nMean Reciprocal Rank (MRR):\nFor each query, MRR evaluates the system‚Äôs accuracy by looking at the rank of the highest-placed relevant document. Specifically, it‚Äôs the average of the reciprocals of these ranks across all the queries. So, if the first relevant document is the top result, the reciprocal rank is 1; if it‚Äôs second, the reciprocal rank is 1/2, and so on.\nNow that we‚Äôve established the scope and familiarized ourselves with the metrics, it‚Äôs time to dive into the experiment. For a hands-on experience, you can also follow along using our Google Colab Notebook\n# Setting Up the Environment\n!pip install llama-index sentence-transformers cohere anthropic voyageai protobuf pypdf\n# Setting Up the Keys\nopenai_api_key = \'YOUR OPENAI API KEY\'\ncohere_api_key = \'YOUR COHEREAI API KEY\'\nanthropic_api_key = \'YOUR ANTHROPIC API KEY\'\nopenai.api_key = openai_api_key\n# Download the Data\nWe will use Llama2 paper for this experiment. Let‚Äôs download the paper.\n!wget --user-agent ""Mozilla"" ""https://arxiv.org/pdf/2307.09288.pdf"" -O ""llama2.pdf""\n# Load the Data\nLet‚Äôs load the data. We will use Pages from start to 36 for the experiment which excludes table of contents, references, and appendix.\nThis data was then parsed by converted to nodes, which represent chunks of data we‚Äôd like to retrieve. We did use chunk_size as 512.\ndocuments = SimpleDirectoryReader(input_files=[""llama2.pdf""]).load_data()\n\nnode_parser = SimpleNodeParser.from_defaults(chunk_size=512)\nnodes = node_parser.get_nodes_from_documents(documents)\n# Generating Question-Context Pairs:\nFor evaluation purposes, we created a dataset of question-context pairs. This dataset can be seen as a set of questions and their corresponding context from our data. To remove bias for the evaluation of embedding(OpenAI/ CohereAI) and Reranker (CohereAI), we use Anthropic LLM to generate Question-Context Pairs.\nLet‚Äôs initialize a prompt template to generate question-context pairs.\n# Prompt to generate questions\nqa_generate_prompt_tmpl = """"""\\\nContext information is below.\n\n---------------------\n{context_str}\n---------------------\n\nGiven the context information and not prior knowledge.\ngenerate only questions based on the below query.\n\nYou are a Professor. Your task is to setup \\\n{num_questions_per']",The insights from varying alpha values on Hit Rate and MRR in single vs. multiple document evaluations are not explicitly detailed in the context provided.,multi_context,"[{'title': 'LlamaIndex: Enhancing Retrieval Performance with Alpha Tuning in Hybrid Search in RAG', 'link': 'https://www.llamaindex.ai/blog/llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00', 'date': 'Jan 31, 2024', 'tags': ""['Llamaindex', 'LLM', 'OpenAI', 'Retrieval Augmented', 'AI']""}, {'title': 'Boosting RAG: Picking the Best Embedding & Reranker models', 'link': 'https://www.llamaindex.ai/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83', 'date': 'Nov 3, 2023', 'tags': ""['Embedding', 'LLM', 'OpenAI', 'Search', 'Llamaindex']""}]",True
77,"What challenges come with extracting data from PDFs, and how does LayoutPDFReader improve RAG?","['Despite recent motivation to utilize NLP for wider range of real world\n  applications, most NLP papers, tasks and pipelines assume raw, clean texts.\n  However, many texts we encounter in the wild, including a vast majority of\n  legal documents (e.g., contracts and legal codes), are not so clean, with many\n  of them being visually structured documents (VSDs) such as PDFs. PDFs are\n  versatile, preserving the visual integrity of documents, but they often pose a\n  significant challenge when it comes to extracting and manipulating their\n  contents.\nIn this discussion, our focus will primarily be on text-only layered PDFs, a\n  category often regarded by many as a resolved issue.\n# Complexity of Parsing PDFs\n- Layout Complexity: PDFs can contain complex layouts, such\n    as multi-column text, tables, images, and intricate formatting. This layout\n    diversity complicates the extraction of structured data.\n- Font encoding issues: PDFs use a variety of font encoding\n    systems, and some of these systems do not map directly to Unicode. This can\n    make it difficult to extract the text accurately.\n- Non-linear text storage: PDFs do not store text in the\n    order it appears on the page. Instead, they store text in objects that can\n    be placed anywhere on the page. This means that the order of the text in the\n    underlying code may not match the order of the text as it appears visually.\n- Inconsistent use of spaces: In some PDFs, spaces are not\n    used consistently or are not used at all between words. This can make it\n    difficult to even identify word boundaries.\n\n# Do we need an efficient parser?\nIn the Age of LLMs, is an Efficient Parser Still Essential When LLMs Can\n    Process Entire PDFs?\nThis question gains relevance if the answer to this next question is ‚ÄúYes‚Äù.\nDo we need Retrieval-Augmented Generation (RAG)?\nWhile LLMs are powerful, they have certain limitations in terms of the amount\n  of text they can process at once and the scope of information they can\n  reference. Further recent research have suggested LLM performance is often\n  highest when relevant information occurs at the beginning or end of the input\n  context, and significantly degrades when models must access relevant\n  information in the middle of long contexts. Techniques like RAG help overcome\n  these limitations, enabling more effective and efficient processing of large\n  documents and broader information retrieval.\nStill Skeptical? Let‚Äôs ask an LLM for confirmation.\n\nNow that we‚Äôve established the importance of an efficient parser, it becomes\n  instrumental in constructing an effective Retrieval-Augmented Generation (RAG)\n  pipeline to address the limitations of an LLM. Let‚Äôs explore how we are\n  achieving this today. It‚Äôs crucial to remember that the quality of the context\n  fed to an LLM is the cornerstone of an effective RAG, as the saying goes,\n  ‚ÄòGarbage In ‚Äî Garbage Out.‚Äô\nIn the context of building LLM-related applications,\n  chunking is the process of breaking down large pieces of text\n  into smaller segments. It‚Äôs an essential technique that helps optimize the\n  relevance of the content we get back from a database once we use the LLM to\n  embed content. Some of the strategies involved are\n- Fixed-size chunking. This is the most common and\n    straightforward approach to chunking: we simply decide the number of tokens\n    in our chunk and, optionally, whether there should be any overlap between\n    them. Easy to implement & most commonly used, but never makes it to a\n    production setting because the output is satisfactory in a Proof of Concept\n    (POC) setup, but its accuracy degrades as we conduct further testing.\n- ‚ÄúContent-aware‚Äù chunking. Set of methods for taking\n    advantage of the nature of the content we‚Äôre chunking and applying more\n    sophisticated chunking to it. Challenging to implement due to the reasons\n    mentioned above, but if tackled correctly, it could be the most ideal\n    building block for a production-grade Information Retrieval (IR) engine.\n\n# Where‚Äôs This Article Headed, Anyway?\nCertainly, let‚Äôs put an end to the historical and background details, shall\n  we?\nIntroducing\n  LayoutPDFReader for ‚ÄúContext-aware‚Äù chunking.\n  LayoutPDFReader\n ', ' can act as the most important tool in your RAG arsenal by parsing PDFs along\n  with hierarchical layout information such as:\n- Identifying sections and subsections, along with their respective hierarchy\n    levels.\n- Merging lines into coherent paragraphs.\n- Establishing connections between sections and paragraphs.\n- Recognizing tables and associating them with their corresponding sections.\n- Handling lists and nested list structures with precision.\n\nThe first step in using the\n  LayoutPDFReader\n  is to provide a URL or file path to it (assuming it‚Äôs already been installed)\n  and get back a document object.\nfrom llmsherpa.readers import LayoutPDFReaderllmsherpa_api_url = ""https://readers.llmsherpa.com/api/document/developer/parseDocument?renderFormat=all""pdf_url = ""https://arxiv.org/pdf/1910.13461.pdf"" # also allowed is a file path e.g. /home/downloads/xyz.pdfpdf_reader = LayoutPDFReader(llmsherpa_api_url)doc = pdf_reader.read_pdf(pdf_url)\n## Vector search and RAG with Smart Chunking\nLayoutPDFReader employs intelligent chunking to maintain the cohesion of\n  related text:\n- It groups all list items together, along with the preceding paragraph.\n- Items within a table are chunked together.\n- It incorporates contextual information from section headers and nested\n    section headers.\n\nAs a quick example, the following code snippet generates a\n  LlamaIndex\n  query engine from the document chunks produced by LayoutPDFReader.\nfrom llama_index.readers.schema.base import Documentfrom llama_index import VectorStoreIndexindex = VectorStoreIndex([])for chunk in doc.chunks():    index.insert(Document(text=chunk.to_context_text(), extra_info={}))query_engine = index.as_query_engine()# Let\'s run one queryresponse = query_engine.query(""list all the tasks that work with bart"")print(response)\nWe get the following response:\nBART works well for text generation, comprehension tasks, abstractive dialogue, question answering, and summarization tasks.\nKey Considerations:\n- LLMSherpa leverages a cost-free and open API server. Your PDFs are not\n    retained beyond temporary storage during the parsing process.\n- LayoutPDFReader has undergone extensive testing with a diverse range of\n    PDFs. However, achieving flawless parsing for every PDF remains a\n    challenging task.\n- Please note that OCR (Optical Character Recognition) functionality is\n    presently unavailable. The tool exclusively supports PDFs equipped with a\n    text layer.\n- For inquiries regarding private hosting options, OCR support, or tailored\n    assistance with particular PDF-related concerns, feel free to reach out to\n    contact@nlmatics.com\n    or to\n    me\n    directly.\n\nIf you have any questions, please leave them in the comments section, and I\n  will try to respond ASAP.\nConnect?\nIf you want to get in touch, feel free to shoot me a message on\n  LinkedIn or via\n  email.\n# References\nhttps://github.com/nlmatics/llmsherpa\nCapturing Logical Structure of Visually Structured Documents with Multimodal Transition Parser\nLost in the Middle: How Language Models Use Long Contexts\n']","Extracting data from PDFs poses several challenges, including layout complexity (multi-column text, tables, images), font encoding issues, non-linear text storage, and inconsistent use of spaces. LayoutPDFReader improves Retrieval-Augmented Generation (RAG) by parsing PDFs with hierarchical layout information, identifying sections and subsections, merging lines into coherent paragraphs, establishing connections between sections and paragraphs, recognizing tables, and handling lists and nested structures with precision.",multi_context,"[{'title': 'Mastering PDFs: Extracting Sections, Headings, Paragraphs, and Tables with Cutting-Edge Parser', 'link': 'https://www.llamaindex.ai/blog/mastering-pdfs-extracting-sections-headings-paragraphs-and-tables-with-cutting-edge-parser-faea18870125', 'date': 'Oct 18, 2023', 'tags': ""['NLP', 'Large Language Models', 'Llamaindex', 'Pdf', 'Rag']""}, {'title': 'Mastering PDFs: Extracting Sections, Headings, Paragraphs, and Tables with Cutting-Edge Parser', 'link': 'https://www.llamaindex.ai/blog/mastering-pdfs-extracting-sections-headings-paragraphs-and-tables-with-cutting-edge-parser-faea18870125', 'date': 'Oct 18, 2023', 'tags': ""['NLP', 'Large Language Models', 'Llamaindex', 'Pdf', 'Rag']""}]",True
78,How does LlamaParse improve Knowledge Graph accuracy for financial data and drug interactions?,"[""Greetings, LlamaIndex enthusiasts! ü¶ô\nWelcome to another exciting weekly update from the world of LlamaVerse!\nWe have an amazing news for you from LlamaIndex. We've officially launched LlamaParse, a GenAI-native document parsing solution. With state-of-the-art table and chart extraction, natural language steerable instructions, and compatibility with over a dozen document types, LlamaParse excels in creating accurate RAG applications from complex documents. After a successful private preview with 2k users and 1M pages parsed, it's now ready to transform your document handling. Check out our launch post for all the details!\nü§©\xa0The highlights:\n- New observability with Instrumentation: Enhanced developer workflow with a new Instrumentation module for improved observability. Docs, Tweet.\n- LlamaParse accepts natural language parsing instructions: Easily extract math snippets from PDFs into LaTeX with LlamaParse. Blogpost, Tweet.\n- Financial Data Parsing: Transform PowerPoint parsing, utilizing LlamaParse to extract and interpret complex financial data from .pptx files, enabling detailed and accurate financial analysis. Notebook, Tweet.\n\n‚ú® Feature Releases and Enhancements:\n- We introduced LlamaIndex v0.10.20, featuring our new Instrumentation module, a leap in observability that simplifies developer workflows by providing a module-level dispatcher, reducing the need for individual callback managers and facilitating comprehensive handler sets across your application. Docs, Tweet.\n- We have launched parsing by prompting feature in LlamaParse to properly extract out any math snippets from PDFs into LaTex which helps you to plug easily into your RAG pipeline. Blogpost, Tweet.\n- We have launched an advanced RAG pipeline for Financial PowerPoints, using LlamaParse to tackle the challenge of parsing .pptx files. Our solution accurately extracts slides, including text, tables, and charts, enabling precise question-answering over complex financial data. Notebook, Tweet.\n- We collaborated with langfuse to launch open-source observability for your RAG pipeline, enhancing your application with integrated tracing, prompt management, and evaluation in just two lines of code. Blogpost, Docs, Tweet.\n- Search-in-the-Chain: a method by Shicheng Xu et al., is now integrated into LlamaIndex, enhancing question-answering with an advanced system that interleaves retrieval and planning. This approach verifies each reasoning step in a chain, allowing for dynamic replanning and application in various agent reasoning contexts. LlamaPack, Tweet\n\nüé•\xa0Demos:\n- Home AI, a tool created with create-llama, to help home searches by using LLMs to automate the parsing of complex property disclosures, enabling users to filter searches with unprecedented detail and efficiency. Blogpost, Code, Tweet.\n\nüó∫Ô∏è Guides:\n- Guide to using LlamaIndex and Mathpix to parse, index, and query complex mathematics within scientific papers, detailing steps from parsing tables and extracting images to indexing in a RAG app and answering questions with precise LaTeX outputs, to showcase hierarchical retrieval technique.\n\n‚úçÔ∏è Tutorials:\n- Thomas Reid‚Äôs tutorial on using LlamaParse can help properly extract text from a Tesla quarterly filings.\n- Sudarshan Koirala video tutorial on RAG with LlamaParse, Qdrant, and Groq.\n- Kyosuke Morita tutorial showing how to match a candidate to jobs based on their CV with LlamaParse + LlamaIndex.\n- Cobus Greyling tutorial on Agentic RAG: Context-Augmented OpenAI Agents.\n- Roey Ben Chaim‚Äôs tutorial on PII Detector: hacking privacy in RAG.\n\nüé•\xa0Webinars:\n- Webinar with Charles Packer, lead author of MemGPT on Long-Term, Self-Editing Memory with MemGPT\n\nüìÖ\xa0Events:\n- We are hosting a RAG meetup in Paris on March 27th featuring talks on advanced RAG strategies, building a RAG CLI, and the significance of open-source RAG in business.\n\n""]",The answer to given question is not present in context,multi_context,"[{'title': 'LlamaIndex Newsletter 2024-03-19', 'link': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-03-19', 'date': 'Mar 19, 2024', 'tags': ""['LlamaParse', 'AI', 'LLM', 'Newsletter']""}]",True
79,What's the dual-stage method for improving document retrieval with LLM and embeddings?,"[' be preferred if the document itself corresponds to a long-piece of text. We do this under the hood with our new document summary index, but you can also choose to do it yourself.\n\nGiven a collection of documents, we can then create document ‚Äúbatches‚Äù and send each batch into the LLM input prompt. The output of each batch would be the set of relevant documents + relevance scores within that batch. The final retrieval response would aggregate relevant documents from all batches.\nYou can use our abstractions in two forms: as a standalone retriever module (ListIndexLLMRetriever) or a reranker module (LLMRerank). The remainder of this blog primarily focuses on the reranker module given the speed/cost.\n## LLM Retriever(ListIndexLLMRetriever)\nThis module is defined over a list index, which simply stores a set of nodes as a flat list. You can build the list index over a set of documents and then use the LLM retriever to retrieve the relevant documents from the index.\nfrom llama_index import GPTListIndex\nfrom llama_index.indices.list.retrievers import ListIndexLLMRetriever\nindex = GPTListIndex.from_documents(documents, service_context=service_context)\n# high - level API\nquery_str = ""What did the author do during his time in college?""\nretriever = index.as_retriever(retriever_mode=""llm"")\nnodes = retriever.retrieve(query_str)\n# lower-level API\nretriever = ListIndexLLMRetriever()\nresponse_synthesizer = ResponseSynthesizer.from_args()\nquery_engine = RetrieverQueryEngine(retriever=retriever, response_synthesizer=response_synthesizer)\nresponse = query_engine.query(query_str)\nUse Case: This could potentially be used in place of our vector store index. You use the LLM instead of embedding-based lookup to select the nodes.\n## LLM Reranker (LLMRerank)\nThis module is defined as part of our NodePostprocessor abstraction, which is defined for second-stage processing after an initial retrieval pass.\nThe postprocessor can be used on its own or as part of a RetrieverQueryEngine call. In the below example we show how to use the postprocessor as an independent module after an initial retriever call from a vector index.\nfrom llama_index.indices.query.schema import QueryBundle\nquery_bundle = QueryBundle(query_str)\n# configure retriever\nretriever = VectorIndexRetriever(\nindex=index,\nsimilarity_top_k=vector_top_k,\n)\nretrieved_nodes = retriever.retrieve(query_bundle)\n# configure reranker\nreranker = LLMRerank(choice_batch_size=5, top_n=reranker_top_n, service_context=service_context)\nretrieved_nodes = reranker.postprocess_nodes(retrieved_nodes, query_bundle)\n## Limitations/Caveats\nThere are certain limitations and caveats to LLM-based retrieval, especially with this initial version.\n- LLM-based retrieval is orders of magnitude slower than embedding-based retrieval. Embedding search over thousands or even millions of embeddings can take less than a second. Each LLM prompt of 4000 tokens to OpenAI can take minutes to complete.\n- Using third-party LLM API‚Äôs costs money.\n- The current method of batching documents may not be optimal, because it relies on an assumption that document batches can be scored independently of each other. This lacks a global view of the ranking for all documents.\n\nUsing the LLM to retrieve and rank every node in the document corpus can be prohibitively expensive. This is why using the LLM as a second-stage reranking step, after a first-stage embedding pass, can be helpful.\n# Initial Experimental Results\nLet‚Äôs take a look at how well LLM reranking works!\nWe show some comparisons between naive top-k embedding-based retrieval as well as the two-stage retrieval pipeline with a first-stage embedding-retrieval filter and second-stage LLM reranking. We also showcase some results of pure LLM-based retrieval (though we don‚Äôt showcase as many results given that it tends to run a lot slower than either of the first two approaches).\nWe analyze results over two very different sources of data: the Great Gatsby and', ""# Summary\nThis blog post outlines some of the core abstractions we have created in LlamaIndex around LLM-powered retrieval and reranking, which helps to create enhancements to document retrieval beyond naive top-k embedding-based lookup.\nLLM-powered retrieval can return more relevant documents than embedding-based retrieval, with the tradeoff being much higher latency and cost. We show how using embedding-based retrieval as a first-stage pass, and second-stage retrieval as a reranking step can help provide a happy medium. We provide results over the Great Gatsby and the Lyft SEC 10-k.\nTwo-stage retrieval pipeline: 1) Top-k embedding retrieval, then 2) LLM-based reranking\n# Introduction and Background\nThere has been a wave of ‚ÄúBuild a chatbot over your data‚Äù applications in the past few months, made possible with frameworks like LlamaIndex and LangChain. A lot of these applications use a standard stack for retrieval augmented generation (RAG):\n- Use a vector store to store unstructured documents (knowledge corpus)\n- Given a query, use a retrieval model to retrieve relevant documents from the corpus, and a synthesis model to generate a response.\n- The retrieval model fetches the top-k documents by embedding similarity to the query.\n\nIn this stack, the retrieval model is not a novel idea; the concept of top-k embedding-based semantic search has been around for at least a decade, and doesn‚Äôt involve the LLM at all.\nThere are a lot of benefits to embedding-based retrieval:\n- It‚Äôs very fast to compute dot products. Doesn‚Äôt require any model calls during query-time.\n- Even if not perfect, embeddings can encode the semantics of the document and query reasonably well. There‚Äôs a class of queries where embedding-based retrieval returns very relevant results.\n\nYet for a variety of reasons, embedding-based retrieval can be imprecise and return irrelevant context to the query, which in turn degrades the quality of the overall RAG system, regardless of the quality of the LLM.\nThis is also not a new problem: one approach to resolve this in existing IR and recommendation systems is to create a two stage process. The first stage uses embedding-based retrieval with a high top-k value to maximize recall while accepting a lower precision. Then the second stage uses a slightly more computationally expensive process that is higher precision and lower recall (for instance with BM25) to ‚Äúrerank‚Äù the existing retrieved candidates.\nCovering the downsides of embedding-based retrieval is worth an entire series of blog posts. This blog post is an initial exploration of an alternative retrieval method and how it can (potentially) augment embedding-based retrieval methods.\n# LLM Retrieval and Reranking\nOver the past week, we‚Äôve developed a variety of initial abstractions around the concept of ‚ÄúLLM-based‚Äù retrieval and reranking. At a high-level, this approach uses the LLM to decide which document(s) / text chunk(s) are relevant to the given query. The input prompt would consist of a set of candidate documents, and the LLM is tasked with selecting the relevant set of documents as well as scoring their relevance with an internal metric.\nSimple diagram of how LLM-based retrieval works\nAn example prompt would look like the following:\nA list of documents is shown below. Each document has a number next to it along with a summary of the document. A question is also provided.\n  Respond with the numbers of the documents you should consult to answer the question, in order of relevance, as well\n  as the relevance score. The relevance score is a number from 1‚Äì10 based on how relevant you think the document is to the question.\n  Do not include any documents that are not relevant to the question.\n  Example format:\n  Document 1:\n  <summary of document 1>\n  Document 2:\n  <summary of document 2>\n  ‚Ä¶\n  Document 10:\n  <summary of document 10>\n  Question: <question>\n  Answer:\n  Doc: 9, Relevance: 7\n  Doc: 3, Relevance: 4\n  Doc: 7, Relevance: 3\n  Let's try this now:\n  {context_str}\n  Question: {query_str}\n  Answer:\nThe prompt format implies that the text for each document should be relatively concise. There are two ways of feeding in the text to the prompt corresponding to each document:\n- You can directly feed in the raw text corresponding to the document. This works well if the document corresponds to a bite-sized text chunk.\n- You can feed in a condensed summary for each document. This would""]","The dual-stage method for improving document retrieval involves two main steps: 1) Top-k embedding retrieval, which uses embedding-based methods to retrieve a broad set of documents, maximizing recall but accepting lower precision; and 2) LLM-based reranking, which refines the results by using a more computationally expensive process to score and rank the retrieved documents based on their relevance to the query.",multi_context,"[{'title': 'Using LLM‚Äôs for Retrieval and Reranking', 'link': 'https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6', 'date': 'May 17, 2023', 'tags': ""['Artificial Intelligence', 'Machine Learning', 'Large Language Models', 'Llamaindex', 'NLP']""}, {'title': 'Using LLM‚Äôs for Retrieval and Reranking', 'link': 'https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6', 'date': 'May 17, 2023', 'tags': ""['Artificial Intelligence', 'Machine Learning', 'Large Language Models', 'Llamaindex', 'NLP']""}]",True
80,What strategies boost LLMs in multi-hop QA regarding context length and retrieval?,"['A: Aimed at question-answering over entire books and movie scripts, NarrativeQA provides question-answer pairs created from summaries of these extensive sources.\n- QuALITY: A multiple-choice question answering set based on stories and articles, QuALITY emphasizes thorough reading, with half the questions designed to be challenging and require careful consideration.\n- MuSiQue: Designed for multi-hop reasoning in question answering, MuSiQue creates multi-hop questions from single-hop ones, emphasizing connected reasoning and minimizing shortcuts.\n- HotpotQA: Based on Wikipedia, HotpotQA requires reading multiple supporting documents for reasoning. It features diverse questions and provides sentence-level support for answers.\n- MultiFieldQA-en: Curated to test long-context understanding across fields, MFQA uses sources like legal documents and academic papers, with annotations done by Ph.D. students.\n\n## Evaluation Metrics:\nThe research team used a wide range of metrics suited to each dataset. The geometric mean of ROUGE scores for QM, the exact matching (EM) score for QLTY, and F1 scores for others were the primary metrics.\n# Results\n- Baseline models without retrieval, having a 4K sequence length, performed poorly since valuable texts get truncated.\n- With retrieval, performance for 4K models like LLaMA2‚Äì70B-4K and GPT-43B-4K significantly improved.\n- HotpotQA, a multi-hop dataset, particularly benefits from longer sequence models.\n- Models with longer contexts (16K, 32K) outperform their 4K counterparts even when fed the same evidence chunks.\n- There exists a unique ‚ÄúU-shaped‚Äù performance curve for LLMs due to the lost in the middle phenomenon, making them better at utilizing information at the beginning or end of the input.\n- The study presents a contrasting perspective to LongBench‚Äôs findings, emphasizing that retrieval is beneficial for models regardless of their context window size.\n\n## Comparing to OpenAI Models:\n- The LLaMA2‚Äì70B-32k model with retrieval surpasses the performance of GPT-3.5-turbo variants and is competitive with Davinci-003, underscoring its robustness in handling long context tasks.\n\n## Comparison of Different Retrievers:\n- Retrieval consistently enhances the performance across different retrievers.\n- Public retrievers outperformed proprietary ones like OpenAI embeddings.\n\n## Comparing with the number of retrieved chunks:\n- The best performance is achieved by retrieving the top 5 or 10 chunks. Retrieving more, up to 20 chunks, doesn‚Äôt offer additional benefits and can even degrade performance.\n- The deterioration in performance when adding more chunks could be due to the lost-in-the-middle phenomenon or the model being sidetracked by non-relevant information.\n\n# Conclusion\nAs we delved deep into understanding how retrieval augmentation and long-context extension interact when applied to leading language models fine-tuned for long-context question-answering and summarization tasks. Here are some things to be noted:\n- Boost in Performance with Retrieval: Implementing retrieval techniques significantly enhances the performance of both shorter 4K context language models and their longer 16K/32K context counterparts.\n- Efficiency of 4K Models with Retrieval: 4K context language models, when combined with retrieval augmentation, can achieve performance levels similar to 16K long context models. Plus, they have the added advantage of being faster during the inference process.\n- Best Model Performance: After enhancing with both context window extension and retrieval augmentation, the standout model, LLaMA2‚Äì70B-32k-ret (LLaMA2‚Äì70B-32k with retrieval), surpasses well-known models like GPT-3.5-turbo-16k and davinci-003.\n\n# References:\n- Retrieval meets long context, large language models.\n- Longbench: A bilingual, multitask benchmark for long context understanding.\n\nWe trust that this blog post on the review of the paper on retrieval augmentation with long-context LLMs has furnished you with meaningful insights. We‚Äôre keen to hear if your experiments align with our findings or present new perspectives ‚Äî divergent results always make for interesting discussions and further exploration.\n', '# Introduction\n## Why Long Context Matters and How Retrieval Augmentation Steps In:\nIn the dynamic landscape of LLMs, two methods have gained traction and seem to be taking center stage: expanding the context window of Large Language Models (LLMs) and enhancing these models with retrieval capabilities. The continued evolution of GPU technology, coupled with breakthroughs in attention mechanisms, has given rise to long-context LLMs. Simultaneously, the concept of retrieval ‚Äî where LLMs pick up only the most relevant context from a standalone retriever ‚Äî promises a revolution in efficiency and speed.\nIn the midst of these evolving narratives, some interesting questions emerge:\n- Retrieval-augmentation versus long context window, which one is better for downstream tasks?\n- Can both methods be combined to get the best of both worlds?\n\nTo dissect these questions, in this blog post we turn to NVIDIA‚Äôs recent study, which harnesses the power of two powerful LLMs: the proprietary GPT ‚Äî 43B and LLaMA2‚Äì70B, the research strives to provide actionable insights for AI practitioners.\n## Prior Research and the NVIDIA Divergence:\nInterestingly, while NVIDIA‚Äôs findings are interesting in many respects, Another recent work by Bai et al. (2023) also ventured into similar territory, although with differing outcomes.\nTheir work explored the impact of retrieval on long context LLMs, evaluating models like GPT-3.5-Turbo-16k and Llama2‚Äì7B-chat-4k. However, their findings diverge from NVIDIA‚Äôs in crucial ways. Bai et al. discerned that retrieval was beneficial only for the Llama2‚Äì7B-chat-4k with a 4K context window, but not for extended context models like GPT-3.5-Turbo-16k. One hypothesis for this difference centers on the challenges tied to experiments using black-box APIs and the smaller white-box LLMs they employed, which potentially had limited capability to integrate context through retrieval.\nNVIDIA‚Äôs work distinguishes itself by tapping into much larger LLMs, yielding results that not only match top-tier models like ChatGPT-3.5 but even indicate further enhancements when incorporating retrieval methods.\n# Models, Datasets, and Evaluation Metrics\n## Large Language Models (LLMs) Explored:\nThe researchers delved deep into the potential of large language models for tasks like generative QA and summarization. Specifically, two models were the primary focus:\n- Nemo GPT-43B: A proprietary 43 billion parameter model trained on 1.1T tokens, 70% of which were in English. This model was fed a rich diet of web archives, Wikipedia, Reddit, books, and more. It contains 48 layers and is trained using RoPE embeddings.\n- LLaMA2‚Äì70B: A publicly available 70B parameter model trained on 2T tokens, primarily in English. It‚Äôs structured with 80 layers and also utilizes RoPE embeddings.\n\n## Context Window Extension:\nTo enhance the models‚Äô capability to process longer contexts, their initial 4K context window length was augmented. The GPT-43B was modified to handle 16K, while the LLaMA2‚Äì70B was expanded to both 16K and 32K, employing the position interpolation method.\n## Instruction Tuning:\nTo optimize the LLMs for the tasks at hand, instruction tuning was implemented. A diverse dataset blend, comprising sources like Soda, ELI5, FLAN, and others, was created. A consistent format template was adopted for multi-turn dialogue training, and the models were meticulously fine-tuned to accentuate the answer segment.\n## Retrieval Models Tested:\nThree retrieval systems were put to the test:\n- Dragon: A state-of-the-art dual encoder model for both supervised and zero-shot information retrieval.\n- Contriever: Utilizes a basic contrastive learning framework and operates unsupervised.\n- OpenAI embedding: The latest version was used, accepting a maximum input of 8,191 tokens.\n\nThe retrieval approach entailed segmenting each document into 300-word sections, encoding both questions and these chunks, and then merging the most pertinent chunks for response generation.\n## Datasets Used for Evaluation:\nThe study employed seven diverse datasets, sourced from the Scroll benchmark and LongBench.\nA snapshot of these datasets includes:\n- QMSum: A query-based summarization dataset, QMSum consists of transcripts from diverse meetings and their corresponding summaries, built upon contextual queries.\n- Qasper: A question-answering dataset centered on NLP papers, Qasper offers a mix of abstractive, extractive, yes/no, and unanswerable questions from the Semantic Scholar Open Research Corpus.\n- NarrativeQ']","Strategies that boost LLMs in multi-hop QA include implementing retrieval techniques, which significantly enhance performance, especially for longer context models (16K, 32K). Additionally, extending the context window of LLMs allows them to process more information, leading to improved outcomes in multi-hop reasoning tasks.",multi_context,"[{'title': 'NVIDIA Research: RAG with Long Context LLMs', 'link': 'https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4', 'date': 'Oct 22, 2023', 'tags': ""['Llamaindex', 'LLM', 'Nvidia', 'Research Paper', 'Llama 2']""}, {'title': 'NVIDIA Research: RAG with Long Context LLMs', 'link': 'https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4', 'date': 'Oct 22, 2023', 'tags': ""['Llamaindex', 'LLM', 'Nvidia', 'Research Paper', 'Llama 2']""}]",True
81,What‚Äôs the main goal of LLM Query Pipelines for improving user experience and query workflows?,"[' Pipelines guide for full details. We go over all the steps above with concrete examples!\nThe notebook guide also logs traces through Arize Phoenix. You can see the full run of each QueryPipeline in the Phoenix dashboard. Our full callback support throughout every component in a QueryComponent allows you to easily integrate with any observability provider.\n\n# Related Work\nThe idea of a declarative syntax for building LLM-powered pipelines is not new. Related works include Haystack as well as the LangChain Expression Language. Other related works include pipelines that are setup in the no-code/low-code setting such as Langflow / Flowise.\nOur main goal here was highlighted above: provide a convenient dev UX to define common query workflows over your data. There‚Äôs a lot of optimizations/guides to be done here!\n# FAQ\nWhat‚Äôs the difference between a QueryPipeline and IngestionPipeline ?\nGreat question. Currently the IngestionPipeline operates during the data ingestion stage, and the QueryPipeline operates during the query stage. That said, there‚Äôs potentially some shared abstractions we‚Äôll develop for both!\n# Conclusion + Resources\nThat‚Äôs it! As mentioned above we‚Äôll be adding a lot more resources and guides soon. In the meantime check out our current guides:\n- Query Pipelines Guide\n- Query Pipelines Walkthrough\n- Query Pipeline Usage Pattern\n- Query Pipelines Module Usage Guide\n\n', 'Today we introduce Query Pipelines, a new declarative API within LlamaIndex that allows you to concisely orchestrate simple-to-advanced query workflows over your data for different use cases (RAG, structured data extraction, and more).\nAt the core of all this is ourQueryPipeline abstraction. It can take in many LlamaIndex modules (LLMs, prompts, query engines, retrievers, itself). It can create a computational graph over these modules (e.g. a sequential chain or a DAG). It has callback support and native support with our observability partners.\nThe end goal is that it‚Äôs even easier to build LLM workflows over your data. Check out our comprehensive introduction guide, as well as our docs page for more details.\nExample `QueryPipeline` setup for an advanced RAG pipeline\n# Context\nOver the past year AI engineers have developed customized, complex orchestration flows with LLMs to solve a variety of different use cases. Over time some common patterns developed. At a top-level, paradigms emerged to query a user‚Äôs data ‚Äî this includes RAG (in a narrow definition) to query unstructured data, and text-to-SQL to query structured data. Other paradigms emerged around use cases like structured data extraction (e.g. prompt the LLM to output JSON, and parse it), prompt chaining (e.g. chain-of-thought), and agents that could interact with external services (combine prompt chaining\nThere is a lot of query orchestration in RAG. Even within RAG itself there can be a lot of work to build an advanced RAG pipeline optimized for performance. Starting from the user query, we may want to run query understanding/transformations (re-writing, routing). We also may want to run multi-stage retrieval algorithms ‚Äî e.g. top-k lookup + reranking. We may also want to use prompts + LLMs to do response synthesis in different ways. Here‚Äôs a great blog on advanced RAG components.\nSource: ‚ÄúAdvanced RAG Techniques: an Illustrated Overview‚Äù by Ivan Ilin\nRAG has become more modular: Instead of a single way to do retrieval/RAG, developers are encouraged to pick and choose the best modules for their use cases. This sentiment is echoed in the RAG Survey paper by Gao et al.\nThis leads to creative new patterns like DSP, Rewrite-Retrieve-Read, or interleaving retrieval+generation multiple times.\n## Previous State of LlamaIndex\nLlamaIndex itself has hundreds of RAG guides and 16+ Llama Pack recipes letting users setup different RAG pipelines, and has been at the forefront of establishing advanced RAG patterns.\nWe‚Äôve also exposed low-level modules such as LLMs, prompts, embeddings, postprocessors and easy subclassability of core components like retrievers and query engines so that users can define their own workflows.\nBut up until now, we didn‚Äôt explicitly have an orchestration abstraction. Users were responsible for figuring out their own workflows by reading the API guides of each module, converting outputs to the right inputs, and using the modules imperatively.\n# Query Pipeline\nAs a result, our QueryPipeline provides a declarative query orchestration abstraction. You can use it to compose both sequential chains and directed acyclic graphs (DAGs) of arbitrary complexity.\nYou can already compose these workflows imperatively with LlamaIndex modules, but the QueryPipeline allows you to do it efficiently with fewer lines of code.\nIt has the following benefits:\n- Express common query workflows with fewer lines of code/boilerplate: Stop writing converter logic between outputs/inputs, and figuring out the exact typing of arguments for each module!\n- Greater readability: Reduced boilerplate leads to greater readability.\n- End-to-end observability: Get callback integration across the entire pipeline (even for arbitrarily nested DAGs), so you stop fiddling around with our observability integrations.\n- [In the future] Easy Serializability: A declarative interface allows the core components to be serialized/redeployed on other systems much more easily.\n- [In the future] Caching: This interface also allows us to build a caching layer under the hood, allowing input re-use.\n\nVisualization of our advanced RAG QueryPipeline using `networkx` and `pyvis`\n# Usage\nThe QueryPipeline allows you to a DAG-based query workflow using LlamaIndex modules. There are two main ways to use it:\n- As a sequential chain (easiest/most concise)\n- As a full DAG (more expressive)\n\nSee our usage pattern guide for more details.\n## Sequential Chain']","The main goal of LLM Query Pipelines is to provide a convenient developer user experience (UX) to define common query workflows over data, making it easier to build LLM workflows.",multi_context,"[{'title': 'Introducing Query Pipelines', 'link': 'https://www.llamaindex.ai/blog/introducing-query-pipelines-025dc2bb0537', 'date': 'Jan 8, 2024', 'tags': ""['Llamaindex', 'Retrieval Augmented', 'LLM', 'AI']""}, {'title': 'Introducing Query Pipelines', 'link': 'https://www.llamaindex.ai/blog/introducing-query-pipelines-025dc2bb0537', 'date': 'Jan 8, 2024', 'tags': ""['Llamaindex', 'Retrieval Augmented', 'LLM', 'AI']""}]",True
82,What role does GPT-3.5 play in turning user queries into SQL for Na2SQL with LlamaIndex's SQLTableQueryEngine?,"['In the dynamic world of AI and data analytics, the ability to bridge the gap between complex data queries and non-technical users is a game-changer. My latest project, Na2SQL, showcases this exciting advancement. Leveraging the power of LlamaIndex and OpenAI‚Äôs GPT-3.5, this app allows users, regardless of their SQL knowledge, to derive valuable insights from a database using simple natural language.\n# Features\n- Intuitive Natural Language Queries: The core of this application is its ability to understand and process natural language queries. Users can ask questions in plain English and receive SQL queries and insights in return, all without any prior SQL experience.\n- Advanced Data Processing: The app doesn‚Äôt just stop at generating SQL queries; it executes these queries and analyzes the results to provide meaningful insights, making it a powerful tool for data analysis.\n- User-Friendly Interface with Streamlit: I chose Streamlit for its simplicity and effectiveness in creating interactive web applications. The app‚Äôs interface is straightforward, ensuring a smooth user experience.\n- Database Viewer: An interactive database viewer in the sidebar on the left allows users to explore the database structure, enhancing their understanding and interaction with the data.\n\n\n# The Tech Stack\nThis project harmoniously integrates several advanced technologies:\n- OpenAI‚Äôs GPT-3.5: At the heart of the application is GPT-3.5, enabling the app to understand natural natural language queries and transform them into valid SQL queries. Furthermore, it also generates the final analysis considering both the user‚Äôs query and the SQL output, thereby providing a comprehensive and relevant response.\n- LlamaIndex: A pivotal component of the app is LlamaIndex‚Äôs SQLTableQueryEngine. This powerful tool translates natural language queries into SQL, handles the execution of these queries, and plays a significant role in the subsequent analysis using GPT 3.5. Its integration ensures a smooth transition from user inputs to database insights, culminating in a meaningful final analysis that encapsulates the entire natural language-to-SQL-to-execution process.\n- LlamaIndex‚Äôs Streamlit LlamaPack: Using LlamaIndex‚Äôs Streamlit LlamaPack, we quickly assemble and highly functional Streamlit UI. This framework significantly simplifies the UI development process, allowing for rapid deployment and an enhanced user experience.\n- SQLite Database: The app interacts with an dummy SQLite ecommerce database, showcasing its ability to work with real-world data.\n\n# Deep Dive into the Code\nIn the heart of the application lies app.py, a script that brings to life the seamless interaction between natural language processing and SQL query generation.\nThis code is an evolution of the Streamlit chatbot LlamaPack available on Llama Hub, further tailored to meet the specific needs of ecommerce data analytics. Let\'s dive into some key portions of the app.py script:\n## 1. Initial Imports and Setup\nThe script begins by importing necessary modules such as Streamlit, SQLAlchemy for database interaction, LlamaIndex for language model services, and other essential libraries.\nimport streamlit as st\nfrom sqlalchemy import create_engine, inspect\nfrom typing import Dict, Any\n\nfrom llama_index import (\n    VectorStoreIndex,\n    ServiceContext,\n    download_loader,\n)\nfrom llama_index.llama_pack.base import BaseLlamaPack\nfrom llama_index.llms import OpenAI\nimport openai\nimport os\nimport pandas as pd\n## 2. StreamlitChatPack Class\nThe StreamlitChatPack class extends the base LlamaPack, setting up the page and modules necessary for the app\'s functionality.\nclass StreamlitChatPack(BaseLlamaPack):\n\n    def __init__(\n        self,\n        page: str = ""Natural Language to SQL Query"",\n        run_from_main: bool = False,\n        **kwargs: Any,\n    ) -&gt; None:\n        """"""Init params.""""""\n        self.page = page\n\n    # ... other methods ...\n## 3. The run Method\nThis method is where the magic happens. It sets up the Streamlit page configuration and initializes the chat functionality.\ndef run(self, *args: Any, **kwargs: Any) -&gt; Any:\n    """"""Run the pipeline.""""""\n    import streamlit as st\n\n    st.set_page_config(\n        page_title=f""{self']","GPT-3.5 plays a crucial role in the Na2SQL application by enabling it to understand natural language queries and transform them into valid SQL queries. It also generates the final analysis based on the user's query and the SQL output, providing a comprehensive and relevant response.",multi_context,"[{'title': 'Transforming Natural Language to SQL and Insights for E-commerce with LlamaIndex, GPT3.5, and Streamlit', 'link': 'https://www.llamaindex.ai/blog/transforming-natural-language-to-sql-and-insights-for-e-commerce-with-llamaindex-gpt3-5-e08edefa21f9', 'date': 'Dec 17, 2023', 'tags': ""['Natural Language To Sql', 'Llamaindex', 'LLM', 'Gpt35', 'OpenAI']""}]",True
83,How does sandboxing ensure safe code execution with LLMs like OpenLLM and LlamaIndex?,"['One of the many amazing feats that LLMs are capable of is generating executable code. This can be used to solve a variety of complex problems that require calculations and fixed logic that traditional computing excels at but LLMs can struggle to perform directly. When building agents to perform complex tasks, equipping your agent with code execution as an available tool can be a powerful strategy.\n\nHowever, this strategy comes with a major drawback: executable code can be flawed or even dangerous to execute, and detecting whether code will be problematic prior to executing it is arguably an expression of the Halting Problem, making it impossible to guarantee success at detection.\n\nThe solution is sandboxing, to isolate potentially problematic code from the host environment. Now, thanks to dynamic sessions in Azure Container Apps, the ability to execute sandboxed code generated by an LLM is simple directly from LlamaIndex. It‚Äôs implemented as a tool that can be used by any LlamaIndex agent.\n\nIn this blog post we‚Äôll show you exactly how to use the new Azure Code Interpreter tool and walk you through a couple of examples of how to make the most of it. You can see the full code in this notebook and read more in the tool documentation on LlamaHub and on learn.microsoft.com.\n## Set up Azure Container Apps dynamic sessions\nFirst, install our python packages including the tool:\npip install llama-index\npip install llama-index-llms-azure\npip install llama-index-tools-azure-code-interpreter\nIn the notebook we‚Äôre using GPT 3.5 Turbo hosted on Azure as the LLM, but you can use any LLM capable of tool use:\nfrom llama_index.llms.azure_openai import AzureOpenAI\nllm = AzureOpenAI(\n    model=""gpt-35-turbo"",\n    deployment_name=""gpt-35-deploy"",\n    api_key=api_key,\n    azure_endpoint=azure_endpoint,\n    api_version=api_version,\n)\nOnce you‚Äôve got your LLM set up, you‚Äôll need to create a session pool to host your executions. Doing this will give you a pool management endpoint URL that you can provide to LlamaIndex like this:\n# Import the AzureCodeInterpreterToolSpec from llama_index\nfrom llama_index.tools.azure_code_interpreter import (\n    AzureCodeInterpreterToolSpec,\n)\n\n# Create the AzureCodeInterpreterToolSpec with the pool_managment_endpoint set to your session management endpoint\n# It is optional to set the local_save_path, but it is recommended to set it to a path where the tool can automatically save any intermediate data generated from Python code\'s output.\nazure_code_interpreter_spec = AzureCodeInterpreterToolSpec(\n    pool_managment_endpoint=""your-pool-management-endpoint"",\n    local_save_path=""local-file-path-to-save-intermediate-data"",\n)\nThis sets up a tool ready to be used with LlamaIndex. You‚Äôre now ready to set up your agent:\n# Import the ReActAgent\nfrom llama_index.core.agent import ReActAgent\n\n# Create the ReActAgent and inject the tools defined in the AzureDynamicSessionsToolSpec\nagent = ReActAgent.from_tools(\n    azure_code_interpreter_spec.to_tool_list(), llm=llm, verbose=True\n)\nIn this example we‚Äôre providing only a single tool, but you could provide any other tools you like to your ReAct agent. Now you‚Äôve got an agent, you‚Äôre ready to ask it to perform tasks!\n## Dynamic sessions code interpreter in action\nIn our first example, we‚Äôre going to ask the agent the time in Seattle. This is usually a tricky task for LLMs, which don‚Äôt know what time it is anywhere!\n# Test the agent with simple answers that could leverage Python codes\nprint(agent.chat(""Tell me the current time in Seattle.""))\nThe agent generates python code to determine the time and convert it to the correct time zone. It passes this code to Azure Container Apps dynamic sessions, which execute the code and return the answer:\nThought: To provide the current time in Seattle, I need to calculate it based on the current UTC time and adjust for Seattle\'s time zone, which is Pacific Daylight Time (PDT) during daylight saving time and Pacific Standard Time (', 'Over the past year, Large Language Models (LLMs) like GPT-4 have not only transformed how we interact with machines but also have redefined the possibilities within the realm of natural language processing (NLP). A notable trend in this evolution is the increasing popularity of open-source LLMs like Llama 2, Falcon, OPT and Yi. Some may prefer them over their commercial counterparts in terms of accessibility, data security and privacy, customization potential, cost, and vendor dependency. Among the tools gaining increasing traction in the LLM space are OpenLLM and LlamaIndex ‚Äî two powerful platforms that, when combined, unlock new use cases for building AI-driven applications.\nOpenLLM is an open-source platform for deploying and operating any open-source LLMs in production. Its flexibility and ease of use make it an ideal choice for AI application developers seeking to harness the power of LLMs. You can easily fine-tune, serve, deploy, and monitor LLMs in a wide range of creative and practical applications.\nLlamaIndex provides a comprehensive framework for managing and retrieving private and domain-specific data. It acts as a bridge between the extensive knowledge of LLMs and the unique, contextual data needs of specific applications.\nOpenLLM‚Äôs support for a diverse range of open-source LLMs and LlamaIndex‚Äôs ability to seamlessly integrate custom data sources provide great customization for developers in both communities. This combination allows them to create AI solutions that are both highly intelligent and properly tailored to specific data contexts, which is very important for query-response systems.\nIn this blog post, I will explain how you can leverage the combined strengths of OpenLLM and LlamaIndex to build an intelligent query-response system. This system can understand, process, and respond to queries by tapping into a custom corpus.\n# Setting up the environment\nThe first step is to create a virtual environment in your machine, which helps prevent conflicts with other Python projects you might be working on. Let‚Äôs just call it llamaindex-openllm and activate it.\npython -m venv llamaindex-openllm\nsource llamaindex-openllm/bin/activate\nInstall the required packages. This command installs OpenLLM with the optional vllm component (I will explain it later).\npip install ""openllm[vllm]"" llama-index llama-index-llms-openllm llama-index-embeddings-huggingface\nFor handling requests, you need to have an LLM server. Here, I use the following command to start a Llama 2 7B local server at http://localhost:3000. Feel free to choose any model that fits your needs. If you already have a remote LLM server, you can skip this step.\nopenllm start meta-llama/Llama-2-7b-chat-hf --backend vllm\nOpenLLM automatically selects the most suitable runtime implementation for the model. For models with vLLM support, OpenLLM uses vLLM by default. Otherwise, it falls back to PyTorch. vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. According to this report, you can achieve 23x LLM inference throughput while reducing P50 latency using vLLM.\nNote: To use the vLLM backend, you need a GPU with at least the Ampere architecture (or newer) and CUDA version 11.8. This demo uses a machine with an Ampere A100‚Äì80G GPU. If your machine has a compatible GPU, you can also choose vLLM. Otherwise, simply install the standard OpenLLM package (pip install openllm) in the previous command.\n# v1: Creating a simple completion service\nBefore building a query-response system, let‚Äôs get familiar with the integration of OpenLLM and LlamaIndex and use it to create a simple completion service.\nThe integration offers two APIs for interactions with LLMs:\n1. OpenLLM: This can be used to initiate a local LLM server directly without the need for starting a separate one using commands like openllm start. Here‚Äôs how you can use it:\nfrom llama_index.llms.openllm import OpenLLM\nllm = OpenLLM(\'meta-llama/Llama-2-7b-chat-hf\')\n2. OpenLLMAPI: This can be used to interact with a server hosted elsewhere, like the Llama 2 7B model I started previously.\nLet‚Äôs try the complete endpoint and see if the Llama 2 7B model is able to tell what OpenLLM is by completing the sentence ‚ÄúOpenLLM is an open source tool for‚Äù']","Sandboxing ensures safe code execution with LLMs like OpenLLM and LlamaIndex by isolating potentially problematic code from the host environment. This isolation helps to prevent any flawed or dangerous code from affecting the system, allowing for safer execution of code generated by LLMs.",multi_context,"[{'title': 'Secure code execution in LlamaIndex with Azure Container Apps dynamic sessions', 'link': 'https://www.llamaindex.ai/blog/secure-code-execution-in-llamaindex-with-azure-container-apps-dynamic-sessions', 'date': 'May 21, 2024', 'tags': ""['Microsoft', 'Azure']""}, {'title': 'Building An Intelligent Query-Response System with LlamaIndex and OpenLLM', 'link': 'https://www.llamaindex.ai/blog/building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf', 'date': 'Jan 3, 2024', 'tags': ""['Openllm', 'Llamaindex', 'AI', 'Bentoml', 'Open Source']""}]",True
84,"What features in RAGs v5 support text and visual data integration, and how do they boost multi-modal app performance?","['Howdy, Llama Enthusiasts ü¶ô,\nWe are thrilled to announce another exciting week filled with full of the latest updates, features, insightful tutorials, guides, webinars, and so much more. Have a groundbreaking project, compelling article, or captivating video? We‚Äôre all ears! Reach out to us at news@llamaindex.ai.\nDon‚Äôt forget to subscribe to our newsletter via our website to have all these exciting developments delivered directly to your inbox.\nü§© First, the highlights:\n- Llama Datasets: A diverse collection of community-contributed datasets for benchmarking RAG pipelines. Blog, Tweet.\n- RAGs v5: Enables multi-modal data handling with natural language for both text and image sources. Tweet.\n- Production RAG Pipeline: New features and a guide for efficient RAG while handling updates to your data, including incremental re-indexing for Google Docs and enhanced transformation and caching processes. Notebook, Tweet.\n- Revamped LlamaHub: A community-driven hub with universal data loaders, a new user interface, and a range of tools, templates, and datasets. Tweet.\n- AutoTranslateDoc: An open-source project for translating GitHub repository documentation into over 15 languages. Blog, Repo, Tweet\n\n‚ú® Feature Releases and Enhancements:\n- We launched Llama Datasets ü¶ôüìù, a collection of community-contributed datasets tailored for benchmarking RAG pipelines in various use cases. These datasets offer flexibility in selecting the most appropriate one for specific LLM applications. The initial release includes a diverse range, such as Code Help Desk, FinanceBench, Mini TruthfulQA, Mini Squad V2, Blockchain Solana, Uber 10K, Llama 2 Paper, Paul Graham Essay, Origin of COVID-19, CovidQADataset, MiniCovidQADataset and LLM Survey Paper. Each dataset, designed as a QA set, integrates smoothly with Llama Index abstractions, providing a platform for comprehensive benchmarking across multiple metrics. All datasets are available on LlamaHub for easy download and evaluation. Blog, Tweet.\n- We launched RAGs v5, enabling multi-modal data handling with natural language for both text and image sources. Key features include enhanced multi-modal indexing, the capability to view sources in any RAG agent, and support for loading entire directories, not just single files. Tweet.\n- We have launched new features and a guide for building a production RAG pipeline, enabling efficient question-answering with LLMs on production data even while it is continuously updated. This includes incremental re-indexing for Google Docs changes and enhanced transformation and caching processes in our updated IngestionPipeline. Notebook, Tweet.\n- We launched a one-click, full-stack LlamaIndex template now available on Replit! This template features a full-stack Next.js app in TypeScript, capable of reading any files you provide, and includes a chat interface for querying those documents. It‚Äôs completely customizable and based on our popular create-llama generator. Replit Template, Tweet.\n- We have introduced RAGEvaluatorPack to easily benchmark your RAG pipeline on any dataset with a single line of code, offering metrics like correctness, relevancy, and context similarity. Docs, Tweet.\n- We released community templates for create-llama, offering a selection of community-contributed starter templates during setup. Current examples include embedded-tables for analyzing complex tables in large PDFs, and multi-document-agent for comparing multiple documents. Tweet.\n- We launched multi-modal support in create-llama, our user-friendly command-line tool for generating full-stack LlamaIndex apps. Now, easily integrate GPT-4-vision in your app, allowing you to upload images to the web interface and receive answers about them in just seconds. Tweet.\n- We launched the Ollama LlamaPack, a new offering that integrates local LLMs and embeddings into a fully local RAG pipeline, enhancing language model accessibility and capabilities. Docs, Tweet.\n- We launched the revamped LlamaHub, a hub for community-driven modules to enhance LLM app development, featuring universal data loaders, a new user interface, and a range of tools, templates, and datasets. Tweet.\n- We introduced AutoTranslateDoc, an open-source project for translating GitHub repository documentation into over 15 languages, including Chinese, Spanish, and French. This tool, successfully implemented in our own LlamaIndex.TS docs, simplifies the internationalization process for open-source projects. Blog, Repo, Tweet\n- We released support for exact match and range queries in 4 vector databases including Weaviate', 'Greetings Llama Enthusiasts ü¶ô!\nAnother week has zoomed past, and here we are with our latest roundup of updates, features, tutorials, and so much more. Have a noteworthy project, article, or video to share? We‚Äôd love to feature it! Reach out to us at news@llamaindex.ai.\nWant these updates straight to your inbox? Simply subscribe to our newsletter on our homepage.\nü§© First, the highlights:\n- Revamped Documentation: Overhauled docs for smoother LLM/RAG app development.\n- Contribution Board: Our new board welcomes community-driven LlamaIndex enhancements.\n- Zephyr-7b-beta Insights: Tested and verified for unmatched ReAct agent task efficiency on LlamaIndex.\n- Image Captioning Boost For RAG: LLaVa‚Äôs outputs are now supercharged with knowledge-based augmentation. Notebook, Tweet\n\n‚ú® Feature Releases and Enhancements:\n- We introduced Retrieval-Augmented Image Captioning, enhancing LLaVa multi-modal model outputs with knowledge base insights. Notebook, Tweet.\n- We introduced the ability to view and set prompts for LlamaIndex modules in just two lines of code. Docs, Tweet.\n- We introduced the integration of our OpenAILike class, allowing users to tap into various open-source LLM projects with OpenAI-compatible APIs, irrespective of the model provider. Tweet.\n- We introduced Prompt Compression for RAG: with LongLLMLingua, which helps to cut token usage and latency by up to 20x. Notebook, Tweet.\n- We introduced a method to refine open-source LLMs like llama2 for structured data outputs. Using LlamaIndex, transform llama2‚Äì7b to produce Pydantic objects without PyTorch. Our guide covers synthetic dataset creation, fine-tuning, and RAG pipeline integration. Notebook, Tweet.\n\nüé• Demos:\n- Harshad Suryawanshi did a demo on equity research report generator using LlamaIndex and Streamlit.\n- Bharat Ramanathan built Wandbot, a live RAG app enabling chat over Weights & Biases documentation, integrated with Discord and SlackHQ. Key features include periodic data ingestion, custom document and code parsing, model fallback, and logging with Weights and biases.\n\nüó∫Ô∏è Guides:\n- We introduced a revamped documentation structure tailored to guide users from prototyping to production of LLM/RAG apps using LlamaIndex. Dive into our 200+ guides to enhance your app. Docs, Tweet.\n- We unveil our new Request For Contribution Github board here. It‚Äôs your guide to contribute to LlamaIndex, streamlining community suggestions.\n- We released the guide on using the Jina 8k open-source text embedding model with LlamaIndex.\n- We introduce our comprehensive survey of llama2-chat models across varying capacities in LlamaIndex. The major insight: While reasoning is enhanced with more parameters, structured outputs remain a challenge. Tweet.\n- We share a guide to test the newly released HuggingFace Zephyr-7b-beta model on LlamaIndex RAG/agent tasks, it stood out as the only 7B LLM capable of handling ReAct agent tasks over data.\n- We share a new guide on Advanced Prompt Engineering for RAG. Learn about understanding, customizing, and extending RAG prompts, from QA templates to few-shot examples and context/query transformations. Tweet.\n\n‚úçÔ∏è Tutorials:\n- Kiran made a blog post on Mastering PDFs: Extracting Sections, Headings, Paragraphs, and Tables with Cutting-Edge Parser.\n- Wenqi Glantz gave us an excellent blog post on Optimizing Text Embeddings with HuggingFace‚Äôs text-embeddings-inference Server and LlamaIndex.\n- Ravi Theja‚Äôs blog post delves into NVIDIA Research on RAG vs Long Context LLMs, questioning the necessity of RAG in the presence of long-context LLMs.\n- Sudarshan Koirala has a tutorial on Extracting Tables + Texts from .htm pages for RAG Using LlamaIndex.\n- Wenqi Glantz also made a second blog post on Multimodal Retrieval with Text Embedding and CLIP Image Embedding for Backyard Birds.\n\n‚öôÔ∏è Integrations & Collaborations:\n- We introduced our new cookbooks in partnership with Gradient AI, enabling effortless fine-tuning of open-source LLMs like Llama 2 and integration into your LlamaIndex RAG pipeline. Docs, Tweet.\n- We introduced integration with HuggingFace Inference API which gives']","RAGs v5 supports text and visual data integration through enhanced multi-modal indexing, the capability to view sources in any RAG agent, and support for loading entire directories, not just single files. These features boost multi-modal app performance by allowing for more efficient handling of diverse data types and improving the overall functionality of applications that utilize both text and image sources.",multi_context,"[{'title': 'LlamaIndex Newsletter 2023‚Äì12‚Äì12', 'link': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-12-12-4a5d542fbb1e', 'date': 'Dec 12, 2023', 'tags': ""['Llamaindex', 'LLM', 'Newsletter', 'Rag', 'AI']""}, {'title': 'LlamaIndex Newsletter 2023‚Äì10‚Äì31', 'link': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-10-31-36244e2b3f0c', 'date': 'Oct 31, 2023', 'tags': ""['LLM', 'AI', 'Newsletter', 'Llamaindex']""}]",True
85,"Why is tokenization important in finetuning, especially for input prompt formatting?","['LaMa\nThe first step here is to open up the Jupyter notebook. The notebook is organized into a series of runnable scripts that each perform the steps needed to load data.\nOur code uses Modal for every step of the orchestration, and Modal is best used on top of the Python scripts themselves. That is why a lot of these cells don‚Äôt contain Python blocks of their own.\nFirst we use Modal to load in the b-mc2/sql-create-context dataset. This is a simple task that just loads in the dataset and formats it into a .jsonl file.\nmodal run src.load_data_sql --data-dir ""data_sql""\nAs we can see, under the hood the task is quite straightforward:\n# Modal stubs allow our function to run remotely\n@stub.function(\n    retries=Retries(\n        max_retries=3,\n        initial_delay=5.0,\n        backoff_coefficient=2.0,\n    ),\n    timeout=60 * 60 * 2,\n    network_file_systems={VOL_MOUNT_PATH.as_posix(): output_vol},\n    cloud=""gcp"",\n)\ndef load_data_sql(data_dir: str = ""data_sql""):\n    from datasets import load_dataset\n\n    dataset = load_dataset(""b-mc2/sql-create-context"")\n\n    dataset_splits = {""train"": dataset[""train""]}\n    out_path = get_data_path(data_dir)\n\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n\n    for key, ds in dataset_splits.items():\n        with open(out_path, ""w"") as f:\n            for item in ds:\n                newitem = {\n                    ""input"": item[""question""],\n                    ""context"": item[""context""],\n                    ""output"": item[""answer""],\n                }\n                f.write(json.dumps(newitem) + ""\\n"")\n## Step 2: Run Finetuning Script\nThe next step is to run our finetuning script on the parsed dataset.\nmodal run src.finetune_sql --data-dir ""data_sql"" --model-dir ""model_sql""\nThe finetuning script performs the following steps.\nSplits the dataset into training and validation splits\ntrain_val = data[""train""].train_test_split(test_size=val_set_size, shuffle=True, seed=42)\ntrain_data = train_val[""train""].shuffle().map(generate_and_tokenize_prompt)\nval_data = train_val[""test""].shuffle().map(generate_and_tokenize_prompt)\nFormats each split into tuples of (input prompt, label): The input query and context are formatted into the same input prompt. The input prompt is then tokenized, and the labels are set to the exact same as the input prompt ‚Äî this allows the model to train on next-token prediction.\ndef generate_and_tokenize_prompt(data_point):\n  full_prompt = generate_prompt_sql(\n      data_point[""input""],\n      data_point[""context""],\n      data_point[""output""],\n  )\n  tokenized_full_prompt = tokenize(full_prompt)\n  if not train_on_inputs:\n      raise NotImplementedError(""not implemented yet"")\n  return tokenized_full_prompt\nThe input prompt is the exact same as what was given at the top of this blog.\nWhen the finetuning script is run, the model is saved in the remote cloud directory specified by model_dir (which is set to a default value if not']",The answer to given question is not present in context,multi_context,"[{'title': 'Easily Finetune Llama 2 for Your Text-to-SQL Applications', 'link': 'https://www.llamaindex.ai/blog/easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d', 'date': 'Aug 17, 2023', 'tags': ""['Llamaindex', 'Fine Tuning', 'Large Language Models', 'AI', 'NLP']""}]",True
86,What's the method for extracting and indexing visual and text data from HTML for better info retrieval in multimodal apps?,"['```\\n""\n            elif element.name == ""img"":\n                img_src = element.get(""src"")\n                img_caption = element.find_next(""figcaption"")\n                caption_text = img_caption.get_text().strip() if img_caption else """"\n                images.append(ImageDocument(image_url=img_src))\n            elif element.name in [""p"", ""span"", ""a""]:\n                current_section[""content""] += element.get_text().strip() + ""\\n""\n\n    if current_section[""content""].strip():\n        sections.append(current_section)\n\n    return images, sections\nI won‚Äôt go into details for the parsing code, but we split the text based on headers h1‚Äìh4 and extract image links. Then, we simply run all the articles through this function to extract all relevant information.\nall_documents = []\nall_images = []\n\n# Directory to search in (current working directory)\ndirectory = os.getcwd()\n\n# Walking through the directory\nfor root, dirs, files in os.walk(directory):\n    for file in files:\n        if file.endswith("".html""):\n            # Update the file path to be relative to the current directory\n            images, documents = process_html_file(os.path.join(root, file))\n            all_documents.extend(documents)\n            all_images.extend(images)\n\ntext_docs = [Document(text=el.pop(""content""), metadata=el) for el in all_documents]\nprint(f""Text document count: {len(text_docs)}"") # Text document count: 252\nprint(f""Image document count: {len(all_images)}"") # Image document count: 328\nWe get a total of 252 text chunks and 328 images. It‚Äôs a bit surprising that I created so many photos, but I know that some are only images of table results. We could use a vision model to filter out irrelevant photos, but I skipped this step here.\n## Indexing data vectors\nAs mentioned, we have to instantiate two vector stores, one for images and the other for text. The CLIP embedding model has a dimension of 512, while the ada-002 has 1536 dimension.\ntext_store = Neo4jVectorStore(\n    url=NEO4J_URI,\n    username=NEO4J_USERNAME,\n    password=NEO4J_PASSWORD,\n    index_name=""text_collection"",\n    node_label=""Chunk"",\n    embedding_dimension=1536\n)\nimage_store = Neo4jVectorStore(\n    url=NEO4J_URI,\n    username=NEO4J_USERNAME,\n    password=NEO4J_PASSWORD,\n    index_name=""image_collection"",\n    node_label=""Image"",\n    embedding_dimension=512\n\n)\nstorage_context = StorageContext.from_defaults(vector_store=text_store)\nNow that the vector stores have been initiated, we use the MultiModalVectorStoreIndex to index both modalities of information we have.\n# Takes 10 min without GPU / 1 min with GPU on Google collab\nindex = MultiModalVectorStoreIndex.from_documents(\n    text_docs + all_images, storage_context=storage_context, image_vector_store=image_store\n)\nUnder the hood, MultiModalVectorStoreIndex uses text and image embedding models to calculate the embeddings and store and index the results in Neo4j. Only the URLs are stored for images, not actual base64 or other representations of images.\n## Multimodal RAG pipeline\nThis piece', 'The field of AI and large language models is evolving rapidly. One year ago, nobody ever used an LLM to enhance their productivity. Today, most of us can‚Äôt imagine working without or not offloading at least some minor tasks to LLMs. Due to much research and interest, LLMs are getting better and wiser every day. Not only that, but their comprehension is starting to span across multiple modalities. With the introduction of GPT-4-Vision and other LLMs that followed it, it seems that LLMs today can tackle and comprehend images very well. Here‚Äôs one example of ChatGPT describing what‚Äôs in the image.\nUsing ChatGPT to describe images.\nAs you can observe, ChatGPT is quite good at comprehending and describing images. We can use its ability to understand images in an RAG application, where instead of relying only on text to generate an accurate and up-to-date answer, we can now combine information from text and pictures to generate more accurate answers than ever before. Using LlamaIndex, implementing multimodal RAG pipelines is as easy as it gets. Inspired by their multimodal cookbook example, I decided to test if I could implement a multimodal RAG application with Neo4j as the database.\nTo implement a multimodal RAG pipeline with LlamaIndex, you simply instantiate two vector stores, one for images and one for text, and then query both of them in order to retrieve relevant information to generate the final answer.\nWorkflow diagram for the blog post. Image by author.\nArticles are first split into images and text. These elements are then converted into vector representations and indexed separately. For text we will use ada-002 text embedding model, while for images we will be using dual encoder model CLIP, which can embed both text and images in the same embedding space. When a question is posed by an end user, two vector similarity search are performed; one to find relevant images and the other for documents. The results are fed into a multimodal LLM, which generates an answer for the user, demonstrating an integrated approach to processing and utilizing mixed media for information retrieval and response generation.\nThe code is available on GitHub.\n## Data preprocessing\nWe will use my Medium articles from 2022 and 2023 as the grounding dataset for an RAG application. The articles contain vast information about Neo4j Graph Data Science library and combining Neo4j with LLM frameworks. When you download your own articles from Medium, you get them in an HTML format. Therefore, we need to employ a bit of coding to extract text and images separately.\ndef process_html_file(file_path):\n    with open(file_path, ""r"", encoding=""utf-8"") as file:\n        soup = BeautifulSoup(file, ""html.parser"")\n\n    # Find the required section\n    content_section = soup.find(""section"", {""data-field"": ""body"", ""class"": ""e-content""})\n\n    if not content_section:\n        return ""Section not found.""\n\n    sections = []\n    current_section = {""header"": """", ""content"": """", ""source"": file_path.split(""/"")[-1]}\n    images = []\n    header_found = False\n\n    for element in content_section.find_all(recursive=True):\n        if element.name in [""h1"", ""h2"", ""h3"", ""h4""]:\n            if header_found and (current_section[""content""].strip()):\n                sections.append(current_section)\n            current_section = {\n                ""header"": element.get_text(),\n                ""content"": """",\n                ""source"": file_path.split(""/"")[-1],\n            }\n            header_found = True\n        elif header_found:\n            if element.name == ""pre"":\n                current_section[""content""] += f""```{element.get_text().strip()}']","The method for extracting and indexing visual and text data from HTML involves parsing the HTML to find specific sections, extracting image links and text content, and then converting these elements into vector representations. The text and images are indexed separately using two vector stores, one for images and one for text, allowing for efficient retrieval of relevant information when a question is posed by an end user.",multi_context,"[{'title': 'Multimodal RAG pipeline with LlamaIndex and Neo4j', 'link': 'https://www.llamaindex.ai/blog/multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206', 'date': 'Dec 18, 2023', 'tags': ""['Neo4j', 'LLM', 'Llamaindex', 'ChatGPT', 'Graph Database']""}, {'title': 'Multimodal RAG pipeline with LlamaIndex and Neo4j', 'link': 'https://www.llamaindex.ai/blog/multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206', 'date': 'Dec 18, 2023', 'tags': ""['Neo4j', 'LLM', 'Llamaindex', 'ChatGPT', 'Graph Database']""}]",True
87,How does document store metadata enhance auto-retrieval for structured city data and unstructured articles?,"[' document store metadata though; that way we don‚Äôt need to spend an extra LLM call in the auto-retrieval step inferring the right metadata filters.\n\n# Experiments\nSo how well does this work? It works surprisingly well across a broad range of queries, from queries that can leverage both structured data and unstructured data to queries that are specific to a structured data collection or unstructured data collection.\n## Setup\nOur experiment setup is very simple. We have a SQL table called city_stats which contains the city, population, and country of three different cities: Toronto, Tokyo, and Berlin.\nWe also use a Pinecone index to store Wikipedia articles corresponding to the three cities. Each article is chunked up and stored as a separate ‚ÄúNode‚Äù object; each chunk also contains a title metadata attribute containing the city name.\nWe then derive the VectorIndexAutoRetriever and RetrieverQueryEngine from the Pinecone vector index.\nfrom llama_index.indices.vector_store.retrievers import VectorIndexAutoRetriever\nfrom llama_index.vector_stores.types import MetadataInfo, VectorStoreInfo\nfrom llama_index.query_engine.retriever_query_engine import RetrieverQueryEngine\n\n\nvector_store_info = VectorStoreInfo(\n    content_info=\'articles about different cities\',\n    metadata_info=[\n        MetadataInfo(\n            name=\'city\', \n            type=\'str\', \n            description=\'The name of the city\'),\n    ]\n)\nvector_auto_retriever = VectorIndexAutoRetriever(vector_index, vector_store_info=vector_store_info)\n\nretriever_query_engine = RetrieverQueryEngine.from_args(\n    vector_auto_retriever, service_context=service_context\n)\nYou can also get the SQL query engine as follows\nsql_query_engine = sql_index.as_query_engine()\nBoth the SQL query engine and vector query engine can be wrapped as QueryEngineTool objects.\nsql_tool = QueryEngineTool.from_defaults(\n    query_engine=sql_query_engine,\n    description=(\n        \'Useful for translating a natural language query into a SQL query over a table containing: \'\n        \'city_stats, containing the population/country of each city\'\n    )\n)\nvector_tool = QueryEngineTool.from_defaults(\n    query_engine=query_engine,\n    description=f\'Useful for answering semantic questions about different cities\',\n)\nFinally, we can define our SQLAutoVectorQueryEngine\nquery_engine = SQLAutoVectorQueryEngine(\n    sql_tool,\n    vector_tool,\n    service_context=service_context\n)\n## Results\nWe run some example queries.\nQuery 1\nquery_engine.query(\n  \'Tell me about the arts and culture of the city with the highest population\'\n)\nIntermediate steps:\n\nFinal Response:\nTokyo is the city with the highest population, with 13.96 million people. It is a vibrant city with a rich culture and a wide variety of art forms. From traditional Japanese art such as calligraphy and woodblock prints to modern art galleries and museums, Tokyo has something for everyone. There are also many festivals and events throughout the year that celebrate the city\'s culture and art. Some popular festivals and events in Tokyo include the Sann≈ç Festival at Hie Shrine, the Sanja Festival at Asakusa Shrine, and the biennial Kanda Festivals. These events often feature parades with elaborately decorated floats and thousands of people. Additionally, an enormous fireworks display over the Sumida River takes place annually on the last Saturday of July, attracting over a million viewers. During spring, when cherry blossoms bloom, many residents gather in Ueno Park, Inokashira Park, and the Shinjuku Gyoen National Garden for picnics under the blossoms.\nThis query runs through the full flow of the SQLAutoVectorQueryEngine . It first queries the SQL database for the city with the highest population (‚ÄúTokyo‚Äù), and then queries the vector database with the new query. The results are combined into a final response.\nQuery 2\nquery_engine.query(""Tell me about the history of Berlin"")\nIntermediate Steps:\n\nFinal Response:\nBerlin', ' example query: ‚ÄúTell me about the arts and culture of the city with the highest population.‚Äù\nThe ‚Äúproper‚Äù way to answer this question is roughly as follows:\n- Query the structured table for the city with the highest population.\n\nSELECT city, population FROM city_stats ORDER BY population DESC LIMIT 1\n- Convert the original question into a more detailed question: ‚ÄúTell me about the arts and culture of Tokyo.‚Äù\n- Ask the new question over your vector database.\n- Use the original question + intermediate queries/responses to SQL db and vector db to synthesize the answer.\n\nLet‚Äôs think about some of the high-level implications of such a sequence:\n- Instead of doing embedding search (and optionally metadata filters) to retrieve relevant context, we want to somehow have a SQL query as a first ‚Äúretrieval‚Äù step.\n- We want to make sure that we can somehow ‚Äújoin‚Äù the results from the SQL query with the context stored in the vector database. There is no existing language to ‚Äújoin‚Äù information between a SQL and vector database. We will have to implement this behavior ourselves.\n- Neither data source can answer this question on its own. The structured table only contains population information. The vector database contains city information but no easy way to query for the city with the maximum population.\n\n# A Query Engine to Combine Structured Analytics and Semantic Search\nWe have created a brand-new query engine ( SQLAutoVectorQueryEngine ) that can query, join, sequence, and combine both structured data from both your SQL database and unstructured data from your vector database in order to synthesize the final answer.\nThe SQLAutoVectorQueryEngine is initialized through passing in a SQL query engine ( GPTNLStructStoreQueryEngine ) as well as a query engine that uses our vector store auto-retriever module ( VectorIndexAutoRetriever ). Both the SQL query engine and vector query engines are wrapped as ‚ÄúTool‚Äù objects containing a name and description field.\nReminder: the VectorIndexAutoRetriever takes in a natural language query as input. Given some knowledge of the metadata schema of the vector database, the auto retriever first infers the other necessary query parameters to pass in (e.g. top-k value, and metadata filters), and executes a query against the vector database with all the query parameters.\nDiagram of the flow for SQLAutoVectorQueryEngine\nDuring query-time, we run the following steps:\n- A selector prompt (similarly used in our RouterQueryEngine , see guide) first chooses whether we should query the SQL database or the vector database. If it chooses to use the vector query engine, then the rest of the function execution is the same as querying the RetrieverQueryEngine with VectorIndexAutoRetriever .\n- If it chooses to query the SQL database, it will execute a text-to-SQL query operation against the database, and (optionally) synthesize a natural language output.\n- A query transformation is run, to convert the original question into a more detailed question given the results from the SQL query. For instance if the original question is ‚ÄúTell me about the arts and culture of the city with the highest population.‚Äù, and the SQL query returns Tokyo as the city with the highest population, then the new query is ‚ÄúTell me about the arts and culture of Tokyo.‚Äù The one exception is if the SQL query itself is enough to answer the original question; if it is, then function execution returns with the SQL query as the response.\n- The new query is then run through through the vector store query engine, which performs retrieval from the vector store and then LLM response synthesis. We enforce using a VectorIndexAutoRetriever module. This allows us to automatically infer the right query parameters (query string, top k, metadata filters), given the result of the SQL query. For instance, with the example above, we may infer the query to be something like query_str=""arts and culture"" and filters={""title"": ""Tokyo""} .\n- The original question, SQL query, SQL response, vector store query, and vector store response are combined into a prompt to synthesize the final answer.\n\nTaking a step back, here are some general comments about this approach:\n- Using our auto-retrieval module is our way of simulating a join between the SQL database and vector database. We effectively use the results from our SQL query to determine the parameters to query the vector database with.\n- This also implies that there doesn‚Äôt need to be an explicit mapping between the items in the SQL database and the metadata in the vector database, since we can rely on the LLM being able come up with the right query for different items. It would be interesting to model explicit relationships between structured tables and']","Document store metadata enhances auto-retrieval for structured city data and unstructured articles by allowing the system to efficiently filter and retrieve relevant information based on metadata attributes. This integration enables the SQLAutoVectorQueryEngine to leverage both structured data from SQL tables and unstructured data from vector databases, facilitating a more comprehensive and accurate response to queries. By using metadata to inform the retrieval process, the system can better match user queries with the appropriate data sources, ultimately improving the quality and relevance of the results.",multi_context,"[{'title': 'Combining Text-to-SQL with Semantic Search for Retrieval Augmented Generation', 'link': 'https://www.llamaindex.ai/blog/combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b', 'date': 'May 28, 2023', 'tags': ""['Artificial Intelligence', 'Machine Learning', 'NLP', 'Data Science', 'Database']""}, {'title': 'Combining Text-to-SQL with Semantic Search for Retrieval Augmented Generation', 'link': 'https://www.llamaindex.ai/blog/combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b', 'date': 'May 28, 2023', 'tags': ""['Artificial Intelligence', 'Machine Learning', 'NLP', 'Data Science', 'Database']""}]",True
88,How does LlamaParse improve retrieval of complex docs with tables in LlamaCloud's services?,"['K filing. Full comparisons are here. A green highlight in a cell means that the RAG pipeline correctly returned the cell value as the answer to a question over that cell. A red highlight means that the question was answered incorrectly.\nThis is where LlamaParse comes in. We‚Äôve developed a proprietary parsing service that is incredibly good at parsing PDFs with complex tables into a well-structured markdown format. This representation directly plugs into the advanced Markdown parsing and recursive retrieval algorithms available in the open-source library. The end result is that you are able to build RAG over complex documents that can answer questions over both tabular and unstructured data. Check out the results below for a comparison:\nComparison of baseline PDF approach (top) vs. LlamaParse + advanced retrieval (bottom)\nResults over the Uber 10K Dataset. For more information on our evaluation metrics check out our evaluation page here.\nThis service is available in a public preview mode: available to everyone, but with a usage limit (1k pages per day). It operates as a standalone service that also plugs into our managed ingestion and retrieval API (see below). Check out our LlamaParse onboarding here for more details.\nfrom llama_parse import LlamaParse\n\nparser = LlamaParse(\n    api_key=""llx-..."",  # can also be set in your env as LLAMA_CLOUD_API_KEY\n    result_type=""markdown"",  # ""markdown"" and ""text"" are available\n    verbose=True\n)\nFor unlimited commercial use of LlamaParse, get in touch with us.\nNext Steps\nOur early users have already given us important feedback on what they‚Äôd like to see next. Currently we primarily support PDFs with tables, but we are also building out better support for figures, and and an expanded set of the most popular document types: .docx, .pptx, .html.\n# Managed Ingestion and Retrieval\nOur other main offering in LlamaCloud is a managed ingestion and retrieval API which allows you to easily declare performant data pipelines for any context-augmented LLM application.\nGet clean data for your LLM application, so you can spend less time wrangling data and more time writing core application logic. LlamaCloud empowers enterprise developers with the following benefits:\n- Engineering Time Savings: Instead of having to write custom connectors and parsing logic in Python, our APIs allow you to directly connect to different data sources.\n- Performance: we provide good out-of-the-box performance for different data types, while offering an intuitive path for experimentation, evaluation, and improvement.\n- Ease Systems Complexity: Handle a large number of data sources with incremental updates.\n\nLet‚Äôs do a brief tour through the core components!\n- Ingestion: Declare a managed pipeline to process and transform/chunk/embed data backed by our 150+ data sources in LlamaHub and our 40+ storage integrations as destinations. Automatically handle syncing and load balancing. Define through the UI or our open-source library.\n- Retrieval: Get access to state-of-the-art, advanced retrieval backed by our open-source library and your data storage. Wrap it in an easy-to-use REST API that you can consume from any language.\n- Playground: Interactive UI to test and refine your ingestion/retrieval strategies pre-deployment, with evaluations in the loop.\n\nLlamaCloud Playground: configure, evaluate, and optimize your ingestion/retrieval pipeline before deployment.\nLlamaCloud Retrieval: Access advanced retrieval over your storage system via an API.\nWe are opening up a private beta to a limited set of enterprise partners for the managed ingestion and retrieval API. If you‚Äôre interested in centralizing your data pipelines and spending more time working on your actual RAG use cases, come talk to us.\n# Launch Partners and Collaborators\nWe opened up access to LlamaParse at our hackathon we co-hosted with Futureproof Labs and Datastax at the beginning of February. We saw some incredible applications of LlamaParse in action, including parsing building codes for Accessory Dwelling Unit (ADU) planning, parsing real-estate disclosures for home buying, and dozens more.\nEric Ciarla, co-founder at Mendable AI, incorporated LlamaParse into Mendable‚Äôs data stack: ‚ÄúWe integrated LlamaParse into our open source data connector repo which powers our production ingestion suite. It was easy to integrate and more powerful than any of the alternatives we tried.‚Äù\nWe‚Äôre also excited to be joined by initial launch partners and collaborators in the LLM and AI ecosystem', 'Today is a big day for the LlamaIndex ecosystem: we are announcing LlamaCloud, a new generation of managed parsing, ingestion, and retrieval services, designed to bring production-grade context-augmentation to your LLM and RAG applications.\nUsing LlamaCloud as an enterprise AI engineer, you can focus on writing the business logic and not on data wrangling. Process large volumes of production data, immediately leading to better response quality. LlamaCloud launches with the following key components:\n- LlamaParse: Proprietary parsing for complex documents with embedded objects such as tables and figures. LlamaParse directly integrates with LlamaIndex ingestion and retrieval to let you build retrieval over complex, semi-structured documents. You‚Äôll be able to answer complex questions that simply weren‚Äôt possible previously.\n- Managed Ingestion and Retrieval API: An API which allows you to easily load, process, and store data for your RAG app and consume it in any language. Backed by data sources in LlamaHub, including LlamaParse, and our data storage integrations.\n\nLlamaParse is available in a public preview setting starting today. It can currently handle PDFs and usage is capped for public use; contact us for commercial terms. The managed ingestion and retrieval API is available as a private preview; we are offering access to a limited set of enterprise design partners. If you‚Äôre interested, get in touch. (We‚Äôve also launched a new version of our website ü¶ô!)\n# RAG is Only as Good as your Data\nA core promise of LLMs is the ability to automate knowledge search, synthesis, extraction, and planning over any source of unstructured data. Over the past year a new data stack has emerged to power these context-augmented LLM applications, popularly referred to as Retrieval-Augmented Generation (RAG). This stack includes loading data, processing it, embedding it, and loading into a vector database. This enables downstream orchestration of retrieval and prompting to provide context within an LLM app.\nThis stack is different from any ETL stack before it, because unlike traditional software, every decision in the data stack directly affects the accuracy of the full LLM-powered system. Every decision like chunk size and embedding model affects LLM outputs, and since LLMs are black boxes, you can‚Äôt unit test your way to correct behavior.\nWe‚Äôve spent the past year hard at work at the forefront of providing tooling and educating users on how to build high-performing, advanced RAG for various use cases. We crossed the 2M monthly download mark, and are used by large enterprises to startups, including Adyen, T-Systems, Jasper.ai, Weights and Biases, DataStax, and many more.\nBut while getting started with our famous 5-line starter example is easy, building production-grade RAG remains a complex and subtle problem. In our hundreds of user conversations, we learned the biggest pain points:\n- Results aren‚Äôt accurate enough: The application was not able to produce satisfactory results for a long-tail of input tasks/queries.\n- The number of parameters to tune is overwhelming: It‚Äôs not clear which parameters across the data parsing, ingestion, retrieval.\n- PDFs are specifically a problem: I have complex docs with lots of messy formatting. How do I represent this in the right way so the LLM can understand it?\n- Data syncing is a challenge: Production data often updates regularly, and continuously syncing new data brings a new set of challenges.\n\nThese are the problems we set out to solve with LlamaCloud.\n# Data Pipelines to Bring you to Production\nWe built LlamaCloud and LlamaParse as the data pipelines to get your RAG application to production more quickly.\n# LlamaParse\nLlamaParse is a state-of-the-art parser designed to specifically unlock RAG over complex PDFs with embedded tables and charts. This simply wasn‚Äôt possible before with other approaches, and we‚Äôre incredibly excited about this technology.\nLlamaParse Demo. Given a PDF file, returns a parsed markdown file that maintains semantic structure within the document.\nFor the past few months we‚Äôve been obsessed with this problem. This is a surprisingly prevalent use case across a variety of data types and verticals, from ArXiv papers to 10K filings to medical reports.\nNaive chunking and retrieval algorithms do terribly. We were the first to propose a novel recursive retrieval RAG technique for being able to hierarchically index and query over tables and text in a document. The only challenge that remained was how to properly parse out tables and text in the first place.\nComparison of LlamaParse vs. PyPDF over the Apple 10']","LlamaParse improves the retrieval of complex documents with tables by providing a proprietary parsing service that effectively converts PDFs with complex tables into a well-structured markdown format. This structured representation allows for advanced retrieval algorithms to be applied, enabling the answering of questions over both tabular and unstructured data, which was not possible with previous approaches.",multi_context,"[{'title': 'Introducing LlamaCloud and LlamaParse', 'link': 'https://www.llamaindex.ai/blog/introducing-llamacloud-and-llamaparse-af8cedf9006b', 'date': 'Feb 20, 2024', 'tags': ""['Llamaindex', 'LLM', 'AI', 'Retrieval Augmented', 'Data']""}, {'title': 'Introducing LlamaCloud and LlamaParse', 'link': 'https://www.llamaindex.ai/blog/introducing-llamacloud-and-llamaparse-af8cedf9006b', 'date': 'Feb 20, 2024', 'tags': ""['Llamaindex', 'LLM', 'AI', 'Retrieval Augmented', 'Data']""}]",True
89,How does Multi-doc QA improve RAG by tackling token loss and info perception?,"[' easily lose key information because it cannot perceive the question information.\nIn recent submissions to ICLR‚Äô24, there have been some similar practices. For example, Recomp[4] reduces the use of tokens in RAG scenarios by jointly training compressors of two different granularities. RAG in Long Context[5] decomposes the long context into a series of chunks and uses retrieval methods for compression, which is actually the retrieval-based method implemented in the LongLLMLingua paper. In addition, Walking Down the Memory Maze[6] also designed a hierarchical summarization tree to enhance the LLM‚Äôs perception of key information.\n\n# Question-aware Fine-grained Prompt Compression\nIn order to make token-level prompt compression also perceive the information of the question, we propose a contrastive perplexity, which compares the difference between the perplexity distribution corresponding to the document and the perplexity distribution corresponding to the document with the question.\n\nAn intuitive feeling is that when the question serves as context, the perplexity corresponding to the relevant tokens in the document will decrease. This decrease in magnitude represents the importance of the tokens in the document relative to the question.\nFigure 3. Comparison between perplexities and contrastive perplexities of tokens in the prompt from Multi-documemnt QA dataset. The document with the ground truth is located on the left side of the dashed line.\nFigure 3 shows the distribution difference in extracting key tokens between perplexity and contrastive perplexity.\n# How to reduce the loss in the middle\nSince Coarse-grained Prompt compression far exceeds other retrieval methods in terms of accuracy, it is a very natural idea to use this ranking information to rearrange the documents that are more related to the question to the beginning and end of the prompt. However, through our testing, we found that rearranging to the beginning of the prompt is more effective than evenly distributing at both ends. So, we choose to reorder the most related document to the beginning of the prompt.\n\n# How to achieve adaptive granular control during compression?\nIn order to better use the information from the two grained compressions, in the fine-grained prompt compression, we dynamically allocate different compression ratios to different documents based on the rank information obtained from the coarse-grained compression, thereby preserving more important information from important documents.\n\n# How to improve the integrity of key information?\nSince LongLLMLingua is a token-level prompt compression, it will inevitably delete some tokens of the word, which may result in some retrieval-related tasks not getting complete results. But this can actually be recovered through a simple subsequence matching method. Specifically, there is a subsequence relationship between the original prompt, compressed prompt and response. By establishing the mapping relationship between the response subsequence that appears in the compressed prompt and the subsequence of the original prompt, the original prompt content can be effectively recovered.\n\n# Experiments\nTo evaluate the effectiveness of LongLLMLingua, we conducted detailed tests in Multi-document QA (RAG) and two long Context benchmarks. Particularly, the dataset chosen for Multi-document QA is very close to the actual RAG scenario (e.g. Bing Chat), where Contriever (one of the state-of-the-art retrieval systems) is used to recall 20 relevant documents including one ground-truth. The original documents have a high semantic relevance with the question.\n\nAs can be seen, compared to Retrieval-based methods and compression-based methods, LongLLMLingua improves performance more in the RAG scenario, and can increase up to 21.4 points at a 4x compression rate, avoiding the original ‚Äúlost in the middle‚Äù situation.\n\nThe results of the two benchmarks, LongBench and ZeroScrolls, also reached similar conclusions. LongLLMLingua is better at retaining key information related to the question in long context scenarios.\nBesides, LongLLMLingua is very efficient and can speed up the end-to-end inference process.\n\n# Used in LlamaIndex\nThank Jerry Liu for your help with the LongLLMLingua project. Now you can use LongLLMLingua as a NodePostprocessor in this widely used RAG framework. For specific usage, you can refer to the example 1, example 2 and the following code.\nfrom llama_index.query_engine import RetrieverQueryEngine\nfrom llama_index.response_synthesizers import CompactAndRefine\nfrom llama_index.indices.postprocessor import LongLLMLinguaPostprocessor\n\nnode_postprocessor = LongLLMLinguaPostprocessor(\n    instruction_str=""Given the context, please answer the final question"",\n    target_token=300,\n    rank_method=""longllmlingua"",\n    additional_compress', 'In the RAG, after the retrieval phase, it‚Äôs necessary to perform Re-ranking + Fine-Grained Prompt Compression + Subsequence Recovery to enhance LLM‚Äôs perception of key information, which is LongLLMLingua.\n\nTL;DR: While Retrieval-Augmented Generation (RAG) is highly effective in various scenarios, it still has drawbacks such as 1) Performance drop, like the ‚ÄúLost in the middle‚Äù issue, 2) High costs, both financially and in terms of latency, and 3) Context windows limitation. LongLLMLingua offers a solution to these problems in RAG or Long Context scenarios via prompt compression. It can boost accuracy by as much as 21.4% while only using ¬º of the tokens. In long context situations, it can save $28 for every 1000 examples.\nSee real-world cases on the project page.\nWe previously wrote a blog post introducing the design of LLMLingua, which started from the perspective of designing a special language for LLMs. This time, our focus will be on the scenarios involving RAG.\nRetrieval-Augmented Generation is currently the most reliable and proven technique for creating AI-agents that are grounded on any specific collection of text. Frameworks like LlamaIndex provide comprehensive RAG solutions to help users utilize specialized data in LLMs more conveniently.\nA common misunderstanding is that retrieving as many relevant documents as possible during the RAG process and stitching them together to form a long retrieved prompt is beneficial, especially as more and more LLMs support longer context windows. However, this method can introduce more noise into the prompt and weaken the LLM‚Äôs perception of key information, leading to issues such as ‚Äòlost in the middle‚Äô[1].\nThese issues become more apparent in real-world scenarios involving RAG. Better retrieval mechanisms can introduce higher quality noise documents, which can more easily lead to a drop in performance.\n# Re-ranking is an intuitive concept.\nOne intuitive idea is to reposition the most relevant information to the sides of the prompt through re-ranking. This concept of re-ranking has already been implemented in frameworks such as LlamaIndex and LangChain.\nHowever, according to our experiments, it‚Äôs difficult for an embedding model to serve as a ‚Äògood‚Äô re-ranker. The underlying reason is the lack of an interaction process between the query and the document. The dual-tower structure of embeddings is not suitable for re-ranking in general scenarios, although it may be effective after fine-tuning.\nUsing LLMs directly as a re-ranker may also lead to misjudgments due to hallucinations. Recently, some re-ranking models have been extended from embedding models, such as bge-rerank. However, such re-ranking models generally have context window limitations.\nTo address the above issues, we propose a Question-aware Coarse-Grained prompt compression method. This method evaluates the relevance between the context and the question based on the perplexity corresponding to the question.\n\nTo mitigate the hallucination problem in smaller LLMs, we append a restrictive statement, specifically ‚ÄúWe can get the answer to this question in the given documents‚Äù, after the question to limit the latent space caused by related hallucinations.\nFigure 1. The accuracy of different methods for ranking documents from Multi-documemnt QA dataset, which increases from top to bottom in terms of Recall@1. Different colors represent different types of methods. Among them, yellow represents traditional relevance methods, green signifies embedding-based methods, and red denotes rerank-based methods. You can find the script in this link.\nResults show that this approach significantly outperforms both embedding models and re-ranking models. We‚Äôve added some recently released embedding and reranking models. As you can see, the performance of bge-rerank-large is very close to that of LongLLMLingua. Reranking models generally perform better than embedding models. Currently, Jina is the best performing method among the embedding models.\n# Compress unrelated and unimportant information\nBesides recalling as many relevant documents as possible, another approach is to compress irrelevant or unimportant contexts as much as possible.\nPrevious work on long context has focused on how to extend LLMs to support longer context windows. However, almost no work has explored whether this can actually improve the performance of downstream tasks. Some previous studies have shown that the presence of more noise in the prompt, as well as the position of key information in the prompt, can affect the performance of LLMs.\nFrom the perspective of prompt compression, Selective Context[2] and LLMLingua[3] estimate the importance of elements by using a small language model to calculate the mutual information or perplexity of the prompt. However, in scenarios like RAG or long context scenarios, this method can']","Multi-doc QA improves RAG by addressing token loss and information perception through methods like LongLLMLingua, which enhances the LLM's ability to retain key information related to the question in long context scenarios. It achieves this by performing Re-ranking, Fine-Grained Prompt Compression, and Subsequence Recovery, which collectively help in mitigating the 'lost in the middle' issue and improve overall performance.",multi_context,"[{'title': 'LongLLMLingua: Bye-bye to Middle Loss and Save on Your RAG Costs via Prompt Compression', 'link': 'https://www.llamaindex.ai/blog/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7', 'date': 'Nov 6, 2023', 'tags': ""['Large Language Models', 'LLM', 'Retrieval', 'NLP', 'ChatGPT']""}, {'title': 'LongLLMLingua: Bye-bye to Middle Loss and Save on Your RAG Costs via Prompt Compression', 'link': 'https://www.llamaindex.ai/blog/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7', 'date': 'Nov 6, 2023', 'tags': ""['Large Language Models', 'LLM', 'Retrieval', 'NLP', 'ChatGPT']""}]",True
90,What integration tracks sessions and traces for RAG apps with Langfuse?,"['This is a guest post from the team at Langfuse\n\nThere are so many different ways to make RAG work for a use case. What vector store to use? What retrieval strategy to use? LlamaIndex makes it easy to try many of them without having to deal with the complexity of integrations, prompts and memory all at once.\n\nInitially, we at Langfuse worked on complex RAG/agent applications and quickly realized that there is a new need for observability and experimentation to tweak and iterate on the details. In the end, these details matter to get from something cool to an actually reliable RAG application that is safe for users and customers. Think of this: if there is a user session of interest in your production RAG application, how can you quickly see whether the retrieved context for that session was actually relevant or the LLM response was on point?\n\nThus, we started working on Langfuse.com (GitHub) to establish an open source LLM engineering platform with tightly integrated features for tracing, prompt management, and evaluation. In the beginning we just solved our own and our friends‚Äô problems. Today we are at over 1000 projects which rely on Langfuse, and 2.3k stars on GitHub. You can either self-host Langfuse or use the cloud instance maintained by us.\n\nWe are thrilled to announce our new integration with LlamaIndex today. This feature was highly requested by our community and aligns with our project\'s focus on native integration with major application frameworks. Thank you to everyone who contributed and tested it during the beta phase!\n## The challenge\nWe love LlamaIndex, since the clean and standardized interface abstracts a lot of complexity away. Let‚Äôs take this simple example of a VectorStoreIndex and a ChatEngine.\nfrom llama_index.core import SimpleDirectoryReader\nfrom llama_index.core import VectorStoreIndex\n\ndocuments = SimpleDirectoryReader(""./data"").load_data()\n\nindex = VectorStoreIndex.from_documents(documents)\n\nchat_engine = index.as_chat_engine()\n\nprint(chat_engine.chat(""What problems can I solve with RAG?""))\nprint(chat_engine.chat(""How do I optimize my RAG application?""))\nIn just 3 lines we loaded our local documents, added them to an index and initialized a ChatEngine with memory. Subsequently we had a stateful conversation with the chat_engine.\n\nThis is awesome to get started, but we quickly run into questions like:\n- ‚ÄúWhat context is actually retrieved from the index to answer the questions?‚Äù\n- ‚ÄúHow is chat memory managed?‚Äù\n- ‚ÄúWhich steps add the most latency to the overall execution? How to optimize it?‚Äù\n\n## One-click OSS observability to the rescue\n\nWe integrated Langfuse to be a one-click integration with LlamaIndex using the global callback manager.\n\nPreparation\n- Install the community package (pip install llama-index-callbacks-langfuse)\n- Copy/paste the environment variables from the Langfuse project settings to your Python project: \'LANGFUSE_SECRET_KEY\', \'LANGFUSE_PUBLIC_KEY\' and \'LANGFUSE_HOST\'\n\n\nNow, you only need to set the global langfuse handler:\nfrom llama_index.core import set_global_handler\n\nset_global_handler(""langfuse"")\nAnd voil√°, with just two lines of code you get detailed traces for all aspects of your RAG application in Langfuse. They automatically include latency and usage/cost breakdowns.\n\n## Group multiple chat threads into a session\nWorking with lots of teams building GenAI/LLM/RAG applications, we‚Äôve continuously added more features that are useful to debug and improve these applications. One example is session tracking for conversational applications to see the traces in context of a full message thread.\n\nTo activate it, just add an id that identifies the session as a trace param before calling the chat_engine.\nfrom llama_index.core import global_handler\n\nglobal_handler.set_trace_params(\n  session_id=""your-session-id""\n)\n\nchat_engine.chat(""What did he do growing up?"")\nchat_engine.chat(""What did he do at USC?"")\nchat_engine.chat(""How old is he?"")\nThereby you can see all these chat invocations grouped into a session view in Langfuse Tracing:\n\nNext to sessions, you can also track individual users or add tags and metadata to your Langfuse traces.\n## Trace more complex applications and use other Langfuse features for prompt']",The integration that tracks sessions and traces for RAG apps with Langfuse is the one-click integration with LlamaIndex using the global callback manager.,reasoning,"[{'title': 'One-click Open Source RAG Observability with Langfuse', 'link': 'https://www.llamaindex.ai/blog/one-click-open-source-rag-observability-with-langfuse', 'date': 'Mar 18, 2024', 'tags': ""['LLM', 'Observability']""}]",True
91,What metrics gauge RAG retrieval performance and their calculations?,"['UPDATE: The pooling method for the Jina AI embeddings has been adjusted to use mean pooling, and the results have been updated accordingly. Notably, the JinaAI-v2-base-en with bge-reranker-largenow exhibits a Hit Rate of 0.938202 and an MRR (Mean Reciprocal Rank) of 0.868539 and withCohereRerank exhibits a Hit Rate of 0.932584, and an MRR of 0.873689.\nWhen building a Retrieval Augmented Generation (RAG) pipeline, one key component is the Retriever. We have a variety of embedding models to choose from, including OpenAI, CohereAI, and open-source sentence transformers. Additionally, there are several rerankers available from CohereAI and sentence transformers.\nBut with all these options, how do we determine the best mix for top-notch retrieval performance? How do we know which embedding model fits our data best? Or which reranker boosts our results the most?\nIn this blog post, we‚Äôll use the Retrieval Evaluation module from LlamaIndex to swiftly determine the best combination of embedding and reranker models. Let\'s dive in!\nLet‚Äôs first start with understanding the metrics available in Retrieval Evaluation\n# Understanding Metrics in Retrieval Evaluation:\nTo gauge the efficacy of our retrieval system, we primarily relied on two widely accepted metrics: Hit Rate and Mean Reciprocal Rank (MRR). Let‚Äôs delve into these metrics to understand their significance and how they operate.\nHit Rate:\nHit rate calculates the fraction of queries where the correct answer is found within the top-k retrieved documents. In simpler terms, it‚Äôs about how often our system gets it right within the top few guesses.\nMean Reciprocal Rank (MRR):\nFor each query, MRR evaluates the system‚Äôs accuracy by looking at the rank of the highest-placed relevant document. Specifically, it‚Äôs the average of the reciprocals of these ranks across all the queries. So, if the first relevant document is the top result, the reciprocal rank is 1; if it‚Äôs second, the reciprocal rank is 1/2, and so on.\nNow that we‚Äôve established the scope and familiarized ourselves with the metrics, it‚Äôs time to dive into the experiment. For a hands-on experience, you can also follow along using our Google Colab Notebook\n# Setting Up the Environment\n!pip install llama-index sentence-transformers cohere anthropic voyageai protobuf pypdf\n# Setting Up the Keys\nopenai_api_key = \'YOUR OPENAI API KEY\'\ncohere_api_key = \'YOUR COHEREAI API KEY\'\nanthropic_api_key = \'YOUR ANTHROPIC API KEY\'\nopenai.api_key = openai_api_key\n# Download the Data\nWe will use Llama2 paper for this experiment. Let‚Äôs download the paper.\n!wget --user-agent ""Mozilla"" ""https://arxiv.org/pdf/2307.09288.pdf"" -O ""llama2.pdf""\n# Load the Data\nLet‚Äôs load the data. We will use Pages from start to 36 for the experiment which excludes table of contents, references, and appendix.\nThis data was then parsed by converted to nodes, which represent chunks of data we‚Äôd like to retrieve. We did use chunk_size as 512.\ndocuments = SimpleDirectoryReader(input_files=[""llama2.pdf""]).load_data()\n\nnode_parser = SimpleNodeParser.from_defaults(chunk_size=512)\nnodes = node_parser.get_nodes_from_documents(documents)\n# Generating Question-Context Pairs:\nFor evaluation purposes, we created a dataset of question-context pairs. This dataset can be seen as a set of questions and their corresponding context from our data. To remove bias for the evaluation of embedding(OpenAI/ CohereAI) and Reranker (CohereAI), we use Anthropic LLM to generate Question-Context Pairs.\nLet‚Äôs initialize a prompt template to generate question-context pairs.\n# Prompt to generate questions\nqa_generate_prompt_tmpl = """"""\\\nContext information is below.\n\n---------------------\n{context_str}\n---------------------\n\nGiven the context information and not prior knowledge.\ngenerate only questions based on the below query.\n\nYou are a Professor. Your task is to setup \\\n{num_questions_per']","The metrics that gauge RAG retrieval performance are Hit Rate and Mean Reciprocal Rank (MRR). Hit Rate calculates the fraction of queries where the correct answer is found within the top-k retrieved documents, while MRR evaluates the system‚Äôs accuracy by looking at the rank of the highest-placed relevant document, averaging the reciprocals of these ranks across all queries.",reasoning,"[{'title': 'Boosting RAG: Picking the Best Embedding & Reranker models', 'link': 'https://www.llamaindex.ai/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83', 'date': 'Nov 3, 2023', 'tags': ""['Embedding', 'LLM', 'OpenAI', 'Search', 'Llamaindex']""}]",True
92,What benefits do Replit templates offer LlamaIndex users for starting projects?,"['Hello LlamaIndex Enthusiasts!\nWelcome to the fifth edition of our LlamaIndex Update series.\n## Most Important Takeaways:\n- We‚Äôve open-sourced SECInsights.ai ‚Äî your gateway to the production RAG framework.\n- Replit templates ‚Äî kickstart your projects with zero environment setup hassles.\n- Build RAG from scratch and get hands-on with our processes.\n\nBut wait, there‚Äôs more!\n- Feature Releases and Enhancements\n- Fine-Tuning Guides\n- Retrieval Tips for RAG\n- Building RAG from Scratch Guides\n- Tutorials\n- Integration with External Platforms\n- Events\n- Webinars\n\nSo, let‚Äôs embark on this journey together. Dive in and explore the offerings of the fifth edition of the LlamaIndex Update series!\n# Feature Releases and Enhancements\n- Open-Sourced RAG Platform: LlamaIndex open-sourced http://secinsights.ai, accelerating RAG app development with chat-based Q&A features. Tweet\n- Linear Adapter Fine-Tuning: LlamaIndex enables efficient fine-tuning of linear adapters on any embedding without re-embedding, enhancing retrieval/RAG across various models. Tweet, Docs, BlogPost\n- Hierarchical Agents: By structuring LLM agents in a parent-child hierarchy, we enhance complex search and retrieval tasks across diverse data, offering more reliability than a standalone agent. Tweet\n- SummaryIndex: We‚Äôve renamed ListIndex to SummaryIndex to make it clearer what its main functionality is. Backward compatibility is maintained for existing code using ListIndex. Tweet\n- Evaluation: LlamaIndex‚Äôs new RAG evaluation toolkit offers async capabilities, diverse assessment criteria, and a centralized BaseEvaluator for easier developer integrations. Tweet, Docs.\n- Hybrid Search for Postgres/pgvector: LlamaIndex introduces a hybrid search for Postgres/pgvector. Tweet, Docs.\n- Replit Templates: LlamaIndex partners with Replit for easy LLM app templates, including ready-to-use Streamlit apps and full Typescript templates. Tweet, Replit Templates.\n\n## LlamaIndex.TS:\n- Launches with MongoDBReader and type-safe metadata. Tweet.\n- Launches with chat history, enhanced keyword index, and Notion DB support. Tweet.\n\n# Fine-Tuning Guides:\n- OpenAI Fine-Tuning: LlamaIndex unveils a fresh guide on harnessing OpenAI fine-tuning to embed knowledge from any text corpus. In short: generate QA pairs with GPT-4, format them into a training dataset, and proceed to fine-tuning. Tweet, Docs.\n- Embedding Fine-Tuning: LlamaIndex has a more advanced embedding fine-tuning feature, enabling complex NN query transformations on any embedding, including custom ones, and offering the ability to save intermediate checkpoints for enhanced model control. Tweet, Docs.\n\n# Retrieval Tips For RAG:\n- Use references (smaller chunks or summaries) instead of embedding full text.\n- Results in 10‚Äì20 % improvement.\n- Embeddings decoupled from main text chunks.\n- Smaller references allow efficient LLM synthesis.\n- Deduplication applied for repetitive references.\n- Evaluated using synthetic dataset; 20‚Äì25% MRR boost.\n\nTweet\n# Building RAG from Scratch Guides:\n- Build Data Ingestion from scratch. Docs.\n- Build Retrieval from scratch. Docs.\n- Build Vector Store from scratch. Docs.\n- Build Response Synthesis from scratch. Docs.\n- Build Router from scratch. Docs.\n- Build Evaluation from scratch. Docs.\n\n# Tutorials:\n- Wenqi Glantz tutorial on Fine-Tuning GPT-3.5 RAG Pipeline with GPT-4 Training Data with LlamaIndex fine-tuning abstractions.\n- Wenqi Glantz tutorial on Fine-Tuning Your Embedding Model to Maximize Relevance Retrieval in RAG Pipeline with LlamaIndex.\n\nTutorials from the LlamaIndex Team.\n- Sourabh tutorial on SEC Insights, End-to-End Guide on secinsights.ai\n- Adam‚Äôs tutorial on Custom Tools for Data Agents.\n- Logan tutorial on retrieval/reranking, covering Node Parsing, AutoMergingRetriever, HierarchicalNodeParser, node post-processors, and the setup of a RouterQueryEngine.\n\n# Integrations with External Platforms\n- Integration with PortkeyAI: LlamaIndex integrates with PortkeyAI, boosting LLM providers']","Replit templates offer LlamaIndex users the benefit of kickstarting their projects with zero environment setup hassles, making it easier to begin development.",reasoning,"[{'title': 'LlamaIndex Update ‚Äî 20/09/2023', 'link': 'https://www.llamaindex.ai/blog/llamaindex-update-20-09-2023-86ed66f78bac', 'date': 'Sep 21, 2023', 'tags': ""['Llamaindex', 'LLM', 'OpenAI', 'AI', 'NLP']""}]",True
93,What multi-modal RAG processes use text and images?,"['(co-authored by Haotian Zhang, Laurie Voss, and Jerry Liu @ LlamaIndex)\n# Overview\nIn this blog we‚Äôre excited to present a fundamentally new paradigm: multi-modal Retrieval-Augmented Generation (RAG). We present new abstractions in LlamaIndex that now enable the following:\n- Multi-modal LLMs and Embeddings\n- Multi-modal Indexing and Retrieval (integrates with vector dbs)\n\n# Multi-Modal RAG\nOne of the most exciting announcements at OpenAI Dev Day was the release of the GPT-4V API. GPT-4V is a multi-modal model that takes in both text/images, and can output text responses. It‚Äôs the latest model in a recent series of advances around multi-modal models: LLaVa, and Fuyu-8B.\nThis extends the capabilities of LLMs in exciting new directions. In the past year, entire application stacks have emerged around the text-in/text-out paradigm. One of the most notable examples is Retrieval Augmented Generation (RAG) ‚Äî combine an LLM with an external text corpus to reason over data that the model isn‚Äôt trained on. One of the most significant impacts of RAG for end-users was how much it accelerated time-to-insight on unstructured text data. By processing an arbitrary document (PDF, web page), loading it into storage, and feeding it into the context window of an LLM, you could extract out any insights you wanted from it.\nThe introduction of GPT-4V API allows us to extend RAG concepts into the hybrid image/text domain, and unlock value from an even greater corpus of data (including images).\nThink about all the steps in a standard RAG pipeline and how it can be extended to a multi-modal setting.\n- Input: The input can be text or images.\n- Retrieval: The retrieved context can be text or images.\n- Synthesis: The answer can be synthesized over both text and images.\n- Response: The returned result can be text and/or images.\n\nThis is just a small part of the overall space too. You can have chained/sequential calls that interleave between image and text reasoning, such as Retrieval Augmented Image Captioning or multi-modal agent loops.\n# Abstractions in LlamaIndex\nWe‚Äôre excited to present new abstractions in LlamaIndex that help make multi-modal RAG possible. For each abstraction, we explicitly note what we‚Äôve done so far and what‚Äôs still to come.\n## Multi-modal LLM\nWe have direct support for GPT-4V via our OpenAIMultiModal class and support for open-source multi-modal models via our ReplicateMultiModal class (currently in beta, so that name might change). Our SimpleDirectoryReader has long been able to ingest audio, images and video, but now you can pass them directly to GPT-4V and ask questions about them, like this:\nfrom llama_index.multi_modal_llms import OpenAIMultiModal\nfrom llama_index import SimpleDirectoryReader\n\nimage_documents = SimpleDirectoryReader(local_directory).load_data()\n\nopenai_mm_llm = OpenAIMultiModal(\n    model=""gpt-4-vision-preview"", api_key=OPENAI_API_TOKEN, max_new_tokens=300\n)\nresponse = openai_mm_llm.complete(\n    prompt=""what is in the image?"", image_documents=image_documents\n)\nThis is a new base model abstraction. Unlike our default LLM class, which has standard completion/chat endpoints, the multi-modal model (MultiModalLLM) can take in both image and text as input.\nThis also unifies the interface between both GPT-4V and open-source models.\nResources\nWe have initial implementations for both GPT-4V and vision models hosted on Replicate. We also have a docs page for multi-modal models:\n- Multi-modal docs page\n- GPT-4V\n- Replicate\n\nDisplayed image and example output from GPT-4V given text query ‚ÄúDescribe image as alternative text‚Äù\nWhat‚Äôs still to come:\n- More multi-modal LLM integrations\n- Chat endpoints\n- Streaming\n\n## Multi-Modal Embeddings\nWe introduce a new MultiModalEmbedding base class that can embed both text and images. It contains all the methods as']",The multi-modal RAG processes involve several steps: Input can be either text or images; Retrieval can yield context that is also text or images; Synthesis allows for answers to be generated over both text and images; and the Response can be returned as text and/or images.,reasoning,"[{'title': 'Multi-Modal RAG', 'link': 'https://www.llamaindex.ai/blog/multi-modal-rag-621de7525fea', 'date': 'Nov 10, 2023', 'tags': ""['Gpt 4v', 'Llamaindex', 'LLM', 'Vision', 'Rag']""}]",True
94,What does LlamaIndex.TS offer devs using LLMs with data?,"['We are beyond excited to announce v0.0.1 of LlamaIndex.TS, a Typescript first library focused on helping you use your private data with large language models.\n\n## What is LlamaIndex?\nOur core goal for LlamaIndex is to help developers easily integrate their data with Large Language Models (LLMs). LLMs, like ChatGPT, have been a revolution in the way we think about handling textual input and data, but all of them have the limitation in what data they have access to. In addition to the ‚Äúknowledge cutoff‚Äù (we are nearing the 2 year anniversary for when ChatGPT‚Äôs latest data was trained) LLMs can‚Äôt access data from your companies, from your personal analyses, or from the data your users generate.\nWith LlamaIndex.TS, we look to achieve that goal by meeting developers at their (my) language of choice, in this case Typescript. We are committed to making this library the easiest to use, most robust solution out there for using data with LLMs.\n## Backstory\nIt was at the Emergency ChatGPT Hackathon hosted by Pete Huang and Rachel Woods that I met Jerry. Having worked in the JS world for the last 8 years, my first question was ‚Äúwhy don‚Äôt you build this in Javascript?‚Äù After he demurred, he very patiently guided me through setting up the Python dev environment. (I think it took us 20 minutes before we figured it all out!) So, when Jerry offered to let me build LlamaIndex.TS I obviously couldn‚Äôt turn him down. Can‚Äôt wait to see what you build with it!\n## Design\nAt a high level, LlamaIndex.TS first takes the file inputs, loads them into a standardized format, and creates an Index (knowledge base).\n\nWe then retrieve the relevant information from the index and use that in our query to the LLM to generate more a grounded response.\n\nCheck out our docs for a more in depth explanation!\n## Playground\nWe are building an open source playground for LlamaIndex.TS. Please check it out at https://llama-playground.vercel.app/ PRs are welcome here! https://github.com/run-llama/ts-playground\n## Main Differences from LlamaIndex Python\n- All function names are üê™ camel cased.\n- The prompt interface is much simpler and uses native javascript template literals.\n- We do not ship non-async versions of functions. Please use await or .then callbacks.\n- We use interfaces and POJOs in lieu of classes where it makes sense. For example, ChatEngine, a base class in Python is an interface in JS. ServiceContext, a class in Python is an interface/POJO in JS.\n\n## Runtimes\nCurrently, we support NodeJS v18 and up. Lots of plans on this front though. Stay tuned!\n## Contributing\nOnly the core features are built out so far, so there is a lot of work that needs to be done on the loader and integration side. If you‚Äôre interested in contributing, please send us a message or even better a PR!\nhttps://github.com/run-llama/LlamaIndexTS\n']","LlamaIndex.TS offers developers a Typescript-first library that helps them easily integrate their private data with Large Language Models (LLMs). It allows developers to load file inputs into a standardized format, create an index (knowledge base), and retrieve relevant information to generate grounded responses using LLMs.",reasoning,"[{'title': 'Introducing LlamaIndex.TS', 'link': 'https://www.llamaindex.ai/blog/introducing-llamaindex-ts-89f41a1f24ab', 'date': 'Jul 24, 2023', 'tags': ""['Artificial Intelligence', 'Large Language Models', 'ChatGPT', 'Chatbots', 'Deep Learning']""}]",True
95,What steps does the ReAct agent take to choose a tool?,"['(model=""gpt-3.5-turbo-0613"")\n# initialize openai agent\nagent = OpenAIAgent.from_tools(tools, llm=llm, verbose=True)\n# initialize ReAct agent\nagent = ReActAgent.from_tools(tools, llm=llm, verbose=True)\n# use agent\nresponse = agent.chat(""What is (121 * 3) + 42?"")\nEach agent takes in a set of Tools. The details behind our tool abstractions are provided below. Each agent also supports two main methods for taking in an input task ‚Äî chat and query. Note that these are the core methods used in our ChatEngine and QueryEngine respectively. In fact that our base agent class (BaseAgent) simply inherits from BaseChatEngine and BaseQueryEngine. chat allows the agent to utilize previously stored conversation history, whereas query is a stateless call - history/state is not preserved over time.\nThe reasoning loop depends on the type of agent. The OpenAI agent calls the OpenAI function API in a while loop, since the tool decision logic is baked into the function API. Given an input prompt and previous chat history (which includes previous function calls), the function API will decide whether to make another function call (pick a Tool), or return an assistant message. If the API returns a function call, then we are responsible for executing the function and passing in a function message in the chat history. If the API returns an assistant message, then the loop is complete (we assume the task is solved).\nThe ReAct agent uses general text completion endpoints, so it can be used with any LLM. A text completion endpoint has a simple input str ‚Üí output str format, which means that the reasoning logic must be encoded in the prompt. The ReAct agent uses an input prompt inspired by the ReAct paper (and adapted into other versions), in order to decide which tool to pick. It looks something like this:\n...\nYou have access to the following tools:\n{tool_desc}\n\nTo answer the question, please use the following format.\n\n```\nThought: I need to use a tool to help me answer the question.\nAction: tool name (one of {tool_names})\nAction Input: the input to the tool, in a JSON format representing the kwargs (e.g. {{""text"": ""hello world"", ""num_beams"": 5}})\n```\nPlease use a valid JSON format for the action input. Do NOT do this {{\'text\': \'hello world\', \'num_beams\': 5}}.\n\nIf this format is used, you will receive a response in the following format:\n\n```\nObservation: tool response\n```\n...\nWe implement ReAct natively over chat prompts; the reasoning loop is implemented as an alternating series of assistant and user messages. The Thought/Action/Action Input section is represented as an assistant message, and the Observation section is implemented as a user message.\nNote: the ReAct prompt expects not only the name of the tool to pick, but also the parameters to fill in the tool in a JSON format. This makes the output not dissimilar from the output of the OpenAI Function API ‚Äî the main difference is that in the case of the function API, the tool-picking logic is baked into the API itself (through a finetuned model), whereas here it is elicited through explicit prompting.\n## Tool Abstractions\nHaving proper tool abstractions is at the core of building data agents. Defining a set of Tools is similar to defining any API interface, with the exception that these Tools are meant for agent rather than human use. We allow users to define both a single Tool as well as a ‚ÄúToolSpec‚Äù containing a series of functions under the hood.\nWe describe the base tool abstraction, as well as how you can easily define tools over existing query engines, other tools.\nBase Tool Abstraction\nThe base tool defines a very generic interface. The __call__ function can take in any series of arguments, and return a generic ToolOutput container that can capture any response. A tool also has metadata containing its name, description, and function schema.\n@dataclass\nclass ToolMetadata:\n    description: str\n    name: Optional[str] = None\n    fn_schema: Optional[Type[BaseModel]] = DefaultToolFnSchema\n\nclass BaseTool:\n    @property\n    @abstractmethod\n    def metadata(self) -&gt; ToolMetadata:\n        pass\n    @abstractmethod\n    def __call__(self, input: Any) -&gt; ToolOutput:\n   ']","The ReAct agent chooses a tool by using a reasoning prompt that includes a Thought/Action/Action Input section. It first thinks about the need to use a tool to answer the question, then specifies the tool name and provides the input parameters in a valid JSON format. This process is part of an alternating series of assistant and user messages, where the Thought/Action/Action Input is represented as an assistant message, and the tool's response is captured in the Observation section of a user message.",reasoning,"[{'title': 'Data Agents', 'link': 'https://www.llamaindex.ai/blog/data-agents-eed797d7972f', 'date': 'Jul 12, 2023', 'tags': ""['Llamaindex', 'NLP', 'AI', 'Agents', 'Data']""}]",True
96,What allows smooth app deployment on a Ray cluster?,"[' is quite powerful; it can perform semantic search over one document, or combine results across multiple documents.\nFor instance, given the following question ‚ÄúWhat is Ray?‚Äù, we can break this into sub-questions ‚ÄúWhat is Ray according to the documentation‚Äù, and ‚ÄúWhat is Ray according to the blog posts‚Äù over the document query engine and blog query engine respectively.\n# Define a sub-question query engine, that can use the individual query engines as tools.\n        query_engine_tools = [\n            QueryEngineTool(\n                query_engine=self.ray_docs_engine,\n                metadata=ToolMetadata(name=""ray_docs_engine"", description=""Provides information about the Ray documentation"")\n            ),\n            QueryEngineTool(\n                query_engine=self.ray_blogs_engine, \n                metadata=ToolMetadata(name=""ray_blogs_engine"", description=""Provides information about Ray blog posts"")\n            ),\n        ]\n\nsub_query_engine = SubQuestionQueryEngine.from_defaults(query_engine_tools=query_engine_tools, service_context=service_context, use_async=False)\nHave a look at deploy_app.py to review the full implementation.\n# Deploying with Ray Serve\n\nWe‚Äôve now created an incredibly powerful query module over your data. As a next step, what if we could seamlessly deploy this function to production and serve users? Ray Serve makes this incredibly easy to do. Ray Serve is a scalable compute layer for serving ML models and LLMs that enables serving individual models or creating composite model pipelines where you can independently deploy, update, and scale individual components.\nTo do this, you just need to do the following steps:\n- Define an outer class that can ‚Äúwrap‚Äù a query engine, and expose a ‚Äúquery‚Äù endpoint\n- Add a `@ray.serve.deployment` decorator on this class\n- Deploy the Ray Serve application\n\nIt will look something like the following:\nfrom ray import serve\n\n@serve.deployment\nclass QADeployment:\n    def __init__(self):\n self.query_engine = ...\n\n    def query(self, query: str):\n            response =  self.query_engine.query(query)\n            source_nodes = response.source_nodes\n            source_str = """"\n            for i in range(len(source_nodes)):\n                node = source_nodes[i]\n                source_str += f""Sub-question {i+1}:\\n""\n                source_str += node.node.text\n                source_str += ""\\n\\n""\n            return f""Response: {str(response)} \\n\\n\\n {source_str}\\n""\n\n    async def __call__(self, request: Request):\n        query = request.query_params[""query""]\n        return str(self.query(query))\n\n# Deploy the Ray Serve application.\ndeployment = QADeployment.bind()\nHave a look at the deploy_app.py for full implementation.\n# Example Queries\nOnce we‚Äôve deployed the application, we can query it with questions about Ray.\nWe can query just one of the data sources:\nQ: ""What is Ray Serve?""\n\nRay Serve is a system for deploying and managing applications on a Ray\ncluster. It provides APIs for deploying applications, managing replicas, and\nmaking requests to applications. It also provides a command line interface\n(CLI) for managing applications and a dashboard']","Ray Serve allows smooth app deployment on a Ray cluster by providing APIs for deploying applications, managing replicas, and making requests to applications. It also includes a command line interface (CLI) for managing applications and a dashboard.",reasoning,"[{'title': 'Build and Scale a Powerful Query Engine with LlamaIndex and Ray', 'link': 'https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4', 'date': 'Jun 27, 2023', 'tags': ""['NLP', 'Distributed Systems', 'AI', 'Large Language Models', 'Parallel Computing']""}]",True
97,How did the mobile shift impact software models and apps like Uber?,"['Hi, I‚Äôm Laurie, and today is my first day as VP of Developer Relations at LlamaIndex!\nQuick background on me: I started my career 27 years ago as a web developer, founded a couple of companies including npm Inc., and have always been about talking to developers about the state of the development world and how we fit into it, whether that‚Äôs on my personal site, in conference talks, or in big community surveys.\nI wrote last week about why LLMs are the future of software. To summarize that post: until now computers have been very good at ingesting, sorting, and transmitting data, but understanding what they were working with was beyond them. Very recently, with tools like GPT-4 and Llama 2 that threshold has been crossed. Software can now read, summarize, and make novel connections within arbitrarily large sets of data. It can write software, it can use tools, it can generate text, images and music. A huge new set of capabilities have been unlocked.\nThe last time I saw a shift this big in the technological landscape was probably the original iPhone. Suddenly, everybody who was doing everything needed to also do it for mobile devices. You sell real estate? Now you do it with an app. You run a dating site? Now you do it with an app. There was a gigantic rush as everyone who did everything in software suddenly saw huge benefits from moving onto a new platform.\nBut the most interesting applications that platform shift enabled were the ones that couldn‚Äôt have existed before. Uber relies on everyone ‚Äî drivers as well as customers ‚Äî already owning a piece of mobile hardware that can connect to the Internet and use GPS to locate them. Prior to the mobile revolution, that would have been an absurd business plan. After it happened, it was taken for granted. So in the same way, I‚Äôm especially interested in seeing what the applications of LLMs are that were simply impossible before.\nAll of which is why I‚Äôm delighted to be at a company at the center of the LLM universe. If you‚Äôre new to LlamaIndex, it‚Äôs a Python and JavaScript framework that lets you quickly put together totally customizable, production-class applications that use LLMs. We let you ingest data from dozens of sources and use any model you want, and we make it simple to link everything together.\nThe team at LlamaIndex is absolutely amazing and I am looking forward to working with Jerry, Simon and Yi. Yi will be focusing more on partnerships.\nWant to see a demo? Our SEC Insights app lets you ingest regulatory documents from major corporations and then ask questions about them. Want to dive and build your own? Get started in Python or JavaScript!\nP.S. If you‚Äôre wondering ‚Äúwhy an alpaca?‚Äù it‚Äôs because my personal mascot is an alpaca and has been for years. Alpacas and llamas are a great match!\n']","The mobile shift impacted software models and apps like Uber by enabling applications that relied on mobile hardware and internet connectivity, which were previously impossible. The mobile revolution allowed for business plans that depended on GPS and real-time location services to become feasible, as everyone had access to mobile devices. This shift created a gigantic rush for software developers to adapt their services for mobile platforms, leading to innovative applications that took advantage of these new capabilities.",reasoning,"[{'title': 'LlamaIndex + Laurie Voss: an alpaca joins the llamas', 'link': 'https://www.llamaindex.ai/blog/llamaindex-laurie-voss-an-alpaca-joins-the-llamas-9cae1081adff', 'date': 'Oct 2, 2023', 'tags': ""['Llamaindex', 'Hires']""}]",True
98,How do metadata filters enhance Qdrant's querying for data?,"[', Chroma, Qdrant and Pinecone, allowing auto-retrieval via metadata filters, elevating the functionality of structured and unstructured data querying. Tweet.\n\nüó∫Ô∏è Guides:\n- Guide on building LLM apps for financial data which is presented at MindsDB event. Learn to query diverse financial data using advanced RAG with techniques for multi-document comparisons, embedded tables, and converting text queries into domain-specific languages.\n- Guide on advanced RAG Cheat Sheet, a concise guide offering solutions for different RAG-related pain points and techniques. It‚Äôs part of our Snowflake BUILD talk and PyData Global talk.\n\n‚úçÔ∏è Tutorials:\n- Blog by Waii.ai on creating an agent that queries both enterprise databases and PDF data, combining advanced text-to-SQL techniques and a Llama Index RAG pipeline, for effective analysis of structured and unstructured data like retail sales trends.\n- Wenqi Glantz‚Äôs tutorial on using LLMs for querying knowledge graphs introduces seven strategies, now easily accessible through our LlamaPacks and featured in our Neo4j query engine.\n- An hour comprehensive workshop tutorial by AIMakerspace on RAG strategies over complex documents through recursive retrieval.\n- Laurie‚Äôs video on using LlamaIndex for multi-modal retrieval-augmented generation apps teaches you to build indexes and retrieve data from text and images, for enhanced query responses.\n- Ravi Theja‚Äôs video on Understanding LlamaIndex 0.9v abstractions and features.\n\nü§ù Integrations:\n- We integrated AssemblyAI with Llama Index TS, enhancing the capabilities and offering new, innovative solutions. Blog.\n- We integrated Panel, a powerful framework for building interactive data apps as a LlamaPack. This provides you with a robust chat interface for talking to your data with full streaming support in a single line of code. Docs, Tweet.\n- We integrated FlagEmbeddingReranker to further boost your RAG pipeline. Notebook, Tweet.\n\nüé• Webinars:\nWebinar featuring Haotian Liu, the author of LLaVa which includes a deep dive into the open-source multi-modal models of LLaVa, which are competitive with GPT-4V, and a presentation on multi-modal use cases with LLaVa + LlamaIndex by Haotian Zhang from the LlamaIndex team.\nüè¢ Calling all enterprises:\nAre you building with LlamaIndex? We are working hard to make LlamaIndex even more Enterprise-ready and have sneak peeks at our upcoming products available for partners. Interested? Get in touch.\n']",The answer to given question is not present in context,reasoning,"[{'title': 'LlamaIndex Newsletter 2023‚Äì12‚Äì12', 'link': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2023-12-12-4a5d542fbb1e', 'date': 'Dec 12, 2023', 'tags': ""['Llamaindex', 'LLM', 'Newsletter', 'Rag', 'AI']""}]",True
99,What new LlamaCloud Chat features boost data access & team collaboration?,"['To build a production-quality LLM agent over your data, you need a production-quality data processing layer. LlamaCloud is that data processing and management layer for your AI knowledge assistants. Since launching a LlamaCloud waitlist last week, we‚Äôve gotten hundreds of signups and published case studies showing how it cuts production development hours by 50%.\nOn top of that, our team has shipped a slew of new features at a breakneck pace in the past week. We‚Äôre excited to highlight these new features that collectively help you set up a chat interface in minutes, increase developer collaboration within your team, and access more data and metadata.\n## Set up a Chat Interface in Minutes\n\nWe are releasing LlamaCloud Chat, which gives you an easy-to-use chat interface over your data. This chat interface is a conversational RAG pipeline built over the advanced retrieval interface that a given pipeline provides, and has out-of-the-box support for streaming and citations - it‚Äôs powered by the same DNA as create-llama, our fully open-source set up tool for LLM applications.\nThe LlamaCloud UI already lets you set up a data pipeline over any data in minutes, and now you get a full-blown ChatGPT over your data in minutes. Besides the chat UI, you also have additional flexibility:\n- You can customize metadata filters in the retrieval parameters\n- You can view retrieved nodes and their source files\n- Besides chunk-level retrieval, you can now do file-level retrieval (more on this soon!)\n\nLlamaCloud is fundamentally a developer tool: with these updates, we enable developers to spend less time on data pipeline setup and iteration, and more time on writing the orchestration logic on top of this interface.\n## Increased Developer Collaboration\nThe team selection interface\nOrganization settings\nWe‚Äôve added organizational features into LlamaCloud, enabling any individual user to create an organization and add other users to the organization. Any user within an organization will have a view of all the organization‚Äôs projects and indexes within each project.\nThis allows your team to have a single-source of truth for your data pipelines. In the past each developer would spend time re-indexing/experimenting with the same sources of data. This feature enables transparency, re-use, and generally more rapid development velocity.\n## Improved Data and Metadata Access\n\nWe‚Äôve made several updates here - we‚Äôve added more data connectors and added features to let you more easily access and customize metadata.\n- We added a Notion, Slack, and Jira Connector\n- Our Sharepoint connector now natively integrates with user IDs that you can filter for, enabling you to build LLM applications with access control.\n- You can now attach metadata to any uploaded file as a CSV - do this through the UI or our API!\n\n## Want to see what LlamaCloud can do for you?\nCome sign up on our waitlist for access. If you‚Äôre interested in chatting about enterprise plans, get in touch.\nIf you‚Äôve gotten access to LlamaCloud, check out our rich repository of demonstrations and examples on how to build different LLM application use cases.\n']","The new LlamaCloud Chat features that boost data access and team collaboration include the ability to customize metadata filters in the retrieval parameters, view retrieved nodes and their source files, and perform file-level retrieval. Additionally, organizational features have been added, allowing users to create organizations, add other users, and provide a single-source of truth for data pipelines, which enhances transparency and re-use among team members.",reasoning,"[{'title': 'The latest updates to LlamaCloud', 'link': 'https://www.llamaindex.ai/blog/the-latest-updates-to-llamacloud', 'date': 'Jul 19, 2024', 'tags': ""['Llamacloud']""}]",True
